{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Update Multi-Task Superconductor Generator from GitHub\n\nRun this notebook to pull the latest code from GitHub into your Google Drive repo.\nPreserves checkpoints and training logs. Invalidates tensor cache if data changed.\n\n**Keep this notebook on Drive** — reuse it anytime you need to update."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "# Update this path if your repo is in a different Drive location\n",
    "REPO_PATH = \"/content/drive/My Drive/Colab Notebooks/SuperconductorVAE/superconductor-vae\"\n",
    "GITHUB_URL = \"https://github.com/jamesconde/superconductor-vae.git\"\n",
    "BRANCH = \"main\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "repo = Path(REPO_PATH)\n",
    "is_git_repo = (repo / '.git').exists()\n",
    "\n",
    "if is_git_repo:\n",
    "    # --- Fast path: git pull ---\n",
    "    print(\"Git repo detected — pulling latest changes...\")\n",
    "    !cd \"{REPO_PATH}\" && git fetch origin {BRANCH} && git reset --hard origin/{BRANCH}\n",
    "    print(\"Done!\")\n",
    "\n",
    "else:\n",
    "    # --- First time or tarball extraction: clone fresh, preserve outputs ---\n",
    "    print(\"No .git directory — cloning fresh from GitHub...\")\n",
    "    print(\"Preserving outputs/ (checkpoints, logs, norm_stats)...\")\n",
    "\n",
    "    # Save outputs we want to keep\n",
    "    preserve_dir = Path(\"/content/_preserved_outputs\")\n",
    "    preserve_dir.mkdir(exist_ok=True)\n",
    "    outputs_dir = repo / \"outputs\"\n",
    "\n",
    "    preserved_files = []\n",
    "    if outputs_dir.exists():\n",
    "        for f in outputs_dir.iterdir():\n",
    "            if f.is_file() and f.suffix in ('.pt', '.json', '.csv', '.log'):\n",
    "                dest = preserve_dir / f.name\n",
    "                print(f\"  Preserving: {f.name} ({f.stat().st_size / 1e6:.1f} MB)\")\n",
    "                shutil.copy2(str(f), str(dest))\n",
    "                preserved_files.append(f.name)\n",
    "\n",
    "    # Also preserve tensor cache if it exists\n",
    "    cache_dir = repo / \"data\" / \"processed\" / \"cache\"\n",
    "    preserved_cache = False\n",
    "    if cache_dir.exists():\n",
    "        print(f\"  Preserving tensor cache...\")\n",
    "        shutil.copytree(str(cache_dir), \"/content/_preserved_cache\", dirs_exist_ok=True)\n",
    "        preserved_cache = True\n",
    "\n",
    "    # Remove old repo and clone\n",
    "    if repo.exists():\n",
    "        shutil.rmtree(str(repo))\n",
    "    !git clone --branch {BRANCH} {GITHUB_URL} \"{REPO_PATH}\"\n",
    "\n",
    "    # Restore outputs\n",
    "    (repo / \"outputs\").mkdir(exist_ok=True)\n",
    "    for fname in preserved_files:\n",
    "        src = preserve_dir / fname\n",
    "        dest = repo / \"outputs\" / fname\n",
    "        shutil.copy2(str(src), str(dest))\n",
    "        print(f\"  Restored: {fname}\")\n",
    "\n",
    "    # Restore cache\n",
    "    if preserved_cache:\n",
    "        cache_dest = repo / \"data\" / \"processed\" / \"cache\"\n",
    "        cache_dest.mkdir(parents=True, exist_ok=True)\n",
    "        shutil.copytree(\"/content/_preserved_cache\", str(cache_dest), dirs_exist_ok=True)\n",
    "        print(\"  Restored tensor cache\")\n",
    "\n",
    "    # Cleanup temp\n",
    "    shutil.rmtree(str(preserve_dir), ignore_errors=True)\n",
    "    shutil.rmtree(\"/content/_preserved_cache\", ignore_errors=True)\n",
    "\n",
    "    print(f\"\\nDone! Repo is now a git clone at: {REPO_PATH}\")\n",
    "    print(\"Future updates will use fast 'git pull'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Optional: Invalidate tensor cache ===\n",
    "# Run this cell if the dataset CSV or preprocessing config changed.\n",
    "# Training will rebuild the cache on next run (~2 min).\n",
    "\n",
    "cache_meta = Path(REPO_PATH) / \"data/processed/cache/cache_meta.json\"\n",
    "if cache_meta.exists():\n",
    "    cache_meta.unlink()\n",
    "    print(\"Tensor cache invalidated — will rebuild on next training run.\")\n",
    "else:\n",
    "    print(\"No cache to invalidate (already fresh or doesn't exist).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## V12.28: Post-Update Migration Steps\n\nIf this is the **first update after V12.28**, run the cells below to:\n1. Install new dependencies (matminer)\n2. Ingest JARVIS superconductor data (adds ~4,300 new samples)\n3. Migrate the checkpoint to the new architecture (Net2Net weight transfer)\n\n**These only need to run ONCE.** After that, skip to the Verify cell.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# === V12.28 Step 1: Install matminer (needed for JARVIS ingestion) ===\n# matminer computes 146 material features (Magpie, valence, ion properties, etc.)\n# Skip if already installed.\n\ntry:\n    import matminer\n    print(f\"matminer already installed: v{matminer.__version__}\")\nexcept ImportError:\n    print(\"Installing matminer...\")\n    !pip install matminer -q\n    print(\"Done!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# === V12.28 Step 2: Ingest JARVIS superconductor data ===\n# Adds ~4,300 new samples from JARVIS databases + 6 physics feature columns.\n# Safe to re-run (deduplicates against existing data).\n# Takes ~2 minutes (matminer feature computation is the bottleneck).\n\nimport subprocess, sys\n\ncsv_path = Path(REPO_PATH) / \"data/processed/supercon_fractions_contrastive.csv\"\n\n# Check if JARVIS data was already ingested (>50K rows means it's done)\nif csv_path.exists():\n    import pandas as pd\n    n_rows = len(pd.read_csv(csv_path, usecols=[0]))\n    if n_rows > 50000:\n        print(f\"JARVIS data already ingested ({n_rows} rows). Skipping.\")\n    else:\n        print(f\"Current CSV has {n_rows} rows. Running JARVIS ingestion...\")\n        !cd \"{REPO_PATH}\" && PYTHONPATH=src python scripts/ingest_jarvis.py\n        print(\"\\nDone! Re-check row count:\")\n        n_rows_new = len(pd.read_csv(csv_path, usecols=[0]))\n        print(f\"  {n_rows} → {n_rows_new} rows\")\nelse:\n    print(\"WARNING: CSV not found. Run training once first to generate the base CSV,\"\n          \" then re-run this cell.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# === V12.28 Step 3: Migrate checkpoint to new architecture ===\n# Applies Net2Net weight transfer from old Tc head to new residual Tc head.\n# Backs up the original checkpoint before modifying it.\n# Safe to re-run (skips if already migrated).\n\nckpt_path = Path(REPO_PATH) / \"outputs/checkpoint_best.pt\"\nbackup_path = ckpt_path.with_suffix('.pt.bak_pre_v1228')\n\nif not ckpt_path.exists():\n    print(\"No checkpoint found — nothing to migrate. Training will start fresh.\")\nelif backup_path.exists():\n    print(f\"Checkpoint already migrated (backup exists at {backup_path.name}). Skipping.\")\nelse:\n    # Dry run first to verify\n    print(\"Running migration dry-run...\")\n    !cd \"{REPO_PATH}\" && PYTHONPATH=src python scripts/migrate_checkpoint_v1228.py --dry-run\n    print(\"\\n\" + \"=\"*50)\n    print(\"Dry run OK. Now applying migration for real...\")\n    print(\"=\"*50 + \"\\n\")\n    !cd \"{REPO_PATH}\" && PYTHONPATH=src python scripts/migrate_checkpoint_v1228.py\n    print(\"\\nCheckpoint migrated! Original backed up as checkpoint_best.pt.bak_pre_v1228\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# === V12.28 Step 4: Invalidate tensor cache (dataset changed) ===\n# The CSV now has new rows and columns — old tensor cache is stale.\n# Training will rebuild it automatically on next run (~2 min).\n\ncache_meta = Path(REPO_PATH) / \"data/processed/cache/cache_meta.json\"\ncache_dir = Path(REPO_PATH) / \"data/processed/cache\"\n\nif cache_dir.exists() and any(cache_dir.iterdir()):\n    shutil.rmtree(str(cache_dir))\n    cache_dir.mkdir(exist_ok=True)\n    print(\"Tensor cache cleared — will rebuild on next training run.\")\nelse:\n    print(\"No tensor cache to clear.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Verify ===\n",
    "print(\"Repo contents:\")\n",
    "!ls \"{REPO_PATH}/\"\n",
    "print()\n",
    "print(\"Latest commit:\")\n",
    "!cd \"{REPO_PATH}\" && git log --oneline -3\n",
    "print()\n",
    "print(\"Outputs:\")\n",
    "!ls -lh \"{REPO_PATH}/outputs/\" 2>/dev/null | head -10\n",
    "print()\n",
    "print(\"Now close this notebook and open train_colab.ipynb to start training.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}