{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Superconductor VAE \u2014 Generative Evaluation & Novel Discovery\n\nEvaluate generative capability and discover novel superconductors.\n\n**Core idea**: Phase 2 self-supervised training generates thousands of formulas per sub-epoch.\nWhile training the model, we simultaneously check every generated formula against:\n1. The **45 held-out superconductors** (holdout recovery)\n2. The training set (known vs novel classification)\n\nThis means Phase 2 training IS the search \u2014 it improves the model AND discovers materials at the same time.\n\n**Workflow**:\n1. **Baseline**: Quick roundtrip validation (encode holdout \u2192 decode, ~1 min)\n2. **Phase 2 Loop** (main event): Train N sub-epochs, each generating + filtering + checking formulas\n   - Holdout recoveries flagged in real-time\n   - Novel candidates logged to `phase2_discoveries.jsonl`\n3. **Post-training search**: Run targeted holdout search on the **improved** model\n4. **Broad exploration**: Generate novel candidates from the improved Z-space\n\n**GPU**: Any GPU works. Larger GPUs = more samples per sub-epoch (auto-scaled).\n\n**Families**: YBCO, LSCO, Hg-cuprate, Tl-cuprate, Bi-cuprate, Iron-based, MgB2, Conventional, Other\n\n**Prerequisites** (one-time setup):\n- A checkpoint in `outputs/` (from `train_colab.ipynb` or uploaded manually)\n- Data cache is built automatically on first run (~5 min)\n- Z-cache is built automatically on first run (~2 min on A100)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1b: Sync Repo with GitHub\n",
    "\n",
    "Pull the latest code from GitHub so Colab matches your most recent push."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Sync repo with GitHub (clone or pull latest changes)\n_REPO = \"/content/drive/My Drive/Colab Notebooks/SuperconductorVAE/superconductor-vae\"\n_GITHUB_URL = \"https://github.com/jamesconde/superconductor-vae.git\"\n\nimport subprocess, os\n\ndef _run_git(*args, **kwargs):\n    \"\"\"Run a git command in _REPO, return (stdout, stderr, returncode).\"\"\"\n    r = subprocess.run(list(args), cwd=_REPO, capture_output=True, text=True, timeout=120, **kwargs)\n    return r.stdout.strip(), r.stderr.strip(), r.returncode\n\nif os.path.isdir(os.path.join(_REPO, '.git')):\n    # Repo exists \u2014 ensure 'origin' remote is configured\n    out, err, rc = _run_git(\"git\", \"remote\", \"get-url\", \"origin\")\n    if rc != 0:\n        print(\"Git repo detected but no 'origin' remote \u2014 adding it...\")\n        _run_git(\"git\", \"remote\", \"add\", \"origin\", _GITHUB_URL)\n        print(f\"  Added origin \u2192 {_GITHUB_URL}\")\n\n    print(\"Pulling latest from GitHub...\")\n    out, err, rc = _run_git(\"git\", \"pull\", \"--ff-only\", \"origin\", \"main\")\n    if rc != 0:\n        # Try without specifying branch (maybe default branch differs)\n        out, err, rc = _run_git(\"git\", \"pull\", \"--ff-only\")\n    print(out)\n    if rc != 0:\n        print(f\"git pull failed (exit {rc}):\")\n        print(err)\n        print(\"\\nTo fix: delete and re-clone:\")\n        print(f\"  !rm -rf '{_REPO}'\")\n        print(\"  Then re-run this cell.\")\n    else:\n        out, _, _ = _run_git(\"git\", \"log\", \"--oneline\", \"-1\")\n        print(f\"Current commit: {out}\")\n\nelif os.path.isdir(os.path.dirname(_REPO)):\n    # Parent dir exists but repo not cloned yet \u2014 clone it\n    print(f\"Cloning repo from GitHub (first time)...\")\n    r = subprocess.run(\n        [\"git\", \"clone\", _GITHUB_URL, _REPO],\n        capture_output=True, text=True, timeout=300,\n    )\n    print(r.stdout.strip())\n    if r.returncode != 0:\n        print(f\"Clone failed: {r.stderr.strip()}\")\n    else:\n        out, _, _ = _run_git(\"git\", \"log\", \"--oneline\", \"-1\")\n        print(f\"Cloned successfully. Current commit: {out}\")\nelse:\n    print(f\"Parent directory not found: {os.path.dirname(_REPO)}\")\n    print(\"Make sure Google Drive is mounted and the path is correct.\")\n    print(f\"Expected: {_REPO}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Path to the superconductor-vae repo on your Google Drive\nREPO_PATH = \"/content/drive/My Drive/Colab Notebooks/SuperconductorVAE/superconductor-vae\"\n\n# Checkpoint to evaluate (uses Drive-based outputs/)\nCHECKPOINT = 'auto'  # 'auto' finds checkpoint_best.pt, or set explicit path\n\n# --- Phase 2 training + discovery (the main event) ---\n# Phase 2 generates formulas as part of self-supervised training.\n# Every formula is checked against holdout set + training set.\n# This IS the search \u2014 training and discovery happen simultaneously.\nPHASE2_EPOCHS = 20          # Number of Phase 2 sub-epochs (set 0 for inference-only)\nPHASE2_LR = 1e-5            # Learning rate for Phase 2 fine-tuning\nPHASE2_HOLDOUT_INTERVAL = 5 # Run mini holdout search every N sub-epochs\n\n# --- Search budget (for post-Phase-2 targeted holdout search) ---\n# 'auto' scales with GPU VRAM. Override with 'small', 'medium', 'large', 'xlarge'.\nSEARCH_BUDGET = 'auto'\n\n# --- Novel discovery (for post-Phase-2 broad Z-space exploration) ---\n# 'auto' scales with VRAM: T4=10K, A100-40=50K, A100-80=100K\nNOVEL_N_SAMPLES = 'auto'\n\n# Minimum predicted Tc (K) to report as novel candidate\nNOVEL_MIN_TC = 10.0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q scipy matminer pymatgen scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Setup Paths and Verify Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "repo = Path(REPO_PATH)\n",
    "\n",
    "# Add src/ to Python path\n",
    "src_path = str(repo / 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "# Verify key files\n",
    "required_files = {\n",
    "    'Training data': repo / 'data/processed/supercon_fractions_contrastive.csv',\n",
    "    'Holdout set': repo / 'data/GENERATIVE_HOLDOUT_DO_NOT_TRAIN.json',\n",
    "    'VAE model': repo / 'src/superconductor/models/attention_vae.py',\n",
    "    'Decoder model': repo / 'src/superconductor/models/autoregressive_decoder.py',\n",
    "}\n",
    "optional_files = {\n",
    "    'Best checkpoint': repo / 'outputs/checkpoint_best.pt',\n",
    "    'Z-cache': repo / 'outputs/latent_cache.pt',\n",
    "    'Phase 2 discoveries': repo / 'outputs/phase2_discoveries.jsonl',\n",
    "}\n",
    "\n",
    "all_found = True\n",
    "for name, path in required_files.items():\n",
    "    exists = path.exists()\n",
    "    status = 'OK' if exists else 'MISSING'\n",
    "    print(f'  [{status}] {name}: {path.name}')\n",
    "    if not exists:\n",
    "        all_found = False\n",
    "\n",
    "print()\n",
    "for name, path in optional_files.items():\n",
    "    exists = path.exists()\n",
    "    status = 'OK' if exists else '---'\n",
    "    print(f'  [{status}] {name}: {path.name}')\n",
    "\n",
    "if not all_found:\n",
    "    raise FileNotFoundError(f'Missing required files. Check REPO_PATH: {REPO_PATH}')\n",
    "\n",
    "# Resolve checkpoint path\n",
    "if CHECKPOINT == 'auto':\n",
    "    CHECKPOINT_PATH = str(repo / 'outputs/checkpoint_best.pt')\n",
    "    if not os.path.exists(CHECKPOINT_PATH):\n",
    "        epoch_files = sorted((repo / 'outputs').glob('checkpoint_epoch_*.pt'))\n",
    "        if epoch_files:\n",
    "            CHECKPOINT_PATH = str(epoch_files[-1])\n",
    "        else:\n",
    "            raise FileNotFoundError('No checkpoint found in outputs/')\n",
    "else:\n",
    "    CHECKPOINT_PATH = str(repo / CHECKPOINT)\n",
    "\n",
    "print(f'\\nCheckpoint: {Path(CHECKPOINT_PATH).name}')\n",
    "print(f'Size: {os.path.getsize(CHECKPOINT_PATH)/1e6:.1f} MB')\n",
    "\n",
    "# GPU info + VRAM-aware scaling\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "VRAM_GB = 0.0\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    VRAM_GB = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    print(f'\\nGPU: {gpu_name}')\n",
    "    print(f'VRAM: {VRAM_GB:.1f} GB')\n",
    "else:\n",
    "    print('\\nWARNING: No GPU detected. Running on CPU (slow).')\n",
    "\n",
    "# --- Auto-scale search parameters based on VRAM ---\n",
    "if SEARCH_BUDGET == 'auto':\n",
    "    if VRAM_GB >= 70:\n",
    "        SEARCH_BUDGET = 'xlarge'\n",
    "    elif VRAM_GB >= 38:\n",
    "        SEARCH_BUDGET = 'large'\n",
    "    elif VRAM_GB >= 14:\n",
    "        SEARCH_BUDGET = 'medium'\n",
    "    else:\n",
    "        SEARCH_BUDGET = 'small'\n",
    "\n",
    "BUDGET_CONFIGS = {\n",
    "    'small':  {'encode_batch': 128, 'decode_batch': 64,  'n_perturbations': 50,\n",
    "               'noise_scales': [0.05, 0.1, 0.2, 0.3], 'n_seeds': 20,\n",
    "               'n_interp_steps': 10, 'max_pairs': 50,\n",
    "               'n_temp_samples': 15, 'temps': [0.01, 0.1, 0.3, 0.5],\n",
    "               'n_pca_comps': 10, 'pca_steps': 10, 'n_neighbors': 60},\n",
    "    'medium': {'encode_batch': 256, 'decode_batch': 128, 'n_perturbations': 100,\n",
    "               'noise_scales': [0.02, 0.05, 0.08, 0.1, 0.15, 0.2, 0.3, 0.5], 'n_seeds': 30,\n",
    "               'n_interp_steps': 15, 'max_pairs': 100,\n",
    "               'n_temp_samples': 30, 'temps': [0.01, 0.05, 0.1, 0.2, 0.3, 0.5, 0.7, 1.0],\n",
    "               'n_pca_comps': 20, 'pca_steps': 20, 'n_neighbors': 100},\n",
    "    'large':  {'encode_batch': 512, 'decode_batch': 256, 'n_perturbations': 200,\n",
    "               'noise_scales': [0.02, 0.05, 0.08, 0.1, 0.12, 0.15, 0.2, 0.3, 0.4, 0.5],\n",
    "               'n_seeds': 50, 'n_interp_steps': 20, 'max_pairs': 150,\n",
    "               'n_temp_samples': 50, 'temps': [0.01, 0.03, 0.05, 0.1, 0.15, 0.2, 0.3, 0.5, 0.7, 1.0],\n",
    "               'n_pca_comps': 30, 'pca_steps': 30, 'n_neighbors': 150},\n",
    "    'xlarge': {'encode_batch': 1024, 'decode_batch': 512, 'n_perturbations': 300,\n",
    "               'noise_scales': [0.01, 0.02, 0.04, 0.06, 0.08, 0.1, 0.12, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5],\n",
    "               'n_seeds': 80, 'n_interp_steps': 25, 'max_pairs': 200,\n",
    "               'n_temp_samples': 80, 'temps': [0.01, 0.02, 0.05, 0.08, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 0.7, 1.0],\n",
    "               'n_pca_comps': 40, 'pca_steps': 40, 'n_neighbors': 200},\n",
    "}\n",
    "B = BUDGET_CONFIGS[SEARCH_BUDGET]\n",
    "\n",
    "if NOVEL_N_SAMPLES == 'auto':\n",
    "    NOVEL_N_SAMPLES = int(max(5000, min(100000, VRAM_GB * 1250)))\n",
    "\n",
    "print(f'\\nSearch budget: {SEARCH_BUDGET}')\n",
    "print(f'  encode_batch={B[\"encode_batch\"]}, decode_batch={B[\"decode_batch\"]}')\n",
    "print(f'  seeds={B[\"n_seeds\"]}, perturbations={B[\"n_perturbations\"]}, noise_scales={len(B[\"noise_scales\"])}')\n",
    "print(f'  temps={len(B[\"temps\"])}, temp_samples={B[\"n_temp_samples\"]}')\n",
    "print(f'Novel discovery: {NOVEL_N_SAMPLES:,} Z-space samples')\n",
    "print(f'Phase 2 epochs: {PHASE2_EPOCHS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Constants & Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from superconductor.models.attention_vae import FullMaterialsVAE\nfrom superconductor.models.autoregressive_decoder import (\n    EnhancedTransformerDecoder, IDX_TO_TOKEN, TOKEN_TO_IDX,\n    PAD_IDX, START_IDX, END_IDX,\n)\nfrom superconductor.data.canonical_ordering import CanonicalOrderer\nfrom superconductor.tokenizer.fraction_tokenizer import FractionAwareTokenizer\n\n# V13+ tokenizer for semantic fraction + isotope tokens (vocab 4752)\n_frac_vocab = str(repo / 'data' / 'fraction_vocab.json')\n_iso_vocab = str(repo / 'data' / 'isotope_vocab.json')\nV13_TOKENIZER = FractionAwareTokenizer(_frac_vocab, isotope_vocab_path=_iso_vocab)\nprint(f'V13 tokenizer: vocab_size={V13_TOKENIZER.vocab_size}')\n\nPROJECT_ROOT = repo\nHOLDOUT_PATH = repo / 'data' / 'GENERATIVE_HOLDOUT_DO_NOT_TRAIN.json'\nCACHE_DIR = repo / 'data' / 'processed' / 'cache'\nOUTPUT_DIR = repo / 'outputs'\n\n# TC normalization constants \u2014 loaded from cache_meta.json after data loading.\n# These are placeholder defaults; overwritten in Cell 6 (Load Model & Data).\nTC_MEAN = 2.725219433789196\nTC_STD = 1.3527019896187407\n\nFAMILY_14_NAMES = [\n    'NOT_SC', 'BCS_CONVENTIONAL', 'CUPRATE_YBCO', 'CUPRATE_LSCO',\n    'CUPRATE_BSCCO', 'CUPRATE_TBCCO', 'CUPRATE_HBCCO', 'CUPRATE_OTHER',\n    'IRON_PNICTIDE', 'IRON_CHALCOGENIDE', 'MGB2_TYPE',\n    'HEAVY_FERMION', 'ORGANIC', 'OTHER_UNKNOWN',\n]\n\nHOLDOUT_FAMILY_TO_14 = {\n    'YBCO': 2, 'LSCO': 3, 'Hg-cuprate': 6, 'Tl-cuprate': 5,\n    'Bi-cuprate': 4, 'Iron-based': 8, 'MgB2': 10,\n    'Conventional': 1, 'Other': 13,\n}\n\nELEMENT_TO_Z = {\n    'H':1,'He':2,'Li':3,'Be':4,'B':5,'C':6,'N':7,'O':8,'F':9,'Ne':10,\n    'Na':11,'Mg':12,'Al':13,'Si':14,'P':15,'S':16,'Cl':17,'Ar':18,'K':19,'Ca':20,\n    'Sc':21,'Ti':22,'V':23,'Cr':24,'Mn':25,'Fe':26,'Co':27,'Ni':28,'Cu':29,'Zn':30,\n    'Ga':31,'Ge':32,'As':33,'Se':34,'Br':35,'Kr':36,'Rb':37,'Sr':38,'Y':39,'Zr':40,\n    'Nb':41,'Mo':42,'Tc':43,'Ru':44,'Rh':45,'Pd':46,'Ag':47,'Cd':48,'In':49,'Sn':50,\n    'Sb':51,'Te':52,'I':53,'Xe':54,'Cs':55,'Ba':56,'La':57,'Ce':58,'Pr':59,'Nd':60,\n    'Pm':61,'Sm':62,'Eu':63,'Gd':64,'Tb':65,'Dy':66,'Ho':67,'Er':68,'Tm':69,'Yb':70,\n    'Lu':71,'Hf':72,'Ta':73,'W':74,'Re':75,'Os':76,'Ir':77,'Pt':78,'Au':79,'Hg':80,\n    'Tl':81,'Pb':82,'Bi':83,'Po':84,'At':85,'Rn':86,'Fr':87,'Ra':88,'Ac':89,'Th':90,\n    'Pa':91,'U':92,\n}\nZ_TO_ELEMENT = {v: k for k, v in ELEMENT_TO_Z.items()}\n_CANONICALIZER = CanonicalOrderer()\n\n\ndef tokens_to_formula(token_ids):\n    \"\"\"Convert token IDs to formula string using V13+ tokenizer.\"\"\"\n    ids = [int(t) for t in token_ids]\n    return V13_TOKENIZER.decode(ids, strip_special=True)\n\n\ndef denormalize_tc(tc_norm):\n    \"\"\"Convert normalized Tc prediction back to Kelvin.\"\"\"\n    tc_log = tc_norm * TC_STD + TC_MEAN\n    return max(0.0, float(np.expm1(tc_log)))\n\n\ndef parse_formula_elements(formula):\n    \"\"\"Extract {element: fraction_value} from formula string.\"\"\"\n    try:\n        elements = _CANONICALIZER.parse_formula(formula)\n        if not elements:\n            return {}\n        result = {}\n        for ef in elements:\n            result[ef.element] = result.get(ef.element, 0) + ef.fraction_value\n        return result\n    except Exception:\n        return {}\n\n\ndef element_similarity(formula_a, formula_b):\n    \"\"\"Compositional similarity: Jaccard on elements + fraction overlap.\"\"\"\n    parsed_a = parse_formula_elements(formula_a)\n    parsed_b = parse_formula_elements(formula_b)\n    if not parsed_a or not parsed_b:\n        return 0.0\n    all_elements = set(parsed_a.keys()) | set(parsed_b.keys())\n    shared = set(parsed_a.keys()) & set(parsed_b.keys())\n    if not all_elements:\n        return 0.0\n    jaccard = len(shared) / len(all_elements)\n    frac_sim = 0.0\n    if shared:\n        total_a = sum(parsed_a.values())\n        total_b = sum(parsed_b.values())\n        for elem in shared:\n            fa = parsed_a[elem] / max(total_a, 1e-8)\n            fb = parsed_b[elem] / max(total_b, 1e-8)\n            frac_sim += min(fa, fb)\n    return 0.5 * jaccard + 0.5 * frac_sim\n\n\ndef element_overlap_score(target_elements, cache_elem_idx):\n    \"\"\"Score a training sample by how many target elements it contains.\"\"\"\n    candidate_elements = set(int(z) for z in cache_elem_idx if z > 0)\n    shared_elems = target_elements & candidate_elements\n    all_elem = target_elements | candidate_elements\n    if not all_elem:\n        return (0, 0.0)\n    return (len(shared_elems), len(shared_elems) / len(all_elem))\n\n\ndef slerp(z1, z2, t):\n    \"\"\"Spherical linear interpolation.\"\"\"\n    z1_norm = F.normalize(z1, dim=-1)\n    z2_norm = F.normalize(z2, dim=-1)\n    omega = torch.acos(torch.clamp(\n        (z1_norm * z2_norm).sum(dim=-1, keepdim=True), -1.0, 1.0\n    ))\n    omega = omega.clamp(min=1e-6)\n    sin_omega = torch.sin(omega)\n    if sin_omega.abs().min() < 1e-6:\n        return (1 - t) * z1 + t * z2\n    s1 = torch.sin((1 - t) * omega) / sin_omega\n    s2 = torch.sin(t * omega) / sin_omega\n    mag1 = z1.norm(dim=-1, keepdim=True)\n    mag2 = z2.norm(dim=-1, keepdim=True)\n    mag = (1 - t) * mag1 + t * mag2\n    return (s1 * z1_norm + s2 * z2_norm) * mag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Load Model & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_models(checkpoint_path):\n    \"\"\"Load encoder and decoder from checkpoint.\"\"\"\n    print(f'Loading checkpoint: {Path(checkpoint_path).name}')\n    checkpoint = torch.load(checkpoint_path, map_location=DEVICE, weights_only=False)\n\n    enc_state_raw = checkpoint.get('encoder_state_dict', {})\n    magpie_dim = 145\n    for k, v in enc_state_raw.items():\n        if 'magpie_encoder' in k and k.endswith('.weight') and v.dim() == 2:\n            magpie_dim = v.shape[1]\n            break\n\n    enc_state = {k.replace('_orig_mod.', ''): v for k, v in enc_state_raw.items()}\n\n    has_numden_head = any('numden_head.' in k for k in enc_state)\n    old_numden_arch = False\n    if 'numden_head.0.weight' in enc_state:\n        if enc_state['numden_head.0.weight'].shape[0] == 128:\n            old_numden_arch = True\n\n    encoder = FullMaterialsVAE(\n        n_elements=118, element_embed_dim=128, n_attention_heads=8,\n        magpie_dim=magpie_dim, fusion_dim=256, encoder_hidden=[512, 256],\n        latent_dim=2048, decoder_hidden=[256, 512], dropout=0.1\n    ).to(DEVICE)\n\n    if old_numden_arch:\n        import torch.nn as nn\n        encoder.numden_head = nn.Sequential(\n            nn.Linear(2048, 128), nn.ReLU(), nn.Linear(128, encoder.max_elements * 2),\n        ).to(DEVICE)\n\n    encoder.load_state_dict(enc_state, strict=False)\n\n    dec_state_raw = checkpoint.get('decoder_state_dict', {})\n    dec_state = {k.replace('_orig_mod.', ''): v for k, v in dec_state_raw.items()}\n\n    stoich_weight_key = 'stoich_to_memory.0.weight'\n    stoich_dim = dec_state[stoich_weight_key].shape[1] if stoich_weight_key in dec_state else 37\n\n    dec_vocab_size = checkpoint.get('tokenizer_vocab_size', None)\n    if dec_vocab_size is None and 'token_embedding.weight' in dec_state:\n        dec_vocab_size = dec_state['token_embedding.weight'].shape[0]\n\n    _d_model = checkpoint.get('d_model', None)\n    if _d_model is None and 'token_embedding.weight' in dec_state:\n        _d_model = dec_state['token_embedding.weight'].shape[1]\n    _d_model = _d_model or 512\n    _dim_ff = checkpoint.get('dim_feedforward', None)\n    if _dim_ff is None and 'transformer_decoder.layers.0.linear1.weight' in dec_state:\n        _dim_ff = dec_state['transformer_decoder.layers.0.linear1.weight'].shape[0]\n    _dim_ff = _dim_ff or 2048\n    _nhead = checkpoint.get('nhead', 8)\n    _num_layers = checkpoint.get('num_layers', 12)\n    # Auto-detect max_len from PE buffer shape (most reliable), then checkpoint key, then default\n    _max_len = checkpoint.get('max_formula_len', None)\n    if _max_len is None and 'pos_encoding.pe' in dec_state:\n        _max_len = dec_state['pos_encoding.pe'].shape[1]\n    _max_len = _max_len or 90  # Safe fallback: covers all formula lengths\n\n    decoder = EnhancedTransformerDecoder(\n        latent_dim=2048, d_model=_d_model, nhead=_nhead, num_layers=_num_layers,\n        dim_feedforward=_dim_ff, dropout=0.1, max_len=_max_len,\n        n_memory_tokens=16, encoder_skip_dim=256,\n        use_skip_connection=False, use_stoich_conditioning=True,\n        max_elements=12, n_stoich_tokens=4,\n        vocab_size=dec_vocab_size, stoich_input_dim=stoich_dim,\n    ).to(DEVICE)\n    decoder.load_state_dict(dec_state, strict=False)\n\n    encoder.eval()\n    decoder.eval()\n\n    epoch = checkpoint.get('epoch', '?')\n    best_exact = checkpoint.get('best_exact', 0)\n    print(f'  Epoch {epoch}, best_exact={best_exact:.4f}, magpie_dim={magpie_dim}, '\n          f'd_model={_d_model}, dim_ff={_dim_ff}, max_len={_max_len}, vocab={dec_vocab_size}')\n    return encoder, decoder, magpie_dim, has_numden_head, epoch\n\n\ndef _build_cache_if_needed():\n    \"\"\"Build data cache from CSV if it doesn't exist (one-time, ~5 min).\"\"\"\n    if (CACHE_DIR / 'cache_meta.json').exists():\n        # Validate cache was built with V13 tokenization (not stale V12)\n        _meta = json.load(open(CACHE_DIR / 'cache_meta.json'))\n        if _meta.get('use_semantic_fractions', False):\n            return  # Cache is V13, good\n        print('WARNING: Existing cache uses V12 tokenization. Rebuilding for V13...')\n    print('=' * 60)\n    print('DATA CACHE NOT FOUND \\u2014 building from CSV (one-time, ~5 min)')\n    print('=' * 60)\n    _scripts_dir = str(repo)\n    if _scripts_dir not in sys.path:\n        sys.path.insert(0, _scripts_dir)\n    import scripts.train_v12_clean as _train\n    _train.TRAIN_CONFIG['use_semantic_fractions'] = True\n    _train.TRAIN_CONFIG['use_isotope_tokens'] = True\n    _train.TRAIN_CONFIG['max_formula_len'] = 30\n    _train.TRAIN_CONFIG['contrastive_mode'] = True\n    _train.TRAIN_CONFIG['num_epochs'] = 0\n    _result = _train.load_and_prepare_data()\n    del _result\n    print('Cache built successfully.')\n\n\ndef load_data(magpie_dim):\n    \"\"\"Load cached tensors. Builds cache from CSV if needed (one-time).\"\"\"\n    _build_cache_if_needed()\n    data = {\n        'elem_idx': torch.load(CACHE_DIR / 'element_indices.pt', map_location='cpu', weights_only=True),\n        'elem_frac': torch.load(CACHE_DIR / 'element_fractions.pt', map_location='cpu', weights_only=True),\n        'elem_mask': torch.load(CACHE_DIR / 'element_mask.pt', map_location='cpu', weights_only=True),\n        'tc': torch.load(CACHE_DIR / 'tc_tensor.pt', map_location='cpu', weights_only=True),\n        'magpie': torch.load(CACHE_DIR / 'magpie_tensor.pt', map_location='cpu', weights_only=True),\n        'is_sc': torch.load(CACHE_DIR / 'is_sc_tensor.pt', map_location='cpu', weights_only=True),\n        'tokens': torch.load(CACHE_DIR / 'formula_tokens.pt', map_location='cpu', weights_only=True),\n    }\n    if data['magpie'].shape[1] > magpie_dim:\n        data['magpie'] = data['magpie'][:, :magpie_dim]\n    meta = json.load(open(CACHE_DIR / 'cache_meta.json'))\n    data['train_indices'] = meta.get('train_indices', list(range(len(data['elem_idx']))))\n    print(f'  {len(data[\"elem_idx\"])} total samples, {len(data[\"train_indices\"])} train')\n    return data, meta\n\n\n# Load everything\nencoder, decoder, magpie_dim, has_numden_head, model_epoch = load_models(CHECKPOINT_PATH)\ndata, cache_meta = load_data(magpie_dim)\n\n# Override TC normalization constants from cache metadata (more accurate than hardcoded)\nTC_MEAN = cache_meta.get('tc_mean', TC_MEAN)\nTC_STD = cache_meta.get('tc_std', TC_STD)\nprint(f'  TC normalization: mean={TC_MEAN:.6f}, std={TC_STD:.6f}')\n\nwith open(HOLDOUT_PATH) as f:\n    holdout = json.load(f)\nholdout_samples = holdout['holdout_samples']\nprint(f'\\n{len(holdout_samples)} holdout materials loaded')\nprint(f'Families: {sorted(set(s[\"family\"] for s in holdout_samples))}')\n\n# Build training formula set for novelty checking\nprint('Building training formula index...')\ntraining_formulas = set()\nfor i in data['train_indices']:\n    f = tokens_to_formula(data['tokens'][i])\n    if f:\n        training_formulas.add(f.strip())\nholdout_formulas = set(s['formula'].strip() for s in holdout_samples)\nprint(f'  {len(training_formulas)} unique training formulas, {len(holdout_formulas)} holdout')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Core Encode/Decode Functions (VRAM-scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def encode_indices(encoder, data, indices):\n",
    "    \"\"\"Encode dataset indices -> Z vectors. Batch size scales with VRAM.\"\"\"\n",
    "    batch_size = B['encode_batch']\n",
    "    all_z = []\n",
    "    for start in range(0, len(indices), batch_size):\n",
    "        batch_idx = indices[start:start + batch_size]\n",
    "        idx_t = torch.tensor(batch_idx, dtype=torch.long)\n",
    "        result = encoder.encode(\n",
    "            data['elem_idx'][idx_t].to(DEVICE),\n",
    "            data['elem_frac'][idx_t].to(DEVICE),\n",
    "            data['elem_mask'][idx_t].to(DEVICE),\n",
    "            data['magpie'][idx_t].to(DEVICE),\n",
    "            data['tc'][idx_t].to(DEVICE),\n",
    "        )\n",
    "        all_z.append(result['z'].cpu())\n",
    "    return torch.cat(all_z, dim=0)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def decode_z_batch(encoder, decoder, z_batch, temperature=0.01):\n",
    "    \"\"\"Decode Z vectors -> formula strings. Batch size scales with VRAM.\"\"\"\n",
    "    batch_size = B['decode_batch']\n",
    "    all_formulas = []\n",
    "    for start in range(0, len(z_batch), batch_size):\n",
    "        z = z_batch[start:start + batch_size].to(DEVICE)\n",
    "        fraction_output = encoder.fraction_head(z)\n",
    "        fraction_pred = fraction_output[:, :encoder.max_elements]\n",
    "        element_count_pred = fraction_output[:, -1]\n",
    "        if has_numden_head and hasattr(encoder, 'numden_head'):\n",
    "            numden_pred = encoder.numden_head(z)\n",
    "            stoich_pred = torch.cat([fraction_pred, numden_pred, element_count_pred.unsqueeze(-1)], dim=-1)\n",
    "        else:\n",
    "            stoich_pred = torch.cat([fraction_pred, element_count_pred.unsqueeze(-1)], dim=-1)\n",
    "        generated, _, _ = decoder.generate_with_kv_cache(\n",
    "            z=z, stoich_pred=stoich_pred, temperature=temperature,\n",
    "        )\n",
    "        for i in range(len(z)):\n",
    "            all_formulas.append(tokens_to_formula(generated[i]))\n",
    "    return all_formulas\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def decode_z_with_tc(encoder, decoder, z_batch, temperature=0.01):\n",
    "    \"\"\"Decode Z vectors -> (formula, tc_kelvin, sc_prob) tuples.\"\"\"\n",
    "    batch_size = B['decode_batch']\n",
    "    results = []\n",
    "    for start in range(0, len(z_batch), batch_size):\n",
    "        z = z_batch[start:start + batch_size].to(DEVICE)\n",
    "        # Get Tc + SC predictions from encoder heads\n",
    "        tc_pred_norm = encoder.tc_head(z).squeeze(-1)\n",
    "        sc_logits = encoder.sc_head(z).squeeze(-1) if hasattr(encoder, 'sc_head') else None\n",
    "        # Get family prediction\n",
    "        family_pred = None\n",
    "        if hasattr(encoder, 'hierarchical_family_head'):\n",
    "            fam_logits = encoder.hierarchical_family_head(z)\n",
    "            family_pred = fam_logits.argmax(dim=-1)\n",
    "        # Decode formula\n",
    "        fraction_output = encoder.fraction_head(z)\n",
    "        fraction_pred = fraction_output[:, :encoder.max_elements]\n",
    "        element_count_pred = fraction_output[:, -1]\n",
    "        if has_numden_head and hasattr(encoder, 'numden_head'):\n",
    "            numden_pred = encoder.numden_head(z)\n",
    "            stoich_pred = torch.cat([fraction_pred, numden_pred, element_count_pred.unsqueeze(-1)], dim=-1)\n",
    "        else:\n",
    "            stoich_pred = torch.cat([fraction_pred, element_count_pred.unsqueeze(-1)], dim=-1)\n",
    "        generated, _, _ = decoder.generate_with_kv_cache(\n",
    "            z=z, stoich_pred=stoich_pred, temperature=temperature,\n",
    "        )\n",
    "        for i in range(len(z)):\n",
    "            formula = tokens_to_formula(generated[i])\n",
    "            tc_k = denormalize_tc(tc_pred_norm[i].item())\n",
    "            sc_p = torch.sigmoid(sc_logits[i]).item() if sc_logits is not None else 0.5\n",
    "            fam = int(family_pred[i].item()) if family_pred is not None else -1\n",
    "            results.append({'formula': formula, 'tc_kelvin': tc_k, 'sc_prob': sc_p, 'family': fam})\n",
    "    return results\n",
    "\n",
    "\n",
    "def find_element_neighbors(target_formula, data, top_k=100):\n",
    "    \"\"\"Find training samples sharing elements with target.\"\"\"\n",
    "    parsed = parse_formula_elements(target_formula)\n",
    "    if not parsed:\n",
    "        return []\n",
    "    target_atomic_nums = set()\n",
    "    for elem in parsed.keys():\n",
    "        z = ELEMENT_TO_Z.get(elem)\n",
    "        if z:\n",
    "            target_atomic_nums.add(z)\n",
    "    scores = []\n",
    "    for i in data['train_indices']:\n",
    "        n_shared, jaccard = element_overlap_score(target_atomic_nums, data['elem_idx'][i])\n",
    "        if n_shared > 0:\n",
    "            scores.append((i, n_shared, jaccard))\n",
    "    scores.sort(key=lambda x: (-x[1], -x[2]))\n",
    "    return [s[0] for s in scores[:top_k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# PART 1: Baseline Roundtrip Validation (Pre-Phase-2)\n\nEncode each holdout target \u2192 decode \u2192 check roundtrip fidelity.\nThis establishes the baseline before Phase 2 training improves the model.\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Roundtrip Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def full_forward(encoder, decoder, elem_idx, elem_frac, elem_mask, magpie, tc, temperature=0.01):\n",
    "    \"\"\"Run full encoder forward (all heads) + formula decoder.\"\"\"\n",
    "    enc_out = encoder(\n",
    "        elem_idx.to(DEVICE), elem_frac.to(DEVICE), elem_mask.to(DEVICE),\n",
    "        magpie.to(DEVICE), tc.to(DEVICE),\n",
    "    )\n",
    "    z = enc_out['z']\n",
    "    tc_pred_norm = enc_out['tc_pred'][0].item()\n",
    "    magpie_pred = enc_out['magpie_pred'][0].cpu()\n",
    "    fraction_pred = enc_out['fraction_pred']\n",
    "    element_count_pred = enc_out['element_count_pred']\n",
    "    numden_pred = enc_out.get('numden_pred')\n",
    "    if numden_pred is not None:\n",
    "        stoich_pred = torch.cat([fraction_pred, numden_pred, element_count_pred.unsqueeze(-1)], dim=-1)\n",
    "    else:\n",
    "        stoich_pred = torch.cat([fraction_pred, element_count_pred.unsqueeze(-1)], dim=-1)\n",
    "    generated, log_probs, entropy = decoder.generate_with_kv_cache(\n",
    "        z=z, stoich_pred=stoich_pred, temperature=temperature,\n",
    "    )\n",
    "    result = {\n",
    "        'formula': tokens_to_formula(generated[0]),\n",
    "        'tc_pred_kelvin': denormalize_tc(tc_pred_norm),\n",
    "        'magpie_pred': magpie_pred,\n",
    "    }\n",
    "    tc_class_logits = enc_out.get('tc_class_logits')\n",
    "    if tc_class_logits is not None:\n",
    "        result['tc_class'] = torch.softmax(tc_class_logits[0].cpu(), dim=-1).argmax().item()\n",
    "    sc_pred = enc_out.get('sc_pred')\n",
    "    if sc_pred is not None:\n",
    "        result['sc_prob'] = torch.sigmoid(sc_pred[0]).item()\n",
    "        result['sc_pred'] = result['sc_prob'] > 0.5\n",
    "    family_composed = enc_out.get('family_composed_14')\n",
    "    if family_composed is not None:\n",
    "        result['family_pred_14'] = family_composed[0].cpu().argmax().item()\n",
    "    hp_pred = enc_out.get('hp_pred')\n",
    "    if hp_pred is not None:\n",
    "        result['hp_prob'] = torch.sigmoid(hp_pred[0]).item()\n",
    "    return result\n",
    "\n",
    "\n",
    "# Run roundtrip validation\n",
    "print('=' * 80)\n",
    "print(f'ROUNDTRIP VALIDATION \u2014 Epoch {model_epoch}')\n",
    "print('=' * 80)\n",
    "print(f'{\"Family\":<14s} {\"True Tc\":>8s} {\"Pred Tc\":>9s} {\"Err\":>7s} {\"Sim\":>5s} {\"SC?\":>5s} {\"FamPred\":<16s} | Formula')\n",
    "print('-' * 120)\n",
    "\n",
    "roundtrip_results = []\n",
    "tc_errors = []\n",
    "magpie_mses = []\n",
    "formula_sims = []\n",
    "sc_correct = 0\n",
    "family_correct = 0\n",
    "\n",
    "for sample in holdout_samples:\n",
    "    formula = sample['formula']\n",
    "    true_tc = sample['Tc']\n",
    "    family = sample['family']\n",
    "    orig_idx = sample.get('original_index')\n",
    "    if orig_idx is None:\n",
    "        continue\n",
    "    idx_t = torch.tensor([orig_idx], dtype=torch.long)\n",
    "    decoded = full_forward(\n",
    "        encoder, decoder,\n",
    "        data['elem_idx'][idx_t], data['elem_frac'][idx_t],\n",
    "        data['elem_mask'][idx_t], data['magpie'][idx_t], data['tc'][idx_t],\n",
    "    )\n",
    "    pred_tc = decoded['tc_pred_kelvin']\n",
    "    gen_formula = decoded['formula']\n",
    "    tc_err = abs(pred_tc - true_tc)\n",
    "    tc_errors.append(tc_err)\n",
    "    mag_mse = F.mse_loss(decoded['magpie_pred'], data['magpie'][orig_idx]).item()\n",
    "    magpie_mses.append(mag_mse)\n",
    "    sim = element_similarity(gen_formula, formula)\n",
    "    formula_sims.append(sim)\n",
    "    exact = gen_formula.strip() == formula.strip()\n",
    "    sc_str = f\"{decoded.get('sc_prob', 0):.2f}\"\n",
    "    if decoded.get('sc_pred', False):\n",
    "        sc_correct += 1\n",
    "    family_pred_str = ''\n",
    "    if 'family_pred_14' in decoded:\n",
    "        pred_idx = decoded['family_pred_14']\n",
    "        pred_name = FAMILY_14_NAMES[pred_idx]\n",
    "        true_idx = HOLDOUT_FAMILY_TO_14.get(family, -1)\n",
    "        match = pred_idx == true_idx\n",
    "        if match:\n",
    "            family_correct += 1\n",
    "        family_pred_str = f\"{pred_name} {'OK' if match else 'X'}\"\n",
    "    exact_str = ' [EXACT]' if exact else ''\n",
    "    print(f'  [{family:<12s}] {true_tc:8.1f} {pred_tc:9.1f} {tc_err:+7.1f} {sim:5.3f} {sc_str:>5s} {family_pred_str:<16s} | {formula}')\n",
    "    if not exact:\n",
    "        print(f'     -> {gen_formula}')\n",
    "    else:\n",
    "        print(f'     -> {gen_formula}{exact_str}')\n",
    "    roundtrip_results.append({\n",
    "        'formula': formula, 'generated': gen_formula, 'exact': exact,\n",
    "        'similarity': sim, 'true_tc': true_tc, 'pred_tc': pred_tc,\n",
    "        'tc_error': tc_err, 'family': family,\n",
    "        'sc_prob': decoded.get('sc_prob'), 'family_pred': family_pred_str,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roundtrip Summary\n",
    "tc_arr = np.array(tc_errors)\n",
    "sim_arr = np.array(formula_sims)\n",
    "mag_arr = np.array(magpie_mses)\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('ROUNDTRIP SUMMARY')\n",
    "print('=' * 80)\n",
    "print(f'\\nTc Prediction:')\n",
    "print(f'  MAE: {tc_arr.mean():.2f} K (median: {np.median(tc_arr):.2f} K)')\n",
    "print(f'  Within 1K: {(tc_arr < 1).sum()}/45 | Within 5K: {(tc_arr < 5).sum()}/45')\n",
    "print(f'\\nFormula Roundtrip:')\n",
    "print(f'  Mean similarity: {sim_arr.mean():.3f}')\n",
    "print(f'  Exact matches: {(sim_arr >= 0.999).sum()}/45')\n",
    "print(f'  > 0.95 similarity: {(sim_arr > 0.95).sum()}/45')\n",
    "print(f'\\nSC Classification: {sc_correct}/45 ({sc_correct/45*100:.1f}%)')\n",
    "print(f'Family Classification: {family_correct}/45 ({family_correct/45*100:.1f}%)')\n",
    "print(f'Magpie MSE: {mag_arr.mean():.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# PHASE 2: Training + Discovery (Main Event)\n\nPhase 2 generates formulas as part of self-supervised training. Each sub-epoch:\n1. Samples Z-vectors (perturbation, SLERP, PCA, element-anchored)\n2. Generates formulas via the decoder\n3. Filters for chemical validity (parse, plausibility, physics, constraints)\n4. **Checks every valid formula against holdout set + training set**\n5. Computes round-trip consistency losses and updates the model\n6. Logs novel discoveries to `outputs/phase2_discoveries.jsonl`\n\nEvery `PHASE2_HOLDOUT_INTERVAL` sub-epochs, runs a mini targeted holdout search.\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_single_target(encoder, decoder, data, target_formula, target_tc, target_family):\n",
    "    \"\"\"Targeted search for a single holdout formula. Budget scales with VRAM.\"\"\"\n",
    "    print(f'\\n  TARGET: {target_formula} (Tc={target_tc}K, {target_family})')\n",
    "\n",
    "    neighbor_indices = find_element_neighbors(target_formula, data, top_k=B['n_neighbors'])\n",
    "    if len(neighbor_indices) < 3:\n",
    "        print(f'    Only {len(neighbor_indices)} neighbors \u2014 skipping')\n",
    "        return {'target': target_formula, 'target_tc': target_tc, 'target_family': target_family,\n",
    "                'best_sim': 0.0, 'best_gen': '', 'n_unique': 0, 'n_total': 0,\n",
    "                'exact': False, 'top_matches': [], 'top_frequent': []}\n",
    "\n",
    "    z_neighbors = encode_indices(encoder, data, neighbor_indices)\n",
    "    z_seeds = z_neighbors[:min(B['n_seeds'], len(z_neighbors))]\n",
    "    print(f'    {len(neighbor_indices)} neighbors, {len(z_seeds)} seeds, Z norm: {z_neighbors.norm(dim=-1).mean():.2f}')\n",
    "\n",
    "    all_candidates_z = []\n",
    "\n",
    "    # Strategy 1: Fine-grained perturbation\n",
    "    for z in z_seeds:\n",
    "        z_exp = z.unsqueeze(0)\n",
    "        for scale in B['noise_scales']:\n",
    "            noise = torch.randn(B['n_perturbations'], z.shape[0]) * scale\n",
    "            all_candidates_z.append(z_exp + noise)\n",
    "\n",
    "    # Strategy 2: Pairwise interpolation (linear + SLERP)\n",
    "    n_seeds = len(z_seeds)\n",
    "    max_pairs = min(n_seeds * (n_seeds - 1) // 2, B['max_pairs'])\n",
    "    pair_count = 0\n",
    "    for i in range(n_seeds):\n",
    "        for j in range(i + 1, n_seeds):\n",
    "            if pair_count >= max_pairs:\n",
    "                break\n",
    "            z1 = z_seeds[i].unsqueeze(0)\n",
    "            z2 = z_seeds[j].unsqueeze(0)\n",
    "            for t in np.linspace(0.05, 0.95, B['n_interp_steps']):\n",
    "                all_candidates_z.append((1 - t) * z1 + t * z2)\n",
    "                all_candidates_z.append(slerp(z1, z2, float(t)))\n",
    "            pair_count += 1\n",
    "        if pair_count >= max_pairs:\n",
    "            break\n",
    "\n",
    "    # Strategy 3: Centroid + random direction walks\n",
    "    centroid = z_seeds.mean(dim=0, keepdim=True)\n",
    "    std = z_seeds.std(dim=0, keepdim=True).clamp(min=1e-6)\n",
    "    for scale in [0.3, 0.5, 1.0, 1.5, 2.0]:\n",
    "        n_dirs = max(30, B['n_seeds'])\n",
    "        directions = torch.randn(n_dirs, z_seeds.shape[1])\n",
    "        directions = F.normalize(directions, dim=-1)\n",
    "        all_candidates_z.append(centroid + scale * std * directions)\n",
    "\n",
    "    # Strategy 4: PCA-directed walks\n",
    "    if len(z_neighbors) >= 10:\n",
    "        z_np = z_neighbors.numpy()\n",
    "        mean = z_np.mean(axis=0)\n",
    "        centered = z_np - mean\n",
    "        U, S, Vt = np.linalg.svd(centered, full_matrices=False)\n",
    "        n_comp = min(B['n_pca_comps'], len(S))\n",
    "        for c in range(n_comp):\n",
    "            direction = torch.from_numpy(Vt[c]).float()\n",
    "            std_along = S[c] / np.sqrt(len(z_neighbors) - 1)\n",
    "            for alpha in np.linspace(-3.0, 3.0, B['pca_steps']):\n",
    "                z_walked = torch.from_numpy(mean).float().unsqueeze(0) + alpha * std_along * direction.unsqueeze(0)\n",
    "                all_candidates_z.append(z_walked)\n",
    "\n",
    "    all_z = torch.cat(all_candidates_z, dim=0)\n",
    "    print(f'    {len(all_z)} Z candidates (greedy decode)...')\n",
    "\n",
    "    # Greedy decode\n",
    "    t0 = time.time()\n",
    "    greedy_formulas = decode_z_batch(encoder, decoder, all_z, temperature=0.01)\n",
    "    print(f'    Greedy decoded in {time.time()-t0:.1f}s')\n",
    "\n",
    "    # Temperature sampling\n",
    "    temp_formulas = []\n",
    "    for temp in B['temps']:\n",
    "        for z_idx in range(min(len(z_seeds), B['n_seeds'] // 2)):\n",
    "            z_repeated = z_seeds[z_idx].unsqueeze(0).repeat(B['n_temp_samples'], 1)\n",
    "            temp_formulas.extend(decode_z_batch(encoder, decoder, z_repeated, temperature=temp))\n",
    "\n",
    "    all_generated = greedy_formulas + temp_formulas\n",
    "    unique_formulas = set(f for f in all_generated if f and len(f) > 1)\n",
    "\n",
    "    # Score candidates\n",
    "    formula_counts = defaultdict(int)\n",
    "    for f in all_generated:\n",
    "        if f and len(f) > 1:\n",
    "            formula_counts[f] += 1\n",
    "\n",
    "    best_sim = 0.0\n",
    "    best_gen = ''\n",
    "    top_matches = []\n",
    "    candidates = []\n",
    "\n",
    "    for formula_gen, count in formula_counts.items():\n",
    "        sim = element_similarity(formula_gen, target_formula)\n",
    "        candidates.append({'formula': formula_gen, 'count': count, 'similarity': sim})\n",
    "        if sim > best_sim:\n",
    "            best_sim = sim\n",
    "            best_gen = formula_gen\n",
    "        if sim >= 0.8:\n",
    "            top_matches.append((formula_gen, sim, count))\n",
    "\n",
    "    candidates.sort(key=lambda x: -x['similarity'])\n",
    "    top_matches.sort(key=lambda x: -x[1])\n",
    "    target_norm = target_formula.strip()\n",
    "    exact = any(f.strip() == target_norm for f in unique_formulas)\n",
    "    top_freq = sorted([(c['formula'], c['count']) for c in candidates], key=lambda x: -x[1])[:5]\n",
    "\n",
    "    print(f'    Total: {len(all_generated)}, unique: {len(unique_formulas)}')\n",
    "    print(f'    RESULT: best_sim={best_sim:.4f}, exact={\"YES\" if exact else \"no\"}')\n",
    "    if top_matches[:3]:\n",
    "        for f, s, c in top_matches[:3]:\n",
    "            print(f'      sim={s:.4f} ({c}x): {f}')\n",
    "\n",
    "    return {\n",
    "        'target': target_formula, 'target_tc': target_tc, 'target_family': target_family,\n",
    "        'exact': exact, 'best_sim': float(best_sim), 'best_gen': best_gen,\n",
    "        'n_unique': len(unique_formulas), 'n_total': len(all_generated),\n",
    "        'n_neighbors': len(neighbor_indices),\n",
    "        'top_matches': [(f, float(s)) for f, s, c in top_matches[:10]],\n",
    "        'top_frequent': top_freq,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## PHASE 2: Training + Discovery (Main Event)\n\nSelf-supervised training generates formulas, checks them against holdouts, and discovers novel materials.\nThe `search_single_target` helper is defined above for mini holdout searches during Phase 2."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from superconductor.training.self_supervised import (\n    SelfSupervisedConfig, SelfSupervisedEpoch,\n)\n\n# --- Initialize Phase 2 engine ---\nphase2_config = SelfSupervisedConfig(\n    enabled=True,\n    start='0',             # Activate immediately (we're past training)\n    max_weight=0.1,\n    n_samples=0,            # Auto-scale with VRAM\n    warmup_epochs=5,        # Short warmup since model is already trained\n    interval=1,             # Run every sub-epoch\n    min_resume_epochs=0,    # No delay\n)\nn_samples = phase2_config.resolve_n_samples(DEVICE)\n\nprint('=' * 80)\nprint(f'PHASE 2: TRAINING + DISCOVERY \u2014 {PHASE2_EPOCHS} sub-epochs')\nprint(f'  Samples per sub-epoch: {n_samples} (VRAM-scaled)')\nprint(f'  Holdout search every {PHASE2_HOLDOUT_INTERVAL} sub-epochs')\nprint('=' * 80)\n\n# Load or build z-cache for Phase 2 sampling (one-time if missing)\nz_cache_path = OUTPUT_DIR / 'latent_cache.pt'\nif not z_cache_path.exists():\n    print('Z-cache not found. Encoding all training data (one-time, ~2 min on A100)...')\n    encoder.eval()\n    _z_idx = data['train_indices']\n    _all_z = encode_indices(encoder, data, _z_idx)\n    _z_cache = {\n        'z_vectors': _all_z,\n        'tc_values': data['tc'][_z_idx],\n        'is_sc': data['is_sc'][_z_idx],\n    }\n    if 'elem_idx' in data:\n        _z_cache['element_indices'] = data['elem_idx'][_z_idx]\n        _z_cache['element_mask'] = data['elem_mask'][_z_idx]\n    torch.save(_z_cache, z_cache_path)\n    print(f'  Saved z-cache: {_all_z.shape[0]} vectors to {z_cache_path.name}')\n    del _all_z, _z_cache\nz_cache_available = z_cache_path.exists()\n\nif PHASE2_EPOCHS > 0:\n    # Create optimizers\n    encoder.train()\n    decoder.train()\n    enc_opt = torch.optim.AdamW(encoder.parameters(), lr=PHASE2_LR, weight_decay=1e-4)\n    dec_opt = torch.optim.AdamW(decoder.parameters(), lr=PHASE2_LR, weight_decay=1e-4)\n\n    phase2_engine = SelfSupervisedEpoch(\n        config=phase2_config,\n        encoder=encoder,\n        decoder=decoder,\n        device=DEVICE,\n        v13_tokenizer=V13_TOKENIZER,\n        known_formulas=training_formulas,\n        holdout_formulas=holdout_formulas,\n        discovery_output_path=str(OUTPUT_DIR / 'phase2_discoveries.jsonl'),\n    )\n\n    if z_cache_available:\n        phase2_engine.load_z_cache(str(z_cache_path))\n    else:\n        print('  WARNING: No z-cache found. Phase 2 sampling will be limited.')\n        print('  Run training first to generate outputs/latent_cache.pt')\n\n    # Force activation\n    phase2_engine._phase2_activation_epoch = 0\n    phase2_engine._activation_exact = 0.9\n\n    # --- Main Phase 2 loop ---\n    phase2_history = []\n    cumulative_holdout = 0\n    cumulative_novel = 0\n    holdout_search_snapshots = []\n\n    for sub_epoch in range(PHASE2_EPOCHS):\n        print(f'\\n{\"=\"*60}')\n        print(f'Phase 2 sub-epoch {sub_epoch + 1}/{PHASE2_EPOCHS}')\n        print(f'{\"=\"*60}')\n\n        metrics = phase2_engine.run(\n            epoch=sub_epoch,\n            current_exact=0.9,\n            enc_opt=enc_opt,\n            dec_opt=dec_opt,\n            main_lr=PHASE2_LR,\n            use_amp=torch.cuda.is_available(),\n            amp_dtype=torch.bfloat16 if torch.cuda.is_available() and\n                      torch.cuda.get_device_capability(0)[0] >= 8 else torch.float16,\n        )\n\n        if metrics:\n            n_valid = metrics.get('phase2_n_valid', 0)\n            n_novel = metrics.get('phase2_n_novel', 0)\n            n_holdout = metrics.get('phase2_n_holdout_recovered', 0)\n            cumulative_holdout += n_holdout\n            cumulative_novel += n_novel\n\n            print(f'  Loss: {metrics.get(\"phase2_total_loss\", 0):.4f} '\n                  f'(RT={metrics.get(\"phase2_loss1_rt\", 0):.4f}, '\n                  f'Consist={metrics.get(\"phase2_loss2_consist\", 0):.4f}, '\n                  f'Phys={metrics.get(\"phase2_loss3_physics\", 0):.4f})')\n            print(f'  Valid: {n_valid}/{metrics.get(\"phase2_n_sampled\", 0)} '\n                  f'({metrics.get(\"phase2_valid_rate\", 0):.1%}), '\n                  f'Unique: {metrics.get(\"phase2_n_unique_formulas\", 0)}, '\n                  f'Z-MSE: {metrics.get(\"phase2_z_mse\", 0):.4f}')\n            print(f'  Discoveries this epoch: novel={n_novel}, holdout={n_holdout}')\n            print(f'  Cumulative: novel={cumulative_novel}, holdout={cumulative_holdout}/45')\n\n            phase2_history.append(metrics)\n\n        # --- Periodic mini holdout search ---\n        if (sub_epoch + 1) % PHASE2_HOLDOUT_INTERVAL == 0 and sub_epoch < PHASE2_EPOCHS - 1:\n            print(f'\\n  --- Mini holdout search (sub-epoch {sub_epoch + 1}) ---')\n            encoder.eval()\n            decoder.eval()\n\n            # Quick search on a subset (9 targets, 1 per family)\n            mini_targets = []\n            seen_families = set()\n            for s in holdout_samples:\n                if s['family'] not in seen_families:\n                    seen_families.add(s['family'])\n                    mini_targets.append(s)\n\n            mini_exact = 0\n            mini_best_sims = []\n            for s in mini_targets:\n                # Use small budget for speed\n                old_budget = B.copy()\n                try:\n                    for k in ['n_perturbations', 'n_seeds', 'n_temp_samples', 'max_pairs']:\n                        B[k] = max(5, B[k] // 5)\n                    B['noise_scales'] = [0.05, 0.1, 0.2]\n                    B['temps'] = [0.1, 0.3]\n                    B['n_pca_comps'] = 5\n                    B['pca_steps'] = 5\n                    B['n_interp_steps'] = 5\n                    B['n_neighbors'] = 30\n\n                    r = search_single_target(encoder, decoder, data,\n                                             s['formula'], s['Tc'], s['family'])\n                    if r.get('exact'):\n                        mini_exact += 1\n                    mini_best_sims.append(r['best_sim'])\n                finally:\n                    for k, v in old_budget.items():\n                        B[k] = v\n\n            avg_sim = np.mean(mini_best_sims) if mini_best_sims else 0\n            holdout_search_snapshots.append({\n                'sub_epoch': sub_epoch + 1,\n                'mini_exact': mini_exact,\n                'avg_sim': avg_sim,\n                'n_targets': len(mini_targets),\n            })\n            print(f'  Mini search: {mini_exact}/{len(mini_targets)} exact, avg_sim={avg_sim:.3f}')\n\n            encoder.train()\n            decoder.train()\n\n    encoder.eval()\n    decoder.eval()\n\n    # --- Phase 2 summary ---\n    print(f'\\n{\"=\"*80}')\n    print(f'PHASE 2 COMPLETE \u2014 {PHASE2_EPOCHS} sub-epochs')\n    print(f'{\"=\"*80}')\n    print(f'  Total holdout recoveries: {cumulative_holdout}/45')\n    print(f'  Total novel discoveries: {cumulative_novel}')\n    if phase2_history:\n        avg_valid_rate = np.mean([m.get('phase2_valid_rate', 0) for m in phase2_history])\n        avg_z_mse = np.mean([m.get('phase2_z_mse', 0) for m in phase2_history])\n        print(f'  Avg valid rate: {avg_valid_rate:.1%}')\n        print(f'  Avg Z-MSE: {avg_z_mse:.4f}')\n    if holdout_search_snapshots:\n        print(f'\\n  Holdout search progress:')\n        for snap in holdout_search_snapshots:\n            print(f'    Sub-epoch {snap[\"sub_epoch\"]}: '\n                  f'{snap[\"mini_exact\"]}/{snap[\"n_targets\"]} exact, avg_sim={snap[\"avg_sim\"]:.3f}')\n\n    # Check phase2_discoveries.jsonl for full results\n    disc_path = OUTPUT_DIR / 'phase2_discoveries.jsonl'\n    if disc_path.exists():\n        import json as _json\n        discoveries = []\n        with open(disc_path) as f:\n            for line in f:\n                if line.strip():\n                    discoveries.append(_json.loads(line))\n        n_novel_disc = sum(1 for d in discoveries if d.get('category') == 'novel')\n        n_holdout_disc = sum(1 for d in discoveries if d.get('category') == 'holdout_recovery')\n        print(f'\\n  Discoveries file: {len(discoveries)} total entries')\n        print(f'    Novel: {n_novel_disc}, Holdout recoveries: {n_holdout_disc}')\n        if n_holdout_disc > 0:\n            print(f'    Recovered holdout formulas:')\n            for d in discoveries:\n                if d.get('category') == 'holdout_recovery':\n                    print(f'      {d[\"formula\"]} (Tc={d.get(\"tc_kelvin\", \"?\"):.1f}K)')\n\n    print(f'\\nModel has been updated in-memory. Proceeding with improved model...')\nelse:\n    phase2_history = []\n    holdout_search_snapshots = []\n    cumulative_holdout = 0\n    cumulative_novel = 0\n    print('Phase 2 skipped (PHASE2_EPOCHS=0). Running inference-only evaluation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 9: Targeted Holdout Search (Post-Phase-2)\n\nFull targeted search on all 45 holdout targets using the **improved** model (after Phase 2 training).\nUses element-anchored Z-space exploration with VRAM-scaled budgets."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run targeted search on all 45 holdout targets\n",
    "print('=' * 80)\n",
    "print(f'TARGETED HOLDOUT SEARCH \u2014 Epoch {model_epoch} (budget={SEARCH_BUDGET})')\n",
    "print('Element-Anchored Z-Space Exploration')\n",
    "print('=' * 80)\n",
    "\n",
    "search_results = []\n",
    "t_start = time.time()\n",
    "\n",
    "for i, sample in enumerate(holdout_samples):\n",
    "    print(f'\\n--- [{i+1}/45] ---')\n",
    "    result = search_single_target(\n",
    "        encoder, decoder, data,\n",
    "        sample['formula'], sample['Tc'], sample['family'],\n",
    "    )\n",
    "    search_results.append(result)\n",
    "\n",
    "total_time = time.time() - t_start\n",
    "print(f'\\nTotal search time: {total_time/60:.1f} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search Results Summary\n",
    "print('\\n' + '=' * 80)\n",
    "print(f'TARGETED SEARCH SUMMARY \u2014 Epoch {model_epoch}')\n",
    "print('=' * 80)\n",
    "\n",
    "for r in search_results:\n",
    "    marker = '***' if r.get('exact') else ' + ' if r['best_sim'] >= 0.95 else '   '\n",
    "    print(f\"{marker} [{r['target_family']:12s}] sim={r['best_sim']:.3f} | {r['target']}\")\n",
    "    if r['best_gen'] and not r.get('exact'):\n",
    "        print(f\"     Best: {r['best_gen']}\")\n",
    "\n",
    "n_exact = sum(1 for r in search_results if r.get('exact'))\n",
    "print(f'\\nExact matches: {n_exact}/{len(search_results)}')\n",
    "\n",
    "print(f'\\nRESULTS BY THRESHOLD:')\n",
    "for thresh in [1.0, 0.99, 0.98, 0.95, 0.90, 0.85, 0.80]:\n",
    "    found = sum(1 for r in search_results if r['best_sim'] >= thresh)\n",
    "    pct = found / len(search_results) * 100\n",
    "    bar = '#' * int(pct / 2)\n",
    "    print(f'  >= {thresh:.2f}: {found:2d}/45 ({pct:5.1f}%) {bar}')\n",
    "\n",
    "print(f'\\nPER-FAMILY BREAKDOWN:')\n",
    "families = sorted(set(r['target_family'] for r in search_results))\n",
    "for fam in families:\n",
    "    fam_results = [r for r in search_results if r['target_family'] == fam]\n",
    "    n_exact_fam = sum(1 for r in fam_results if r.get('exact'))\n",
    "    n_095 = sum(1 for r in fam_results if r['best_sim'] >= 0.95)\n",
    "    avg_sim = np.mean([r['best_sim'] for r in fam_results])\n",
    "    print(f'  {fam:14s}: exact={n_exact_fam}/5, >=0.95={n_095}/5, avg_sim={avg_sim:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# PART 2: Novel Superconductor Discovery (Post-Phase-2)\n\nBroad Z-space exploration using the **improved** model (after Phase 2 training).\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Encode Training Data -> Z Cache\n",
    "\n",
    "Build a complete Z-space representation of all training data for broad exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for pre-computed z-cache first\n",
    "z_cache_path = OUTPUT_DIR / 'latent_cache.pt'\n",
    "if z_cache_path.exists():\n",
    "    print(f'Loading pre-computed Z-cache from {z_cache_path.name}...')\n",
    "    z_cache = torch.load(z_cache_path, map_location='cpu', weights_only=True)\n",
    "    all_z = z_cache['z_vectors']\n",
    "    all_tc = z_cache.get('tc_values', data['tc'])\n",
    "    all_is_sc = z_cache.get('is_sc', data['is_sc'])\n",
    "    print(f'  Z-cache: {all_z.shape[0]} vectors, {all_z.shape[1]} dims, '\n",
    "          f'norm={all_z.norm(dim=-1).mean():.2f}')\n",
    "else:\n",
    "    print('No Z-cache found. Encoding all training data (takes ~2 min on A100)...')\n",
    "    all_z = encode_indices(encoder, data, data['train_indices'])\n",
    "    all_tc = data['tc'][data['train_indices']]\n",
    "    all_is_sc = data['is_sc'][data['train_indices']]\n",
    "    print(f'  Encoded {all_z.shape[0]} samples, Z norm={all_z.norm(dim=-1).mean():.2f}')\n",
    "\n",
    "# SC-only z-vectors for high-Tc exploration\n",
    "sc_mask = all_is_sc.bool()\n",
    "z_sc = all_z[sc_mask]\n",
    "tc_sc = all_tc[sc_mask]\n",
    "print(f'  SC samples: {z_sc.shape[0]}, mean Tc_norm={tc_sc.mean():.3f}')\n",
    "\n",
    "# Identify high-Tc region (top 10%)\n",
    "tc_threshold = torch.quantile(tc_sc, 0.9).item()\n",
    "high_tc_mask = tc_sc >= tc_threshold\n",
    "z_high_tc = z_sc[high_tc_mask]\n",
    "print(f'  High-Tc (top 10%): {z_high_tc.shape[0]} samples, '\n",
    "      f'Tc_norm >= {tc_threshold:.3f} ({denormalize_tc(tc_threshold):.1f} K)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: Broad Z-Space Exploration for Novel Materials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print(f'NOVEL SUPERCONDUCTOR DISCOVERY \u2014 {NOVEL_N_SAMPLES:,} Z-space samples')\n",
    "print('=' * 80)\n",
    "\n",
    "z_norm_mean = z_sc.norm(dim=-1).mean().item()\n",
    "z_norm_std = z_sc.norm(dim=-1).std().item()\n",
    "z_mean = z_sc.mean(dim=0)\n",
    "z_std = z_sc.std(dim=0).clamp(min=1e-6)\n",
    "\n",
    "novel_z_candidates = []\n",
    "\n",
    "# Allocate budget across strategies\n",
    "n_perturb = int(NOVEL_N_SAMPLES * 0.35)   # 35% perturbation around SC samples\n",
    "n_high_tc = int(NOVEL_N_SAMPLES * 0.25)    # 25% around high-Tc region\n",
    "n_slerp = int(NOVEL_N_SAMPLES * 0.20)      # 20% SLERP between SC pairs\n",
    "n_pca = int(NOVEL_N_SAMPLES * 0.10)         # 10% PCA-directed walks\n",
    "n_gradient = int(NOVEL_N_SAMPLES * 0.10)    # 10% gradient-based Tc optimization\n",
    "\n",
    "print(f'\\nStrategy allocation:')\n",
    "print(f'  Perturbation (SC):  {n_perturb:,}')\n",
    "print(f'  High-Tc region:     {n_high_tc:,}')\n",
    "print(f'  SLERP interpolation: {n_slerp:,}')\n",
    "print(f'  PCA walks:          {n_pca:,}')\n",
    "print(f'  Gradient Tc optim:  {n_gradient:,}')\n",
    "\n",
    "# Strategy 1: Perturbation around random SC training samples\n",
    "print('\\n[1/5] Perturbation around SC samples...')\n",
    "seed_indices = torch.randperm(len(z_sc))[:min(500, len(z_sc))]\n",
    "samples_per_seed = max(1, n_perturb // len(seed_indices))\n",
    "for idx in seed_indices:\n",
    "    z_seed = z_sc[idx].unsqueeze(0)\n",
    "    noise_scale = np.random.choice([0.05, 0.1, 0.15, 0.2, 0.3])\n",
    "    noise = torch.randn(samples_per_seed, z_seed.shape[1]) * noise_scale\n",
    "    novel_z_candidates.append(z_seed + noise)\n",
    "print(f'  Generated {sum(c.shape[0] for c in novel_z_candidates)} perturbation candidates')\n",
    "\n",
    "# Strategy 2: Dense exploration around high-Tc region\n",
    "print('[2/5] High-Tc region exploration...')\n",
    "high_tc_centroid = z_high_tc.mean(dim=0, keepdim=True)\n",
    "high_tc_std = z_high_tc.std(dim=0, keepdim=True).clamp(min=1e-6)\n",
    "for scale in [0.3, 0.5, 0.8, 1.0, 1.5, 2.0]:\n",
    "    n_this = n_high_tc // 6\n",
    "    noise = torch.randn(n_this, z_sc.shape[1]) * scale\n",
    "    novel_z_candidates.append(high_tc_centroid + high_tc_std * noise)\n",
    "print(f'  Cumulative: {sum(c.shape[0] for c in novel_z_candidates)} candidates')\n",
    "\n",
    "# Strategy 3: SLERP between random SC pairs\n",
    "print('[3/5] SLERP interpolation between SC pairs...')\n",
    "n_pairs = n_slerp // 10  # 10 interpolation points per pair\n",
    "pair_idx = torch.randperm(len(z_sc))[:2 * n_pairs].reshape(n_pairs, 2)\n",
    "for i in range(n_pairs):\n",
    "    z1 = z_sc[pair_idx[i, 0]].unsqueeze(0)\n",
    "    z2 = z_sc[pair_idx[i, 1]].unsqueeze(0)\n",
    "    for t in np.linspace(0.1, 0.9, 10):\n",
    "        novel_z_candidates.append(slerp(z1, z2, float(t)))\n",
    "print(f'  Cumulative: {sum(c.shape[0] for c in novel_z_candidates)} candidates')\n",
    "\n",
    "# Strategy 4: PCA-directed walks\n",
    "print('[4/5] PCA-directed walks...')\n",
    "z_sc_np = z_sc.numpy()\n",
    "sc_mean = z_sc_np.mean(axis=0)\n",
    "sc_centered = z_sc_np - sc_mean\n",
    "U, S, Vt = np.linalg.svd(sc_centered, full_matrices=False)\n",
    "n_comps = min(30, len(S))\n",
    "steps_per_comp = max(1, n_pca // (n_comps * 10))\n",
    "for c in range(n_comps):\n",
    "    direction = torch.from_numpy(Vt[c]).float()\n",
    "    std_along = S[c] / np.sqrt(len(z_sc) - 1)\n",
    "    for alpha in np.linspace(-3.0, 3.0, steps_per_comp):\n",
    "        z_walked = torch.from_numpy(sc_mean).float().unsqueeze(0) + alpha * std_along * direction.unsqueeze(0)\n",
    "        novel_z_candidates.append(z_walked)\n",
    "print(f'  Cumulative: {sum(c.shape[0] for c in novel_z_candidates)} candidates')\n",
    "\n",
    "# Strategy 5: Gradient-based Tc optimization\n",
    "print('[5/5] Gradient-based Tc optimization...')\n",
    "n_grad_starts = min(n_gradient, 500)\n",
    "grad_start_idx = torch.randperm(len(z_high_tc))[:n_grad_starts]\n",
    "grad_z_list = []\n",
    "encoder.eval()  # Keep encoder in eval mode\n",
    "for idx in grad_start_idx:\n",
    "    z_opt = z_high_tc[idx].clone().unsqueeze(0).to(DEVICE).requires_grad_(True)\n",
    "    optimizer = torch.optim.Adam([z_opt], lr=0.1)\n",
    "    for step in range(20):  # 20 gradient steps per starting point\n",
    "        optimizer.zero_grad()\n",
    "        tc_pred = encoder.tc_head(z_opt).squeeze()\n",
    "        # Maximize Tc while staying near training distribution\n",
    "        z_reg = ((z_opt.norm(dim=-1) - z_norm_mean) ** 2).mean() * 0.01\n",
    "        loss = -tc_pred + z_reg  # Negative because we want to maximize Tc\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    grad_z_list.append(z_opt.detach().cpu())\n",
    "if grad_z_list:\n",
    "    novel_z_candidates.append(torch.cat(grad_z_list, dim=0))\n",
    "print(f'  Cumulative: {sum(c.shape[0] for c in novel_z_candidates)} candidates')\n",
    "\n",
    "all_novel_z = torch.cat(novel_z_candidates, dim=0)\n",
    "print(f'\\nTotal novel Z candidates: {len(all_novel_z):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12: Decode & Validate Novel Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Decoding novel candidates...')\n",
    "t0 = time.time()\n",
    "novel_results = decode_z_with_tc(encoder, decoder, all_novel_z, temperature=0.01)\n",
    "print(f'  Decoded {len(novel_results):,} candidates in {time.time()-t0:.1f}s')\n",
    "\n",
    "# Deduplicate and classify\n",
    "formula_best = {}  # formula -> best result (highest Tc)\n",
    "for r in novel_results:\n",
    "    f = r['formula'].strip()\n",
    "    if not f or len(f) < 2:\n",
    "        continue\n",
    "    if f not in formula_best or r['tc_kelvin'] > formula_best[f]['tc_kelvin']:\n",
    "        formula_best[f] = r\n",
    "\n",
    "print(f'  Unique formulas: {len(formula_best):,}')\n",
    "\n",
    "# Classify: known (training), holdout recovery, or novel\n",
    "known_count = 0\n",
    "holdout_recovered = []\n",
    "novel_candidates = []\n",
    "\n",
    "for formula, result in formula_best.items():\n",
    "    if formula in training_formulas:\n",
    "        known_count += 1\n",
    "    elif formula in holdout_formulas:\n",
    "        holdout_recovered.append(result)\n",
    "    else:\n",
    "        # Check if it parses as a valid chemical formula\n",
    "        parsed = parse_formula_elements(formula)\n",
    "        if parsed and len(parsed) >= 2:  # At least 2 elements\n",
    "            result['parsed_elements'] = parsed\n",
    "            result['n_elements'] = len(parsed)\n",
    "            novel_candidates.append(result)\n",
    "\n",
    "print(f'\\nClassification:')\n",
    "print(f'  Known (training): {known_count}')\n",
    "print(f'  Holdout recovered: {len(holdout_recovered)}')\n",
    "print(f'  Novel (valid parse, 2+ elements): {len(novel_candidates)}')\n",
    "print(f'  Invalid/single-element: {len(formula_best) - known_count - len(holdout_recovered) - len(novel_candidates)}')\n",
    "\n",
    "if holdout_recovered:\n",
    "    print(f'\\nHoldout Recoveries (found during broad exploration!):')\n",
    "    for r in holdout_recovered:\n",
    "        print(f'  {r[\"formula\"]} (pred Tc={r[\"tc_kelvin\"]:.1f}K, SC prob={r[\"sc_prob\"]:.3f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and rank novel candidates\n",
    "# Keep those predicted as superconductors with Tc above threshold\n",
    "sc_novels = [r for r in novel_candidates if r['sc_prob'] > 0.5 and r['tc_kelvin'] >= NOVEL_MIN_TC]\n",
    "sc_novels.sort(key=lambda x: -x['tc_kelvin'])\n",
    "\n",
    "print('=' * 80)\n",
    "print(f'TOP NOVEL SUPERCONDUCTOR CANDIDATES (SC prob > 0.5, Tc >= {NOVEL_MIN_TC}K)')\n",
    "print('=' * 80)\n",
    "print(f'{\"Rank\":>4s} {\"Formula\":<35s} {\"Tc (K)\":>8s} {\"SC prob\":>8s} {\"Family\":>16s} {\"Elements\":>10s}')\n",
    "print('-' * 90)\n",
    "\n",
    "for i, r in enumerate(sc_novels[:50]):\n",
    "    fam_name = FAMILY_14_NAMES[r['family']] if 0 <= r['family'] < len(FAMILY_14_NAMES) else '?'\n",
    "    elems = ', '.join(sorted(r.get('parsed_elements', {}).keys()))\n",
    "    print(f'{i+1:4d}  {r[\"formula\"]:<35s} {r[\"tc_kelvin\"]:8.1f} {r[\"sc_prob\"]:8.3f} {fam_name:>16s} {elems}')\n",
    "\n",
    "print(f'\\nTotal novel SC candidates: {len(sc_novels)}')\n",
    "\n",
    "# Tc distribution of novel candidates\n",
    "if sc_novels:\n",
    "    tc_vals = [r['tc_kelvin'] for r in sc_novels]\n",
    "    print(f'\\nNovel Tc distribution:')\n",
    "    for lo, hi, label in [(0,10,'0-10K'), (10,30,'10-30K'), (30,77,'30-77K'), (77,120,'77-120K'), (120,200,'120-200K'), (200,9999,'>200K')]:\n",
    "        n = sum(1 for t in tc_vals if lo <= t < hi)\n",
    "        if n > 0:\n",
    "            print(f'  {label:>10s}: {n:5d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Results & Visualization\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 14: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "fam_colors = {\n",
    "    'YBCO': '#e74c3c', 'LSCO': '#3498db', 'Hg-cuprate': '#2ecc71',\n",
    "    'Tl-cuprate': '#9b59b6', 'Bi-cuprate': '#f39c12', 'Iron-based': '#1abc9c',\n",
    "    'MgB2': '#e67e22', 'Conventional': '#95a5a6', 'Other': '#34495e',\n",
    "}\n",
    "\n",
    "# Plot 1: Similarity distribution (holdout search)\n",
    "sims = [r['best_sim'] for r in search_results]\n",
    "colors = ['#2ecc71' if s >= 0.95 else '#f39c12' if s >= 0.80 else '#e74c3c' for s in sims]\n",
    "axes[0,0].barh(range(len(sims)), sims, color=colors)\n",
    "axes[0,0].set_xlabel('Best Similarity')\n",
    "axes[0,0].set_ylabel('Holdout Target Index')\n",
    "axes[0,0].set_title('Part 1: Holdout Recovery \u2014 Best Match per Target')\n",
    "axes[0,0].axvline(x=0.95, color='green', linestyle='--', alpha=0.5, label='0.95')\n",
    "axes[0,0].axvline(x=0.80, color='orange', linestyle='--', alpha=0.5, label='0.80')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].set_xlim(0, 1.05)\n",
    "\n",
    "# Plot 2: Tc prediction error (roundtrip)\n",
    "for sample, err in zip(holdout_samples, tc_errors):\n",
    "    axes[0,1].scatter(sample['Tc'], err, color=fam_colors.get(sample['family'], 'gray'),\n",
    "                     s=50, alpha=0.7, label=sample['family'])\n",
    "handles, labels = axes[0,1].get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "axes[0,1].legend(by_label.values(), by_label.keys(), fontsize=7, loc='upper left')\n",
    "axes[0,1].set_xlabel('True Tc (K)')\n",
    "axes[0,1].set_ylabel('Absolute Error (K)')\n",
    "axes[0,1].set_title('Tc Prediction Error vs True Tc')\n",
    "axes[0,1].axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Plot 3: Per-family average similarity\n",
    "fam_avg = {}\n",
    "for r in search_results:\n",
    "    fam = r['target_family']\n",
    "    fam_avg.setdefault(fam, []).append(r['best_sim'])\n",
    "fam_names = sorted(fam_avg.keys())\n",
    "fam_means = [np.mean(fam_avg[f]) for f in fam_names]\n",
    "axes[1,0].barh(fam_names, fam_means, color=[fam_colors.get(f, 'gray') for f in fam_names])\n",
    "axes[1,0].set_xlabel('Average Best Similarity')\n",
    "axes[1,0].set_title('Average Similarity by Family')\n",
    "axes[1,0].set_xlim(0, 1.05)\n",
    "axes[1,0].axvline(x=0.95, color='green', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Plot 4: Novel candidate Tc distribution\n",
    "if sc_novels:\n",
    "    novel_tcs = [r['tc_kelvin'] for r in sc_novels]\n",
    "    axes[1,1].hist(novel_tcs, bins=30, color='#3498db', edgecolor='white', alpha=0.8)\n",
    "    axes[1,1].axvline(x=77, color='red', linestyle='--', alpha=0.7, label='LN2 (77K)')\n",
    "    axes[1,1].axvline(x=120, color='orange', linestyle='--', alpha=0.7, label='120K')\n",
    "    axes[1,1].set_xlabel('Predicted Tc (K)')\n",
    "    axes[1,1].set_ylabel('Count')\n",
    "    axes[1,1].set_title(f'Part 2: Novel Candidates \u2014 Tc Distribution (n={len(sc_novels)})')\n",
    "    axes[1,1].legend()\n",
    "else:\n",
    "    axes[1,1].text(0.5, 0.5, 'No novel SC candidates found', ha='center', va='center', fontsize=14)\n",
    "    axes[1,1].set_title('Part 2: Novel Candidates')\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_path = str(OUTPUT_DIR / 'generative_evaluation.png')\n",
    "plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f'Saved: {fig_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 15: Grade Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_mae = np.mean(tc_errors)\n",
    "tc_within_1k = (np.array(tc_errors) < 1).sum()\n",
    "n_exact_search = sum(1 for r in search_results if r.get('exact'))\n",
    "n_095 = sum(1 for r in search_results if r['best_sim'] >= 0.95)\n",
    "n_099 = sum(1 for r in search_results if r['best_sim'] >= 0.99)\n",
    "avg_best_sim = np.mean([r['best_sim'] for r in search_results])\n",
    "\n",
    "def grade(val, thresholds):\n",
    "    grades = ['A+', 'A', 'B+', 'B', 'C', 'D', 'F']\n",
    "    for i, t in enumerate(thresholds):\n",
    "        if val >= t:\n",
    "            return grades[i]\n",
    "    return grades[-1]\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print(f'GENERATIVE EVALUATION GRADE CARD \u2014 Epoch {model_epoch}')\n",
    "print('=' * 80)\n",
    "\n",
    "metrics_list = [\n",
    "    ('--- Part 1: Holdout Recovery ---', '', ''),\n",
    "    ('SC Classification', f'{sc_correct}/45 ({sc_correct/45*100:.0f}%)',\n",
    "     grade(sc_correct/45, [0.98, 0.95, 0.90, 0.80, 0.70, 0.60])),\n",
    "    ('Tc Prediction MAE', f'{tc_mae:.2f}K',\n",
    "     grade(1/(1+tc_mae), [0.67, 0.50, 0.33, 0.20, 0.10, 0.05])),\n",
    "    ('Tc within 1K', f'{tc_within_1k}/45 ({tc_within_1k/45*100:.0f}%)',\n",
    "     grade(tc_within_1k/45, [0.90, 0.80, 0.70, 0.60, 0.50, 0.30])),\n",
    "    ('Family Classification', f'{family_correct}/45 ({family_correct/45*100:.0f}%)',\n",
    "     grade(family_correct/45, [0.95, 0.90, 0.80, 0.70, 0.60, 0.50])),\n",
    "    ('Formula Exact (search)', f'{n_exact_search}/45 ({n_exact_search/45*100:.0f}%)',\n",
    "     grade(n_exact_search/45, [0.80, 0.60, 0.40, 0.25, 0.15, 0.05])),\n",
    "    ('Formula >= 0.95 sim', f'{n_095}/45 ({n_095/45*100:.0f}%)',\n",
    "     grade(n_095/45, [0.95, 0.85, 0.70, 0.55, 0.40, 0.25])),\n",
    "    ('Average Best Similarity', f'{avg_best_sim:.3f}',\n",
    "     grade(avg_best_sim, [0.95, 0.90, 0.85, 0.80, 0.70, 0.60])),\n",
    "    ('Magpie MSE', f'{np.mean(magpie_mses):.4f}',\n",
    "     grade(1/(1+np.mean(magpie_mses)*10), [0.90, 0.80, 0.60, 0.40, 0.25, 0.10])),\n",
    "    ('--- Part 2: Novel Discovery ---', '', ''),\n",
    "    ('Novel SC candidates', f'{len(sc_novels):,}', ''),\n",
    "    ('Holdout recovered (broad)', f'{len(holdout_recovered)}/45', ''),\n",
    "    ('Novel Tc > 77K (LN2)', f'{sum(1 for r in sc_novels if r[\"tc_kelvin\"] > 77):,}', ''),\n",
    "    ('Novel Tc > 120K', f'{sum(1 for r in sc_novels if r[\"tc_kelvin\"] > 120):,}', ''),\n",
    "]\n",
    "\n",
    "print(f'\\n{\"Metric\":<28s} {\"Result\":<28s} {\"Grade\":>5s}')\n",
    "print('-' * 65)\n",
    "for name, result, g in metrics_list:\n",
    "    if name.startswith('---'):\n",
    "        print(f'\\n{name}')\n",
    "    else:\n",
    "        print(f'  {name:<26s} {result:<28s} {g:>5s}')\n",
    "\n",
    "print(f'\\n{\"=\"*65}')\n",
    "print(\n",
    "    f'The model {\"CAN\" if n_exact_search >= 10 else \"CANNOT yet\"} reliably generate '\n",
    "    f'unseen superconductors.\\n'\n",
    "    f'  - {n_exact_search}/45 exact matches from Z-space exploration\\n'\n",
    "    f'  - {n_095}/45 with >= 0.95 compositional similarity\\n'\n",
    "    f'  - Tc prediction: {tc_mae:.2f}K MAE on held-out materials\\n'\n",
    "    f'  - {len(sc_novels):,} novel SC candidates generated ({sum(1 for r in sc_novels if r[\"tc_kelvin\"] > 77)} above LN2 temp)\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 16: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'timestamp': time.strftime('%Y-%m-%dT%H:%M:%S'),\n",
    "    'checkpoint': Path(CHECKPOINT_PATH).name,\n",
    "    'epoch': model_epoch,\n",
    "    'search_budget': SEARCH_BUDGET,\n",
    "    'vram_gb': round(VRAM_GB, 1),\n",
    "    'part1_roundtrip': {\n",
    "        'tc_mae_K': float(tc_mae),\n",
    "        'tc_within_1K': int(tc_within_1k),\n",
    "        'sc_accuracy': sc_correct / 45,\n",
    "        'family_accuracy': family_correct / 45,\n",
    "        'magpie_mse': float(np.mean(magpie_mses)),\n",
    "        'formula_mean_sim': float(np.mean(formula_sims)),\n",
    "        'details': roundtrip_results,\n",
    "    },\n",
    "    'part1_search': {\n",
    "        'n_exact': n_exact_search,\n",
    "        'n_095': n_095,\n",
    "        'n_099': n_099,\n",
    "        'avg_best_sim': float(avg_best_sim),\n",
    "        'details': [{k: v for k, v in r.items() if k != 'all_candidates'} for r in search_results],\n",
    "    },\n",
    "    'part2_novel': {\n",
    "        'n_total_sampled': len(all_novel_z),\n",
    "        'n_unique_formulas': len(formula_best),\n",
    "        'n_known': known_count,\n",
    "        'n_holdout_recovered': len(holdout_recovered),\n",
    "        'n_novel_sc': len(sc_novels),\n",
    "        'n_novel_above_77K': sum(1 for r in sc_novels if r['tc_kelvin'] > 77),\n",
    "        'n_novel_above_120K': sum(1 for r in sc_novels if r['tc_kelvin'] > 120),\n",
    "        'top_50': [{'formula': r['formula'], 'tc_kelvin': round(r['tc_kelvin'], 2),\n",
    "                     'sc_prob': round(r['sc_prob'], 4),\n",
    "                     'family': FAMILY_14_NAMES[r['family']] if 0 <= r['family'] < len(FAMILY_14_NAMES) else '?',\n",
    "                     'elements': sorted(r.get('parsed_elements', {}).keys())}\n",
    "                    for r in sc_novels[:50]],\n",
    "        'holdout_recovered': [{'formula': r['formula'], 'tc_kelvin': round(r['tc_kelvin'], 2)}\n",
    "                              for r in holdout_recovered],\n",
    "    },\n",
    "}\n",
    "\n",
    "output_path = str(OUTPUT_DIR / f'generative_evaluation_epoch_{model_epoch}.json')\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(output, f, indent=2, default=str)\n",
    "print(f'Results saved to: {output_path}')\n",
    "\n",
    "# Also save full novel candidates list\n",
    "if sc_novels:\n",
    "    novel_path = str(OUTPUT_DIR / f'novel_candidates_epoch_{model_epoch}.json')\n",
    "    novel_output = [{'formula': r['formula'], 'tc_kelvin': round(r['tc_kelvin'], 2),\n",
    "                      'sc_prob': round(r['sc_prob'], 4),\n",
    "                      'family': FAMILY_14_NAMES[r['family']] if 0 <= r['family'] < len(FAMILY_14_NAMES) else '?',\n",
    "                      'n_elements': r.get('n_elements', 0),\n",
    "                      'elements': sorted(r.get('parsed_elements', {}).keys())}\n",
    "                     for r in sc_novels]\n",
    "    with open(novel_path, 'w') as f:\n",
    "        json.dump(novel_output, f, indent=2)\n",
    "    print(f'Novel candidates saved to: {novel_path} ({len(sc_novels)} candidates)')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}