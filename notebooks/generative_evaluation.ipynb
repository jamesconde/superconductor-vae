{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Superconductor VAE — Generative Evaluation\n",
    "\n",
    "**Goal**: Evaluate whether the trained VAE is a good *generator* of superconductor formulas.\n",
    "\n",
    "This notebook runs two evaluations against the **45 held-out superconductors** (5 per family, never seen during training):\n",
    "\n",
    "1. **Roundtrip Validation**: Encode each holdout → Z → decode all heads (formula, Tc, Magpie, SC class, family)\n",
    "2. **Targeted Holdout Search**: Element-anchored Z-space exploration with PCA walks, perturbation, interpolation, and temperature sampling (~27K candidates per target)\n",
    "3. **Self-Consistency Check**: Verify all model heads agree (SC↔Tc, SC↔Family, Tc↔Bucket)\n",
    "\n",
    "**Families**: YBCO, LSCO, Hg-cuprate, Tl-cuprate, Bi-cuprate, Iron-based, MgB2, Conventional, Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repo and install dependencies\n",
    "import os\n",
    "\n",
    "REPO_DIR = '/content/superconductor-vae'\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone https://github.com/jamesconde/superconductor-vae.git {REPO_DIR}\n",
    "else:\n",
    "    !cd {REPO_DIR} && git pull\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "!pip install -q scipy matminer pymatgen scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload checkpoint_best.pt\n",
    "# Option A: Upload directly\n",
    "# Option B: Copy from Google Drive (uncomment below)\n",
    "\n",
    "import os\n",
    "CHECKPOINT_PATH = os.path.join(REPO_DIR, 'outputs', 'checkpoint_best.pt')\n",
    "\n",
    "# --- Option A: Direct upload ---\n",
    "if not os.path.exists(CHECKPOINT_PATH):\n",
    "    from google.colab import files\n",
    "    print(\"Upload checkpoint_best.pt:\")\n",
    "    uploaded = files.upload()\n",
    "    for name, data in uploaded.items():\n",
    "        os.makedirs(os.path.dirname(CHECKPOINT_PATH), exist_ok=True)\n",
    "        with open(CHECKPOINT_PATH, 'wb') as f:\n",
    "            f.write(data)\n",
    "        print(f\"Saved {name} → {CHECKPOINT_PATH} ({len(data)/1e6:.1f} MB)\")\n",
    "\n",
    "# --- Option B: Google Drive (uncomment) ---\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# !cp /content/drive/MyDrive/path/to/checkpoint_best.pt {CHECKPOINT_PATH}\n",
    "\n",
    "print(f\"Checkpoint: {CHECKPOINT_PATH}\")\n",
    "print(f\"Exists: {os.path.exists(CHECKPOINT_PATH)}\")\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    print(f\"Size: {os.path.getsize(CHECKPOINT_PATH)/1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Add source to path\n",
    "sys.path.insert(0, os.path.join(REPO_DIR, 'src'))\n",
    "\n",
    "from superconductor.models.attention_vae import FullMaterialsVAE\n",
    "from superconductor.models.autoregressive_decoder import (\n",
    "    EnhancedTransformerDecoder, IDX_TO_TOKEN, TOKEN_TO_IDX,\n",
    "    PAD_IDX, START_IDX, END_IDX,\n",
    ")\n",
    "from superconductor.data.canonical_ordering import CanonicalOrderer, ElementWithFraction\n",
    "\n",
    "PROJECT_ROOT = Path(REPO_DIR)\n",
    "HOLDOUT_PATH = PROJECT_ROOT / 'data' / 'GENERATIVE_HOLDOUT_DO_NOT_TRAIN.json'\n",
    "CACHE_DIR = PROJECT_ROOT / 'data' / 'processed' / 'cache'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "TC_MEAN = 2.725219433789196\n",
    "TC_STD = 1.3527019896187407\n",
    "\n",
    "FAMILY_14_NAMES = [\n",
    "    'NOT_SC', 'BCS_CONVENTIONAL', 'CUPRATE_YBCO', 'CUPRATE_LSCO',\n",
    "    'CUPRATE_BSCCO', 'CUPRATE_TBCCO', 'CUPRATE_HBCCO', 'CUPRATE_OTHER',\n",
    "    'IRON_PNICTIDE', 'IRON_CHALCOGENIDE', 'MGB2_TYPE',\n",
    "    'HEAVY_FERMION', 'ORGANIC', 'OTHER_UNKNOWN',\n",
    "]\n",
    "\n",
    "TC_BUCKET_NAMES = ['non-SC (0K)', 'low (0-10K)', 'medium (10-50K)',\n",
    "                   'high (50-100K)', 'very-high (100K+)']\n",
    "\n",
    "HOLDOUT_FAMILY_TO_14 = {\n",
    "    'YBCO': 2, 'LSCO': 3, 'Hg-cuprate': 6, 'Tl-cuprate': 5,\n",
    "    'Bi-cuprate': 4, 'Iron-based': 8, 'MgB2': 10,\n",
    "    'Conventional': 1, 'Other': 13,\n",
    "}\n",
    "\n",
    "ELEMENT_TO_Z = {\n",
    "    'H': 1, 'He': 2, 'Li': 3, 'Be': 4, 'B': 5, 'C': 6, 'N': 7, 'O': 8,\n",
    "    'F': 9, 'Ne': 10, 'Na': 11, 'Mg': 12, 'Al': 13, 'Si': 14, 'P': 15,\n",
    "    'S': 16, 'Cl': 17, 'Ar': 18, 'K': 19, 'Ca': 20, 'Sc': 21, 'Ti': 22,\n",
    "    'V': 23, 'Cr': 24, 'Mn': 25, 'Fe': 26, 'Co': 27, 'Ni': 28, 'Cu': 29,\n",
    "    'Zn': 30, 'Ga': 31, 'Ge': 32, 'As': 33, 'Se': 34, 'Br': 35, 'Kr': 36,\n",
    "    'Rb': 37, 'Sr': 38, 'Y': 39, 'Zr': 40, 'Nb': 41, 'Mo': 42, 'Tc': 43,\n",
    "    'Ru': 44, 'Rh': 45, 'Pd': 46, 'Ag': 47, 'Cd': 48, 'In': 49, 'Sn': 50,\n",
    "    'Sb': 51, 'Te': 52, 'I': 53, 'Xe': 54, 'Cs': 55, 'Ba': 56, 'La': 57,\n",
    "    'Ce': 58, 'Pr': 59, 'Nd': 60, 'Pm': 61, 'Sm': 62, 'Eu': 63, 'Gd': 64,\n",
    "    'Tb': 65, 'Dy': 66, 'Ho': 67, 'Er': 68, 'Tm': 69, 'Yb': 70, 'Lu': 71,\n",
    "    'Hf': 72, 'Ta': 73, 'W': 74, 'Re': 75, 'Os': 76, 'Ir': 77, 'Pt': 78,\n",
    "    'Au': 79, 'Hg': 80, 'Tl': 81, 'Pb': 82, 'Bi': 83, 'Po': 84, 'At': 85,\n",
    "    'Rn': 86, 'Fr': 87, 'Ra': 88, 'Ac': 89, 'Th': 90, 'Pa': 91, 'U': 92,\n",
    "}\n",
    "\n",
    "_CANONICALIZER = CanonicalOrderer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_formula(token_ids):\n",
    "    \"\"\"Convert token IDs to formula string.\"\"\"\n",
    "    tokens = []\n",
    "    for tid in token_ids:\n",
    "        tid = int(tid)\n",
    "        if tid == PAD_IDX or tid == START_IDX:\n",
    "            continue\n",
    "        if tid == END_IDX:\n",
    "            break\n",
    "        token = IDX_TO_TOKEN.get(tid, '?')\n",
    "        tokens.append(token)\n",
    "    return ''.join(tokens)\n",
    "\n",
    "\n",
    "def denormalize_tc(tc_norm):\n",
    "    \"\"\"Convert normalized Tc prediction back to Kelvin.\"\"\"\n",
    "    tc_log = tc_norm * TC_STD + TC_MEAN\n",
    "    return max(0.0, np.expm1(tc_log))\n",
    "\n",
    "\n",
    "def parse_formula_elements(formula):\n",
    "    \"\"\"Extract {element: fraction_value} from formula string.\"\"\"\n",
    "    try:\n",
    "        elements = _CANONICALIZER.parse_formula(formula)\n",
    "        if not elements:\n",
    "            return {}\n",
    "        result = {}\n",
    "        for ef in elements:\n",
    "            val = ef.fraction_value\n",
    "            result[ef.element] = result.get(ef.element, 0) + val\n",
    "        return result\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "\n",
    "def element_similarity(formula_a, formula_b):\n",
    "    \"\"\"Compositional similarity: Jaccard on elements + fraction overlap.\"\"\"\n",
    "    parsed_a = parse_formula_elements(formula_a)\n",
    "    parsed_b = parse_formula_elements(formula_b)\n",
    "    if not parsed_a or not parsed_b:\n",
    "        return 0.0\n",
    "    all_elements = set(parsed_a.keys()) | set(parsed_b.keys())\n",
    "    shared = set(parsed_a.keys()) & set(parsed_b.keys())\n",
    "    if not all_elements:\n",
    "        return 0.0\n",
    "    jaccard = len(shared) / len(all_elements)\n",
    "    if shared:\n",
    "        total_a = sum(parsed_a.values())\n",
    "        total_b = sum(parsed_b.values())\n",
    "        frac_overlap = 0.0\n",
    "        for elem in shared:\n",
    "            fa = parsed_a[elem] / max(total_a, 1e-8)\n",
    "            fb = parsed_b[elem] / max(total_b, 1e-8)\n",
    "            frac_overlap += min(fa, fb)\n",
    "        frac_sim = frac_overlap\n",
    "    else:\n",
    "        frac_sim = 0.0\n",
    "    return 0.5 * jaccard + 0.5 * frac_sim\n",
    "\n",
    "\n",
    "def element_overlap_score(target_elements, cache_elem_idx):\n",
    "    \"\"\"Score a training sample by how many target elements it contains.\"\"\"\n",
    "    candidate_elements = set(int(z) for z in cache_elem_idx if z > 0)\n",
    "    shared_elems = target_elements & candidate_elements\n",
    "    all_elem = target_elements | candidate_elements\n",
    "    if not all_elem:\n",
    "        return (0, 0.0)\n",
    "    return (len(shared_elems), len(shared_elems) / len(all_elem))\n",
    "\n",
    "\n",
    "def slerp(z1, z2, t):\n",
    "    \"\"\"Spherical linear interpolation.\"\"\"\n",
    "    z1_norm = F.normalize(z1, dim=-1)\n",
    "    z2_norm = F.normalize(z2, dim=-1)\n",
    "    omega = torch.acos(torch.clamp(\n",
    "        (z1_norm * z2_norm).sum(dim=-1, keepdim=True), -1.0, 1.0\n",
    "    ))\n",
    "    omega = omega.clamp(min=1e-6)\n",
    "    sin_omega = torch.sin(omega)\n",
    "    if sin_omega.abs().min() < 1e-6:\n",
    "        return (1 - t) * z1 + t * z2\n",
    "    s1 = torch.sin((1 - t) * omega) / sin_omega\n",
    "    s2 = torch.sin(t * omega) / sin_omega\n",
    "    mag1 = z1.norm(dim=-1, keepdim=True)\n",
    "    mag2 = z2.norm(dim=-1, keepdim=True)\n",
    "    mag = (1 - t) * mag1 + t * mag2\n",
    "    return (s1 * z1_norm + s2 * z2_norm) * mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(checkpoint_path):\n",
    "    \"\"\"Load encoder and decoder from checkpoint.\"\"\"\n",
    "    print(f\"Loading checkpoint: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE, weights_only=False)\n",
    "\n",
    "    enc_state_raw = checkpoint.get('encoder_state_dict', {})\n",
    "    magpie_dim = 145\n",
    "    for k, v in enc_state_raw.items():\n",
    "        if 'magpie_encoder' in k and k.endswith('.weight') and v.dim() == 2:\n",
    "            magpie_dim = v.shape[1]\n",
    "            break\n",
    "\n",
    "    enc_state = {k.replace('_orig_mod.', ''): v for k, v in enc_state_raw.items()}\n",
    "\n",
    "    # Detect numden_head architecture\n",
    "    has_numden_head = any('numden_head.' in k for k in enc_state)\n",
    "    numden_first_key = 'numden_head.0.weight'\n",
    "    old_numden_arch = False\n",
    "    if numden_first_key in enc_state:\n",
    "        if enc_state[numden_first_key].shape[0] == 128:\n",
    "            old_numden_arch = True\n",
    "            print(f\"  Detected OLD numden_head architecture (128-dim)\")\n",
    "\n",
    "    encoder = FullMaterialsVAE(\n",
    "        n_elements=118, element_embed_dim=128, n_attention_heads=8,\n",
    "        magpie_dim=magpie_dim, fusion_dim=256, encoder_hidden=[512, 256],\n",
    "        latent_dim=2048, decoder_hidden=[256, 512], dropout=0.1\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    if old_numden_arch:\n",
    "        import torch.nn as nn\n",
    "        encoder.numden_head = nn.Sequential(\n",
    "            nn.Linear(2048, 128), nn.ReLU(), nn.Linear(128, encoder.max_elements * 2),\n",
    "        ).to(DEVICE)\n",
    "\n",
    "    encoder.load_state_dict(enc_state, strict=False)\n",
    "\n",
    "    dec_state_raw = checkpoint.get('decoder_state_dict', {})\n",
    "    dec_state = {k.replace('_orig_mod.', ''): v for k, v in dec_state_raw.items()}\n",
    "\n",
    "    stoich_weight_key = 'stoich_to_memory.0.weight'\n",
    "    stoich_dim = dec_state[stoich_weight_key].shape[1] if stoich_weight_key in dec_state else 37\n",
    "\n",
    "    dec_vocab_size = checkpoint.get('tokenizer_vocab_size', None)\n",
    "    if dec_vocab_size is None and 'token_embedding.weight' in dec_state:\n",
    "        dec_vocab_size = dec_state['token_embedding.weight'].shape[0]\n",
    "\n",
    "    _d_model = checkpoint.get('d_model', None)\n",
    "    if _d_model is None and 'token_embedding.weight' in dec_state:\n",
    "        _d_model = dec_state['token_embedding.weight'].shape[1]\n",
    "    _d_model = _d_model or 512\n",
    "    _dim_ff = checkpoint.get('dim_feedforward', None)\n",
    "    if _dim_ff is None and 'transformer_decoder.layers.0.linear1.weight' in dec_state:\n",
    "        _dim_ff = dec_state['transformer_decoder.layers.0.linear1.weight'].shape[0]\n",
    "    _dim_ff = _dim_ff or 2048\n",
    "    _nhead = checkpoint.get('nhead', 8)\n",
    "    _num_layers = checkpoint.get('num_layers', 12)\n",
    "    _max_len = checkpoint.get('max_formula_len', 60)\n",
    "\n",
    "    decoder = EnhancedTransformerDecoder(\n",
    "        latent_dim=2048, d_model=_d_model, nhead=_nhead, num_layers=_num_layers,\n",
    "        dim_feedforward=_dim_ff, dropout=0.1, max_len=_max_len,\n",
    "        n_memory_tokens=16, encoder_skip_dim=256,\n",
    "        use_skip_connection=False, use_stoich_conditioning=True,\n",
    "        max_elements=12, n_stoich_tokens=4,\n",
    "        vocab_size=dec_vocab_size, stoich_input_dim=stoich_dim,\n",
    "    ).to(DEVICE)\n",
    "    decoder.load_state_dict(dec_state, strict=False)\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    epoch = checkpoint.get('epoch', '?')\n",
    "    print(f\"  Loaded epoch {epoch}, magpie_dim={magpie_dim}, d_model={_d_model}, \"\n",
    "          f\"dim_ff={_dim_ff}, vocab={dec_vocab_size}, numden={has_numden_head}\")\n",
    "    return encoder, decoder, magpie_dim, has_numden_head, epoch\n",
    "\n",
    "\n",
    "def load_data(magpie_dim):\n",
    "    \"\"\"Load cached tensors.\"\"\"\n",
    "    data = {\n",
    "        'elem_idx': torch.load(CACHE_DIR / 'element_indices.pt', map_location='cpu', weights_only=True),\n",
    "        'elem_frac': torch.load(CACHE_DIR / 'element_fractions.pt', map_location='cpu', weights_only=True),\n",
    "        'elem_mask': torch.load(CACHE_DIR / 'element_mask.pt', map_location='cpu', weights_only=True),\n",
    "        'tc': torch.load(CACHE_DIR / 'tc_tensor.pt', map_location='cpu', weights_only=True),\n",
    "        'magpie': torch.load(CACHE_DIR / 'magpie_tensor.pt', map_location='cpu', weights_only=True),\n",
    "        'is_sc': torch.load(CACHE_DIR / 'is_sc_tensor.pt', map_location='cpu', weights_only=True),\n",
    "        'tokens': torch.load(CACHE_DIR / 'formula_tokens.pt', map_location='cpu', weights_only=True),\n",
    "    }\n",
    "    if data['magpie'].shape[1] > magpie_dim:\n",
    "        data['magpie'] = data['magpie'][:, :magpie_dim]\n",
    "    meta = json.load(open(CACHE_DIR / 'cache_meta.json'))\n",
    "    data['train_indices'] = meta.get('train_indices', list(range(len(data['elem_idx']))))\n",
    "    print(f\"  {len(data['elem_idx'])} total samples, {len(data['train_indices'])} train\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load everything\n",
    "encoder, decoder, magpie_dim, has_numden_head, model_epoch = load_models(CHECKPOINT_PATH)\n",
    "data = load_data(magpie_dim)\n",
    "\n",
    "with open(HOLDOUT_PATH) as f:\n",
    "    holdout = json.load(f)\n",
    "\n",
    "holdout_samples = holdout['holdout_samples']\n",
    "print(f\"\\n{len(holdout_samples)} holdout materials loaded\")\n",
    "print(f\"Families: {sorted(set(s['family'] for s in holdout_samples))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Part 1: Roundtrip Validation\n",
    "\n",
    "For each of the 45 holdout superconductors, encode it → Z → decode ALL heads:\n",
    "- Formula (autoregressive generation)\n",
    "- Tc (regression, Kelvin)\n",
    "- SC classification (binary)\n",
    "- Family classification (14-class)\n",
    "- Magpie features (145-dim)\n",
    "- High-pressure prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def full_forward(encoder, decoder, elem_idx, elem_frac, elem_mask, magpie, tc, temperature=0.01):\n",
    "    \"\"\"Run full encoder forward (all heads) + formula decoder.\"\"\"\n",
    "    enc_out = encoder(\n",
    "        elem_idx.to(DEVICE), elem_frac.to(DEVICE), elem_mask.to(DEVICE),\n",
    "        magpie.to(DEVICE), tc.to(DEVICE),\n",
    "    )\n",
    "    z = enc_out['z']\n",
    "    tc_pred_norm = enc_out['tc_pred'][0].item()\n",
    "    magpie_pred = enc_out['magpie_pred'][0].cpu()\n",
    "\n",
    "    # Stoich conditioning for decoder\n",
    "    fraction_pred = enc_out['fraction_pred']\n",
    "    element_count_pred = enc_out['element_count_pred']\n",
    "    numden_pred = enc_out.get('numden_pred')\n",
    "    if numden_pred is not None:\n",
    "        stoich_pred = torch.cat([fraction_pred, numden_pred, element_count_pred.unsqueeze(-1)], dim=-1)\n",
    "    else:\n",
    "        stoich_pred = torch.cat([fraction_pred, element_count_pred.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "    generated, log_probs, entropy = decoder.generate_with_kv_cache(\n",
    "        z=z, stoich_pred=stoich_pred, temperature=temperature,\n",
    "    )\n",
    "    formula = tokens_to_formula(generated[0])\n",
    "    tc_kelvin = denormalize_tc(tc_pred_norm)\n",
    "\n",
    "    result = {\n",
    "        'formula': formula, 'tc_pred_kelvin': tc_kelvin, 'tc_pred_norm': tc_pred_norm,\n",
    "        'magpie_pred': magpie_pred,\n",
    "    }\n",
    "\n",
    "    # Tc classification\n",
    "    tc_class_logits = enc_out.get('tc_class_logits')\n",
    "    if tc_class_logits is not None:\n",
    "        probs = torch.softmax(tc_class_logits[0].cpu(), dim=-1)\n",
    "        result['tc_class'] = probs.argmax().item()\n",
    "\n",
    "    # SC classification\n",
    "    sc_pred = enc_out.get('sc_pred')\n",
    "    if sc_pred is not None:\n",
    "        result['sc_prob'] = torch.sigmoid(sc_pred[0]).item()\n",
    "        result['sc_pred'] = result['sc_prob'] > 0.5\n",
    "\n",
    "    # Family classification\n",
    "    family_composed = enc_out.get('family_composed_14')\n",
    "    if family_composed is not None:\n",
    "        result['family_pred_14'] = family_composed[0].cpu().argmax().item()\n",
    "\n",
    "    # High-pressure\n",
    "    hp_pred = enc_out.get('hp_pred')\n",
    "    if hp_pred is not None:\n",
    "        result['hp_prob'] = torch.sigmoid(hp_pred[0]).item()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run roundtrip validation\n",
    "print(\"=\" * 80)\n",
    "print(f\"ROUNDTRIP VALIDATION — Epoch {model_epoch}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Family':<14s} {'True Tc':>8s} {'Pred Tc':>9s} {'Err':>7s} {'Sim':>5s} {'SC?':>5s} {'FamPred':<16s} | Formula\")\n",
    "print(\"-\" * 120)\n",
    "\n",
    "roundtrip_results = []\n",
    "tc_errors = []\n",
    "magpie_mses = []\n",
    "formula_sims = []\n",
    "sc_correct = 0\n",
    "family_correct = 0\n",
    "\n",
    "for sample in holdout_samples:\n",
    "    formula = sample['formula']\n",
    "    true_tc = sample['Tc']\n",
    "    family = sample['family']\n",
    "    orig_idx = sample.get('original_index')\n",
    "    if orig_idx is None:\n",
    "        continue\n",
    "\n",
    "    idx_t = torch.tensor([orig_idx], dtype=torch.long)\n",
    "    decoded = full_forward(\n",
    "        encoder, decoder,\n",
    "        data['elem_idx'][idx_t], data['elem_frac'][idx_t],\n",
    "        data['elem_mask'][idx_t], data['magpie'][idx_t], data['tc'][idx_t],\n",
    "    )\n",
    "\n",
    "    pred_tc = decoded['tc_pred_kelvin']\n",
    "    gen_formula = decoded['formula']\n",
    "    tc_err = abs(pred_tc - true_tc)\n",
    "    tc_errors.append(tc_err)\n",
    "\n",
    "    mag_mse = F.mse_loss(decoded['magpie_pred'], data['magpie'][orig_idx]).item()\n",
    "    magpie_mses.append(mag_mse)\n",
    "\n",
    "    sim = element_similarity(gen_formula, formula)\n",
    "    formula_sims.append(sim)\n",
    "    exact = gen_formula.strip() == formula.strip()\n",
    "\n",
    "    sc_str = f\"{decoded.get('sc_prob', 0):.2f}\"\n",
    "    if decoded.get('sc_pred', False):\n",
    "        sc_correct += 1\n",
    "\n",
    "    family_pred_str = ''\n",
    "    if 'family_pred_14' in decoded:\n",
    "        pred_idx = decoded['family_pred_14']\n",
    "        pred_name = FAMILY_14_NAMES[pred_idx]\n",
    "        true_idx = HOLDOUT_FAMILY_TO_14.get(family, -1)\n",
    "        match = pred_idx == true_idx\n",
    "        if match:\n",
    "            family_correct += 1\n",
    "        family_pred_str = f\"{pred_name} {'OK' if match else 'X'}\"\n",
    "\n",
    "    exact_str = ' [EXACT]' if exact else ''\n",
    "    print(f\"  [{family:<12s}] {true_tc:8.1f} {pred_tc:9.1f} {tc_err:+7.1f} {sim:5.3f} {sc_str:>5s} {family_pred_str:<16s} | {formula}\")\n",
    "    if not exact:\n",
    "        print(f\"     -> {gen_formula}\")\n",
    "    else:\n",
    "        print(f\"     -> {gen_formula}{exact_str}\")\n",
    "\n",
    "    roundtrip_results.append({\n",
    "        'formula': formula, 'generated': gen_formula, 'exact': exact,\n",
    "        'similarity': sim, 'true_tc': true_tc, 'pred_tc': pred_tc,\n",
    "        'tc_error': tc_err, 'family': family,\n",
    "        'sc_prob': decoded.get('sc_prob'), 'family_pred': family_pred_str,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roundtrip Summary\n",
    "tc_arr = np.array(tc_errors)\n",
    "sim_arr = np.array(formula_sims)\n",
    "mag_arr = np.array(magpie_mses)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ROUNDTRIP SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nTc Prediction:\")\n",
    "print(f\"  MAE: {tc_arr.mean():.2f} K (median: {np.median(tc_arr):.2f} K)\")\n",
    "print(f\"  Within 1K: {(tc_arr < 1).sum()}/45 | Within 5K: {(tc_arr < 5).sum()}/45\")\n",
    "\n",
    "print(f\"\\nFormula Roundtrip:\")\n",
    "print(f\"  Mean similarity: {sim_arr.mean():.3f}\")\n",
    "print(f\"  Exact matches: {(sim_arr >= 0.999).sum()}/45\")\n",
    "print(f\"  > 0.95 similarity: {(sim_arr > 0.95).sum()}/45\")\n",
    "\n",
    "print(f\"\\nSC Classification: {sc_correct}/45 ({sc_correct/45*100:.1f}%)\")\n",
    "print(f\"Family Classification: {family_correct}/45 ({family_correct/45*100:.1f}%)\")\n",
    "print(f\"Magpie MSE: {mag_arr.mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Part 2: Targeted Holdout Search\n",
    "\n",
    "For each holdout formula, find training samples sharing the same elements, encode those as Z seeds, and explore the Z neighborhood using:\n",
    "\n",
    "1. **Fine-grained perturbation** (8 noise scales x 100 samples x 30 seeds)\n",
    "2. **Pairwise interpolation** (linear + SLERP, 15 steps, up to 100 pairs)\n",
    "3. **Centroid + random walks** (5 scales x 30 directions)\n",
    "4. **PCA-directed walks** (top 20 principal components of neighbor Z distribution)\n",
    "5. **Temperature sampling** (8 temperatures x 30 samples x 15 seeds)\n",
    "\n",
    "Total: ~27,500 Z-space candidates per target formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search parameters\n",
    "N_PERTURBATIONS = 100\n",
    "NOISE_SCALES = [0.02, 0.05, 0.08, 0.1, 0.15, 0.2, 0.3, 0.5]\n",
    "N_INTERPOLATION_STEPS = 15\n",
    "N_TEMPERATURE_SAMPLES = 30\n",
    "TEMPERATURES = [0.01, 0.05, 0.1, 0.2, 0.3, 0.5, 0.7, 1.0]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_indices(encoder, data, indices):\n",
    "    \"\"\"Encode specific dataset indices -> Z vectors.\"\"\"\n",
    "    batch_size = 128\n",
    "    all_z = []\n",
    "    for start in range(0, len(indices), batch_size):\n",
    "        batch_idx = indices[start:start + batch_size]\n",
    "        idx_t = torch.tensor(batch_idx, dtype=torch.long)\n",
    "        result = encoder.encode(\n",
    "            data['elem_idx'][idx_t].to(DEVICE),\n",
    "            data['elem_frac'][idx_t].to(DEVICE),\n",
    "            data['elem_mask'][idx_t].to(DEVICE),\n",
    "            data['magpie'][idx_t].to(DEVICE),\n",
    "            data['tc'][idx_t].to(DEVICE),\n",
    "        )\n",
    "        all_z.append(result['z'].cpu())\n",
    "    return torch.cat(all_z, dim=0)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def decode_z_batch(encoder, decoder, z_batch, has_numden_head=False, temperature=0.01):\n",
    "    \"\"\"Decode Z vectors -> formula strings.\"\"\"\n",
    "    batch_size = 64\n",
    "    all_formulas = []\n",
    "    for start in range(0, len(z_batch), batch_size):\n",
    "        z = z_batch[start:start + batch_size].to(DEVICE)\n",
    "        fraction_output = encoder.fraction_head(z)\n",
    "        fraction_pred = fraction_output[:, :encoder.max_elements]\n",
    "        element_count_pred = fraction_output[:, -1]\n",
    "        if has_numden_head and hasattr(encoder, 'numden_head'):\n",
    "            numden_pred = encoder.numden_head(z)\n",
    "            stoich_pred = torch.cat([fraction_pred, numden_pred, element_count_pred.unsqueeze(-1)], dim=-1)\n",
    "        else:\n",
    "            stoich_pred = torch.cat([fraction_pred, element_count_pred.unsqueeze(-1)], dim=-1)\n",
    "        generated, _, _ = decoder.generate_with_kv_cache(\n",
    "            z=z, stoich_pred=stoich_pred, temperature=temperature,\n",
    "        )\n",
    "        for i in range(len(z)):\n",
    "            all_formulas.append(tokens_to_formula(generated[i]))\n",
    "    return all_formulas\n",
    "\n",
    "\n",
    "def find_element_neighbors(target_formula, data, top_k=100):\n",
    "    \"\"\"Find training samples sharing elements with target.\"\"\"\n",
    "    parsed = parse_formula_elements(target_formula)\n",
    "    if not parsed:\n",
    "        return []\n",
    "    target_atomic_nums = set()\n",
    "    for elem in parsed.keys():\n",
    "        z = ELEMENT_TO_Z.get(elem)\n",
    "        if z:\n",
    "            target_atomic_nums.add(z)\n",
    "\n",
    "    scores = []\n",
    "    for i in data['train_indices']:\n",
    "        n_shared, jaccard = element_overlap_score(target_atomic_nums, data['elem_idx'][i])\n",
    "        if n_shared > 0:\n",
    "            scores.append((i, n_shared, jaccard))\n",
    "    scores.sort(key=lambda x: (-x[1], -x[2]))\n",
    "    return [s[0] for s in scores[:top_k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_single_target(encoder, decoder, data, target_formula, target_tc, target_family, has_numden_head=False):\n",
    "    \"\"\"Targeted search for a single holdout formula.\"\"\"\n",
    "    print(f\"\\n  TARGET: {target_formula} (Tc={target_tc}K, {target_family})\")\n",
    "\n",
    "    # Step 1: Find element-matched neighbors\n",
    "    neighbor_indices = find_element_neighbors(target_formula, data, top_k=100)\n",
    "    if len(neighbor_indices) < 3:\n",
    "        print(f\"    Only {len(neighbor_indices)} neighbors — skipping\")\n",
    "        return {'target': target_formula, 'target_tc': target_tc, 'target_family': target_family,\n",
    "                'best_sim': 0.0, 'best_gen': '', 'n_unique': 0, 'n_total': 0,\n",
    "                'exact': False, 'top_matches': [], 'top_frequent': []}\n",
    "\n",
    "    # Step 2: Encode neighbors\n",
    "    z_neighbors = encode_indices(encoder, data, neighbor_indices)\n",
    "    z_seeds = z_neighbors[:min(30, len(z_neighbors))]\n",
    "    print(f\"    {len(neighbor_indices)} neighbors, {len(z_seeds)} seeds, Z norm: {z_neighbors.norm(dim=-1).mean():.2f}\")\n",
    "\n",
    "    # Step 3: Generate candidate Z vectors\n",
    "    all_candidates_z = []\n",
    "\n",
    "    # Strategy 1: Fine-grained perturbation\n",
    "    for z in z_seeds:\n",
    "        z_exp = z.unsqueeze(0)\n",
    "        for scale in NOISE_SCALES:\n",
    "            noise = torch.randn(N_PERTURBATIONS, z.shape[0]) * scale\n",
    "            all_candidates_z.append(z_exp + noise)\n",
    "\n",
    "    # Strategy 2: Pairwise interpolation (linear + SLERP)\n",
    "    n_seeds = len(z_seeds)\n",
    "    max_pairs = min(n_seeds * (n_seeds - 1) // 2, 100)\n",
    "    pair_count = 0\n",
    "    for i in range(n_seeds):\n",
    "        for j in range(i + 1, n_seeds):\n",
    "            if pair_count >= max_pairs:\n",
    "                break\n",
    "            z1 = z_seeds[i].unsqueeze(0)\n",
    "            z2 = z_seeds[j].unsqueeze(0)\n",
    "            for t in np.linspace(0.05, 0.95, N_INTERPOLATION_STEPS):\n",
    "                all_candidates_z.append((1 - t) * z1 + t * z2)\n",
    "                all_candidates_z.append(slerp(z1, z2, float(t)))\n",
    "            pair_count += 1\n",
    "        if pair_count >= max_pairs:\n",
    "            break\n",
    "\n",
    "    # Strategy 3: Centroid + random direction walks\n",
    "    centroid = z_seeds.mean(dim=0, keepdim=True)\n",
    "    std = z_seeds.std(dim=0, keepdim=True).clamp(min=1e-6)\n",
    "    for scale in [0.3, 0.5, 1.0, 1.5, 2.0]:\n",
    "        directions = torch.randn(30, z_seeds.shape[1])\n",
    "        directions = F.normalize(directions, dim=-1)\n",
    "        all_candidates_z.append(centroid + scale * std * directions)\n",
    "\n",
    "    # Strategy 4: PCA-directed walks\n",
    "    if len(z_neighbors) >= 10:\n",
    "        z_np = z_neighbors.numpy()\n",
    "        mean = z_np.mean(axis=0)\n",
    "        centered = z_np - mean\n",
    "        U, S, Vt = np.linalg.svd(centered, full_matrices=False)\n",
    "        n_comp = min(20, len(S))\n",
    "        for c in range(n_comp):\n",
    "            direction = torch.from_numpy(Vt[c]).float()\n",
    "            std_along = S[c] / np.sqrt(len(z_neighbors) - 1)\n",
    "            for alpha in np.linspace(-3.0, 3.0, 20):\n",
    "                z_walked = torch.from_numpy(mean).float().unsqueeze(0) + alpha * std_along * direction.unsqueeze(0)\n",
    "                all_candidates_z.append(z_walked)\n",
    "\n",
    "    all_z = torch.cat(all_candidates_z, dim=0)\n",
    "    print(f\"    {len(all_z)} Z candidates (greedy decode)...\")\n",
    "\n",
    "    # Step 4: Greedy decode\n",
    "    t0 = time.time()\n",
    "    greedy_formulas = decode_z_batch(encoder, decoder, all_z, has_numden_head=has_numden_head, temperature=0.01)\n",
    "    print(f\"    Greedy decoded in {time.time()-t0:.1f}s\")\n",
    "\n",
    "    # Step 5: Temperature sampling\n",
    "    temp_formulas = []\n",
    "    for temp in TEMPERATURES:\n",
    "        for z_idx in range(min(len(z_seeds), 15)):\n",
    "            z_repeated = z_seeds[z_idx].unsqueeze(0).repeat(N_TEMPERATURE_SAMPLES, 1)\n",
    "            temp_formulas.extend(decode_z_batch(encoder, decoder, z_repeated, has_numden_head=has_numden_head, temperature=temp))\n",
    "\n",
    "    all_generated = greedy_formulas + temp_formulas\n",
    "    unique_formulas = set(f for f in all_generated if f and len(f) > 1)\n",
    "\n",
    "    # Step 6: Score candidates\n",
    "    formula_counts = defaultdict(int)\n",
    "    for f in all_generated:\n",
    "        if f and len(f) > 1:\n",
    "            formula_counts[f] += 1\n",
    "\n",
    "    candidates = []\n",
    "    best_sim = 0.0\n",
    "    best_gen = ''\n",
    "    top_matches = []\n",
    "\n",
    "    for formula_gen, count in formula_counts.items():\n",
    "        sim = element_similarity(formula_gen, target_formula)\n",
    "        candidates.append({'formula': formula_gen, 'count': count, 'similarity': sim})\n",
    "        if sim > best_sim:\n",
    "            best_sim = sim\n",
    "            best_gen = formula_gen\n",
    "        if sim >= 0.8:\n",
    "            top_matches.append((formula_gen, sim, count))\n",
    "\n",
    "    candidates.sort(key=lambda x: -x['similarity'])\n",
    "    top_matches.sort(key=lambda x: -x[1])\n",
    "\n",
    "    target_norm = target_formula.strip()\n",
    "    exact = any(f.strip() == target_norm for f in unique_formulas)\n",
    "\n",
    "    # Top by frequency\n",
    "    top_freq = sorted([(c['formula'], c['count']) for c in candidates], key=lambda x: -x[1])[:5]\n",
    "\n",
    "    print(f\"    Total: {len(all_generated)}, unique: {len(unique_formulas)}\")\n",
    "    print(f\"    RESULT: best_sim={best_sim:.4f}, exact={'YES' if exact else 'no'}\")\n",
    "    if top_matches[:3]:\n",
    "        for f, s, c in top_matches[:3]:\n",
    "            print(f\"      sim={s:.4f} ({c}x): {f}\")\n",
    "\n",
    "    return {\n",
    "        'target': target_formula, 'target_tc': target_tc, 'target_family': target_family,\n",
    "        'exact': exact, 'best_sim': float(best_sim), 'best_gen': best_gen,\n",
    "        'n_unique': len(unique_formulas), 'n_total': len(all_generated),\n",
    "        'n_neighbors': len(neighbor_indices),\n",
    "        'top_matches': [(f, float(s)) for f, s, c in top_matches[:10]],\n",
    "        'top_frequent': top_freq,\n",
    "        'all_candidates': candidates[:50],  # Top 50 by similarity\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run targeted search on all 45 holdout targets\n",
    "print(\"=\" * 80)\n",
    "print(f\"TARGETED HOLDOUT SEARCH — Epoch {model_epoch}\")\n",
    "print(\"Element-Anchored Z-Space Exploration\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "search_results = []\n",
    "t_start = time.time()\n",
    "\n",
    "for i, sample in enumerate(holdout_samples):\n",
    "    print(f\"\\n--- [{i+1}/45] ---\")\n",
    "    result = search_single_target(\n",
    "        encoder, decoder, data,\n",
    "        sample['formula'], sample['Tc'], sample['family'],\n",
    "        has_numden_head=has_numden_head,\n",
    "    )\n",
    "    search_results.append(result)\n",
    "\n",
    "total_time = time.time() - t_start\n",
    "print(f\"\\nTotal search time: {total_time/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search Results Summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"TARGETED SEARCH SUMMARY — Epoch {model_epoch}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for r in search_results:\n",
    "    marker = '***' if r.get('exact') else ' + ' if r['best_sim'] >= 0.95 else '   '\n",
    "    print(f\"{marker} [{r['target_family']:12s}] sim={r['best_sim']:.3f} | {r['target']}\")\n",
    "    if r['best_gen'] and not r.get('exact'):\n",
    "        print(f\"     Best: {r['best_gen']}\")\n",
    "\n",
    "n_exact = sum(1 for r in search_results if r.get('exact'))\n",
    "print(f\"\\nExact matches: {n_exact}/{len(search_results)}\")\n",
    "\n",
    "print(f\"\\nRESULTS BY THRESHOLD:\")\n",
    "for thresh in [1.0, 0.99, 0.98, 0.95, 0.90, 0.85, 0.80]:\n",
    "    found = sum(1 for r in search_results if r['best_sim'] >= thresh)\n",
    "    pct = found / len(search_results) * 100\n",
    "    bar = '#' * int(pct / 2)\n",
    "    print(f\"  >= {thresh:.2f}: {found:2d}/45 ({pct:5.1f}%) {bar}\")\n",
    "\n",
    "# Per-family breakdown\n",
    "print(f\"\\nPER-FAMILY BREAKDOWN:\")\n",
    "families = sorted(set(r['target_family'] for r in search_results))\n",
    "for fam in families:\n",
    "    fam_results = [r for r in search_results if r['target_family'] == fam]\n",
    "    n_exact_fam = sum(1 for r in fam_results if r.get('exact'))\n",
    "    n_095 = sum(1 for r in fam_results if r['best_sim'] >= 0.95)\n",
    "    avg_sim = np.mean([r['best_sim'] for r in fam_results])\n",
    "    print(f\"  {fam:14s}: exact={n_exact_fam}/5, >=0.95={n_095}/5, avg_sim={avg_sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: Similarity distribution\n",
    "sims = [r['best_sim'] for r in search_results]\n",
    "colors = ['#2ecc71' if s >= 0.95 else '#f39c12' if s >= 0.80 else '#e74c3c' for s in sims]\n",
    "axes[0].barh(range(len(sims)), sims, color=colors)\n",
    "axes[0].set_xlabel('Best Similarity')\n",
    "axes[0].set_ylabel('Holdout Target Index')\n",
    "axes[0].set_title('Best Match Similarity per Holdout Target')\n",
    "axes[0].axvline(x=0.95, color='green', linestyle='--', alpha=0.5, label='0.95')\n",
    "axes[0].axvline(x=0.80, color='orange', linestyle='--', alpha=0.5, label='0.80')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlim(0, 1.05)\n",
    "\n",
    "# Plot 2: Tc prediction error (roundtrip)\n",
    "fam_colors = {\n",
    "    'YBCO': '#e74c3c', 'LSCO': '#3498db', 'Hg-cuprate': '#2ecc71',\n",
    "    'Tl-cuprate': '#9b59b6', 'Bi-cuprate': '#f39c12', 'Iron-based': '#1abc9c',\n",
    "    'MgB2': '#e67e22', 'Conventional': '#95a5a6', 'Other': '#34495e',\n",
    "}\n",
    "for sample, err in zip(holdout_samples, tc_errors):\n",
    "    axes[1].scatter(sample['Tc'], err, color=fam_colors.get(sample['family'], 'gray'),\n",
    "                   s=50, alpha=0.7)\n",
    "axes[1].set_xlabel('True Tc (K)')\n",
    "axes[1].set_ylabel('Absolute Error (K)')\n",
    "axes[1].set_title('Tc Prediction Error vs True Tc')\n",
    "axes[1].axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Plot 3: Per-family average similarity\n",
    "fam_avg = {}\n",
    "for r in search_results:\n",
    "    fam = r['target_family']\n",
    "    fam_avg.setdefault(fam, []).append(r['best_sim'])\n",
    "fam_names = sorted(fam_avg.keys())\n",
    "fam_means = [np.mean(fam_avg[f]) for f in fam_names]\n",
    "bars = axes[2].barh(fam_names, fam_means, color=[fam_colors.get(f, 'gray') for f in fam_names])\n",
    "axes[2].set_xlabel('Average Best Similarity')\n",
    "axes[2].set_title('Average Similarity by Family')\n",
    "axes[2].set_xlim(0, 1.05)\n",
    "axes[2].axvline(x=0.95, color='green', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(REPO_DIR, 'outputs', 'generative_evaluation.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: outputs/generative_evaluation.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade card\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"GENERATIVE EVALUATION GRADE CARD — Epoch {model_epoch}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "tc_mae = np.mean(tc_errors)\n",
    "tc_within_1k = (np.array(tc_errors) < 1).sum()\n",
    "n_exact_search = sum(1 for r in search_results if r.get('exact'))\n",
    "n_095 = sum(1 for r in search_results if r['best_sim'] >= 0.95)\n",
    "n_099 = sum(1 for r in search_results if r['best_sim'] >= 0.99)\n",
    "avg_best_sim = np.mean([r['best_sim'] for r in search_results])\n",
    "\n",
    "def grade(val, thresholds):\n",
    "    \"\"\"A+, A, B+, B, C, D, F grading.\"\"\"\n",
    "    grades = ['A+', 'A', 'B+', 'B', 'C', 'D', 'F']\n",
    "    for i, t in enumerate(thresholds):\n",
    "        if val >= t:\n",
    "            return grades[i]\n",
    "    return grades[-1]\n",
    "\n",
    "metrics = [\n",
    "    ('SC Classification', f'{sc_correct}/45 ({sc_correct/45*100:.0f}%)',\n",
    "     grade(sc_correct/45, [0.98, 0.95, 0.90, 0.80, 0.70, 0.60])),\n",
    "    ('Tc Prediction MAE', f'{tc_mae:.2f}K',\n",
    "     grade(1/(1+tc_mae), [0.67, 0.50, 0.33, 0.20, 0.10, 0.05])),\n",
    "    ('Tc within 1K', f'{tc_within_1k}/45 ({tc_within_1k/45*100:.0f}%)',\n",
    "     grade(tc_within_1k/45, [0.90, 0.80, 0.70, 0.60, 0.50, 0.30])),\n",
    "    ('Family Classification', f'{family_correct}/45 ({family_correct/45*100:.0f}%)',\n",
    "     grade(family_correct/45, [0.95, 0.90, 0.80, 0.70, 0.60, 0.50])),\n",
    "    ('Formula Exact (search)', f'{n_exact_search}/45 ({n_exact_search/45*100:.0f}%)',\n",
    "     grade(n_exact_search/45, [0.80, 0.60, 0.40, 0.25, 0.15, 0.05])),\n",
    "    ('Formula >= 0.99 sim', f'{n_099}/45 ({n_099/45*100:.0f}%)',\n",
    "     grade(n_099/45, [0.90, 0.80, 0.60, 0.40, 0.25, 0.10])),\n",
    "    ('Formula >= 0.95 sim', f'{n_095}/45 ({n_095/45*100:.0f}%)',\n",
    "     grade(n_095/45, [0.95, 0.85, 0.70, 0.55, 0.40, 0.25])),\n",
    "    ('Average Best Similarity', f'{avg_best_sim:.3f}',\n",
    "     grade(avg_best_sim, [0.95, 0.90, 0.85, 0.80, 0.70, 0.60])),\n",
    "    ('Magpie MSE', f'{np.mean(magpie_mses):.4f}',\n",
    "     grade(1/(1+np.mean(magpie_mses)*10), [0.90, 0.80, 0.60, 0.40, 0.25, 0.10])),\n",
    "]\n",
    "\n",
    "print(f\"\\n{'Metric':<28s} {'Result':<28s} {'Grade':>5s}\")\n",
    "print(\"-\" * 65)\n",
    "for name, result, g in metrics:\n",
    "    print(f\"  {name:<26s} {result:<28s} {g:>5s}\")\n",
    "\n",
    "print(f\"\\n{'='*65}\")\n",
    "overall = (\n",
    "    f\"The model {'CAN' if n_exact_search >= 10 else 'CANNOT yet'} reliably generate \"\n",
    "    f\"unseen superconductors.\\n\"\n",
    "    f\"  - {n_exact_search}/45 exact matches from Z-space exploration\\n\"\n",
    "    f\"  - {n_095}/45 with >= 0.95 compositional similarity\\n\"\n",
    "    f\"  - Tc prediction: {tc_mae:.2f}K MAE on held-out materials\\n\"\n",
    ")\n",
    "print(overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON\n",
    "output = {\n",
    "    'timestamp': time.strftime('%Y-%m-%dT%H:%M:%S'),\n",
    "    'checkpoint': str(CHECKPOINT_PATH),\n",
    "    'epoch': model_epoch,\n",
    "    'roundtrip': {\n",
    "        'tc_mae_K': float(tc_mae),\n",
    "        'tc_within_1K': int(tc_within_1k),\n",
    "        'sc_accuracy': sc_correct / 45,\n",
    "        'family_accuracy': family_correct / 45,\n",
    "        'magpie_mse': float(np.mean(magpie_mses)),\n",
    "        'formula_mean_sim': float(np.mean(formula_sims)),\n",
    "        'details': roundtrip_results,\n",
    "    },\n",
    "    'search': {\n",
    "        'n_exact': n_exact_search,\n",
    "        'n_095': n_095,\n",
    "        'n_099': n_099,\n",
    "        'avg_best_sim': float(avg_best_sim),\n",
    "        'details': [{k: v for k, v in r.items() if k != 'all_candidates'} for r in search_results],\n",
    "    },\n",
    "}\n",
    "output_path = os.path.join(REPO_DIR, 'outputs', f'generative_evaluation_epoch_{model_epoch}.json')\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(output, f, indent=2, default=str)\n",
    "print(f\"Results saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}