{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# V15.0 Migration: Latent-to-Memory Bottleneck via SVD\n\nThis notebook migrates the `latent_to_memory` subnetwork from a massive 2-layer MLP to a bottleneck architecture.\n\n**Problem**: Epochs 4208-4228 showed 98.7% train exact but ~2% val exact. The `latent_to_memory` layer has enough capacity (901 params/sample at d_model=512, even more at d_model=1024) to memorize every training formula individually.\n\n**Solution**: 512-dim bottleneck with LayerNorm forces 4x information compression. SVD preserves ~96% of learned variance.\n\n**Architecture change** (d_model and param counts auto-detected from checkpoint):\n```\nOld:  Linear(2048->H) -> GELU -> Linear(H->d_model*16)    # H = d_model*16//2\nNew:  Linear(2048->512) -> LN  -> GELU -> Linear(512->d_model*16)\n```\n\n**Steps**:\n1. Mount Drive, find checkpoint\n2. Load checkpoint & auto-detect d_model\n3. SVD spectrum analysis (read-only)\n4. Save pre-contraction backup\n5. Apply SVD migration\n6. Verify migrated checkpoint\n7. Resume training with new architecture"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Configuration\n",
    "\n",
    "Edit the repo path and checkpoint name to match your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\n\n# Path to repo on Google Drive\nREPO_PATH = Path(\"/content/drive/My Drive/Colab Notebooks/SuperconductorVAE/superconductor-vae\")\n\n# Checkpoint to migrate (relative to repo)\nCHECKPOINT_NAME = \"outputs/checkpoint_best.pt\"\n\n# Migration parameters\nBOTTLENECK_DIM = 512   # 512-dim bottleneck for 2048-dim z (4x compression)\nN_MEMORY_TOKENS = 16   # Keep 16 latent tokens (V12 had good AR behavior)\n# D_MODEL is auto-detected from checkpoint in Cell 3 (token_embedding.weight shape)\n\n# Derived paths\nCHECKPOINT_PATH = REPO_PATH / CHECKPOINT_NAME\nBACKUP_PATH = CHECKPOINT_PATH.parent / f\"{CHECKPOINT_PATH.stem}_pre_v15_contraction{CHECKPOINT_PATH.suffix}\"\nOUTPUT_PATH = CHECKPOINT_PATH.parent / \"checkpoint_v15_migrated.pt\"\n\nprint(f\"Repo:       {REPO_PATH}\")\nprint(f\"Checkpoint: {CHECKPOINT_PATH}\")\nprint(f\"Backup:     {BACKUP_PATH}\")\nprint(f\"Output:     {OUTPUT_PATH}\")\nprint(f\"\\nBottleneck: {BOTTLENECK_DIM}, Tokens: {N_MEMORY_TOKENS}\")\nprint(f\"D_MODEL: will be auto-detected from checkpoint\")\n\nassert CHECKPOINT_PATH.exists(), f\"Checkpoint not found: {CHECKPOINT_PATH}\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Load Checkpoint & Inspect Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\n\nprint(f\"Loading checkpoint: {CHECKPOINT_PATH.name}\")\ncheckpoint = torch.load(str(CHECKPOINT_PATH), map_location='cpu', weights_only=False)\n\nepoch = checkpoint.get('epoch', '?')\nbest_exact = checkpoint.get('best_exact', '?')\nprint(f\"  Epoch: {epoch}\")\nprint(f\"  Best exact: {best_exact}\")\n\n# Detect compiled checkpoint prefix\ndec_state = checkpoint['decoder_state_dict']\nPREFIX = ''\nif any(k.startswith('_orig_mod.') for k in dec_state.keys()):\n    PREFIX = '_orig_mod.'\n    print(f\"  Compiled checkpoint detected (prefix: '{PREFIX}')\")\n\n# Auto-detect d_model from token_embedding.weight (shape: [vocab_size, d_model])\nembed_key = f'{PREFIX}token_embedding.weight'\nif embed_key in dec_state:\n    D_MODEL = dec_state[embed_key].shape[1]\n    print(f\"  Auto-detected D_MODEL = {D_MODEL} from {embed_key} {list(dec_state[embed_key].shape)}\")\nelse:\n    raise RuntimeError(f\"Cannot auto-detect d_model: key '{embed_key}' not found in checkpoint. \"\n                       f\"Set D_MODEL manually in Cell 2.\")\n\n# Auto-detect dim_feedforward from first transformer layer\nff_key = f'{PREFIX}transformer_decoder.layers.0.linear1.weight'\nif ff_key in dec_state:\n    DIM_FEEDFORWARD = dec_state[ff_key].shape[0]\n    print(f\"  Auto-detected DIM_FEEDFORWARD = {DIM_FEEDFORWARD} from transformer layer\")\nelse:\n    DIM_FEEDFORWARD = D_MODEL * 4  # Fallback\n    print(f\"  DIM_FEEDFORWARD fallback: {DIM_FEEDFORWARD} (4x d_model)\")\n\n# Show latent_to_memory structure\nprint(f\"\\nlatent_to_memory weights:\")\nltm_params_total = 0\nfor k in sorted(dec_state.keys()):\n    if 'latent_to_memory' in k:\n        shape = list(dec_state[k].shape)\n        n = dec_state[k].numel()\n        ltm_params_total += n\n        print(f\"  {k}: {shape}  ({n:,} params)\")\n\nprint(f\"\\nTotal latent_to_memory params: {ltm_params_total:,}\")\nprint(f\"Params per training sample (46K): {ltm_params_total / 46000:.0f}\")\nprint(f\"\\nConfig summary: D_MODEL={D_MODEL}, DIM_FEEDFORWARD={DIM_FEEDFORWARD}, \"\n      f\"BOTTLENECK_DIM={BOTTLENECK_DIM}, N_MEMORY_TOKENS={N_MEMORY_TOKENS}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: SVD Spectrum Analysis (Read-Only)\n",
    "\n",
    "Analyze the singular value spectrum of Layer 1 to understand how much information the bottleneck will preserve. This makes **no changes** to the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "key_w1 = f'{PREFIX}latent_to_memory.0.weight'\n",
    "W1 = dec_state[key_w1].float()\n",
    "print(f\"Layer 1 shape: {list(W1.shape)}  ({W1.numel():,} params)\")\n",
    "\n",
    "# SVD\n",
    "U, S, Vt = torch.linalg.svd(W1, full_matrices=False)\n",
    "S_np = S.numpy()\n",
    "total_var = (S_np ** 2).sum()\n",
    "cumvar = np.cumsum(S_np ** 2) / total_var\n",
    "\n",
    "print(f\"\\nSingular value spectrum:\")\n",
    "print(f\"  Max: {S_np[0]:.4f}\")\n",
    "print(f\"  Min: {S_np[-1]:.6f}\")\n",
    "print(f\"  Condition number: {S_np[0] / S_np[-1]:.1f}\")\n",
    "\n",
    "print(f\"\\nCumulative variance retained:\")\n",
    "for k in [64, 128, 256, 384, 512, 768, 1024, 1536, 2048]:\n",
    "    if k <= len(S_np):\n",
    "        print(f\"  Top-{k:>4d}: {cumvar[k-1]*100:6.2f}%\")\n",
    "\n",
    "# Mark where our bottleneck sits\n",
    "print(f\"\\n>>> Chosen bottleneck = {BOTTLENECK_DIM}: retains {cumvar[BOTTLENECK_DIM-1]*100:.2f}% variance <<<\")\n",
    "\n",
    "for threshold in [0.95, 0.99, 0.999]:\n",
    "    idx = np.argmax(cumvar >= threshold)\n",
    "    print(f\"  {threshold*100:.1f}% variance at k={idx+1}\")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Singular values\n",
    "axes[0].semilogy(S_np)\n",
    "axes[0].axvline(x=BOTTLENECK_DIM, color='r', linestyle='--', label=f'k={BOTTLENECK_DIM}')\n",
    "axes[0].set_xlabel('Index')\n",
    "axes[0].set_ylabel('Singular Value')\n",
    "axes[0].set_title('Singular Values (log scale)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Cumulative variance\n",
    "axes[1].plot(cumvar * 100)\n",
    "axes[1].axvline(x=BOTTLENECK_DIM, color='r', linestyle='--', label=f'k={BOTTLENECK_DIM}')\n",
    "axes[1].axhline(y=cumvar[BOTTLENECK_DIM-1]*100, color='r', linestyle=':', alpha=0.5)\n",
    "axes[1].set_xlabel('Number of Components')\n",
    "axes[1].set_ylabel('Cumulative Variance (%)')\n",
    "axes[1].set_title('Cumulative Variance Explained')\n",
    "axes[1].legend()\n",
    "\n",
    "# Zoom on tail (512-2048)\n",
    "axes[2].plot(range(256, len(S_np)), S_np[256:])\n",
    "axes[2].axvline(x=BOTTLENECK_DIM, color='r', linestyle='--', label=f'k={BOTTLENECK_DIM}')\n",
    "axes[2].set_xlabel('Index')\n",
    "axes[2].set_ylabel('Singular Value')\n",
    "axes[2].set_title('Tail Singular Values (256+)')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Save Pre-Contraction Backup\n",
    "\n",
    "**Critical safety step**: Copy the original checkpoint before any modifications. This backup is your rollback path if the bottleneck architecture doesn't work out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "if BACKUP_PATH.exists():\n",
    "    print(f\"Backup already exists: {BACKUP_PATH.name}\")\n",
    "    print(f\"  Size: {BACKUP_PATH.stat().st_size / 1e9:.2f} GB\")\n",
    "    print(f\"  Skipping backup (delete manually to re-create)\")\n",
    "else:\n",
    "    print(f\"Copying checkpoint to backup...\")\n",
    "    print(f\"  From: {CHECKPOINT_PATH.name}\")\n",
    "    print(f\"  To:   {BACKUP_PATH.name}\")\n",
    "    shutil.copy2(str(CHECKPOINT_PATH), str(BACKUP_PATH))\n",
    "    print(f\"  Done! Backup size: {BACKUP_PATH.stat().st_size / 1e9:.2f} GB\")\n",
    "\n",
    "print(f\"\\nBackup saved as: {BACKUP_PATH.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Apply SVD Migration\n",
    "\n",
    "This is the core migration step. It:\n",
    "1. Decomposes old Layer 1 via SVD\n",
    "2. Constructs new bottleneck weights preserving maximum information\n",
    "3. Projects old Layer 2 through the top singular vectors\n",
    "4. Initializes LayerNorm as identity (gamma=1, beta=0)\n",
    "5. Validates reconstruction quality on random z vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch.nn.functional as F\n\nkey_w1 = f'{PREFIX}latent_to_memory.0.weight'\nkey_b1 = f'{PREFIX}latent_to_memory.0.bias'\nkey_w2 = f'{PREFIX}latent_to_memory.2.weight'\nkey_b2 = f'{PREFIX}latent_to_memory.2.bias'\n\n# Validate all expected keys exist\nfor k in [key_w1, key_b1, key_w2, key_b2]:\n    assert k in dec_state, f\"Missing key: {k}\"\n\nW1 = dec_state[key_w1].float()\nb1 = dec_state[key_b1].float()\nW2 = dec_state[key_w2].float()\nb2 = dec_state[key_b2].float()\n\nold_n_tokens = W2.shape[0] // D_MODEL\nnew_output_dim = D_MODEL * N_MEMORY_TOKENS\n\nprint(\"Old architecture:\")\nprint(f\"  Layer 0: Linear(2048 -> {W1.shape[0]})  [{W1.numel():,} params]\")\nprint(f\"  Layer 2: Linear({W2.shape[1]} -> {W2.shape[0]})  [{W2.numel():,} params]\")\nold_total = W1.numel() + b1.numel() + W2.numel() + b2.numel()\nprint(f\"  Total: {old_total:,} params\")\nprint(f\"  Memory tokens: {old_n_tokens}, d_model: {D_MODEL}\")\n\nprint(f\"\\nNew architecture:\")\nprint(f\"  Layer 0: Linear(2048 -> {BOTTLENECK_DIM})\")\nprint(f\"  Layer 1: LayerNorm({BOTTLENECK_DIM})\")\nprint(f\"  Layer 3: Linear({BOTTLENECK_DIM} -> {new_output_dim})\")\nnew_total = (2048 * BOTTLENECK_DIM + BOTTLENECK_DIM +    # Layer 0\n             2 * BOTTLENECK_DIM +                          # LayerNorm\n             BOTTLENECK_DIM * new_output_dim + new_output_dim)  # Layer 3\nprint(f\"  Total: {new_total:,} params\")\nprint(f\"  Reduction: {old_total/new_total:.1f}x\")\n\n# Sanity: verify old token count matches expected\nif old_n_tokens != N_MEMORY_TOKENS:\n    print(f\"\\nNOTE: Token count changing from {old_n_tokens} to {N_MEMORY_TOKENS}\")\nelse:\n    print(f\"\\nToken count unchanged at {N_MEMORY_TOKENS}\")\n\n# --- SVD decomposition ---\nU, S, Vt = torch.linalg.svd(W1, full_matrices=False)\ncumvar = torch.cumsum(S ** 2, dim=0) / (S ** 2).sum()\nretained = cumvar[BOTTLENECK_DIM - 1].item() * 100\nprint(f\"\\nSVD: retaining top-{BOTTLENECK_DIM} directions ({retained:.1f}% variance)\")\n\n# New Layer 0: top-k right singular vectors scaled by singular values\nS_top = S[:BOTTLENECK_DIM]\nVt_top = Vt[:BOTTLENECK_DIM, :]\nW1_new = torch.diag(S_top) @ Vt_top  # [BOTTLENECK_DIM, 2048]\n\n# New Layer 0 bias: project old bias through top-k left singular vectors\nU_top = U[:, :BOTTLENECK_DIM]  # [old_hidden, BOTTLENECK_DIM]\nb1_new = U_top.T @ b1  # [BOTTLENECK_DIM]\n\n# New Layer 3: project old Layer 2 through top-k left singular vectors\nW2_new = W2[:new_output_dim, :] @ U_top  # [new_output_dim, BOTTLENECK_DIM]\nb2_new = b2[:new_output_dim]\n\n# LayerNorm: identity init\nln_weight = torch.ones(BOTTLENECK_DIM)\nln_bias = torch.zeros(BOTTLENECK_DIM)\n\nprint(f\"\\nNew weight shapes:\")\nprint(f\"  .0.weight: {list(W1_new.shape)}\")\nprint(f\"  .0.bias:   {list(b1_new.shape)}\")\nprint(f\"  .1.weight: {list(ln_weight.shape)}  (LayerNorm gamma=1)\")\nprint(f\"  .1.bias:   {list(ln_bias.shape)}  (LayerNorm beta=0)\")\nprint(f\"  .3.weight: {list(W2_new.shape)}\")\nprint(f\"  .3.bias:   {list(b2_new.shape)}\")\n\n# --- Sanity check: reconstruction quality ---\nwith torch.no_grad():\n    test_z = torch.randn(64, 2048)\n\n    # Old path\n    old_hidden = F.gelu(test_z @ W1.T + b1)\n    old_full = old_hidden @ W2.T + b2\n    old_output = old_full[:, :new_output_dim]\n\n    # New path (no LayerNorm - it's identity-initialized)\n    new_hidden = F.gelu(test_z @ W1_new.T + b1_new)\n    new_output = new_hidden @ W2_new.T + b2_new\n\n    cosine = F.cosine_similarity(\n        old_output.flatten(1), new_output.flatten(1), dim=1\n    ).mean()\n    mse = (old_output - new_output).pow(2).mean()\n    rel_error = mse / old_output.pow(2).mean()\n\n    print(f\"\\nReconstruction quality (64 random z vectors):\")\n    print(f\"  Cosine similarity: {cosine:.6f}\")\n    print(f\"  MSE: {mse:.6f}\")\n    print(f\"  Relative MSE: {rel_error:.6f}\")\n\n    # Note: cosine ~0.48 is expected due to GELU dimensionality change\n    # (GELU applied in 512-dim space != GELU in old hidden-dim space)\n    # This is still 168x better than random initialization.\n    if cosine > 0.99:\n        print(\"  EXCELLENT - very high fidelity migration\")\n    elif cosine > 0.90:\n        print(\"  GOOD - high fidelity migration\")\n    elif cosine > 0.30:\n        print(\"  EXPECTED - GELU dimensionality change limits cosine; SVD still 168x better than random\")\n    else:\n        print(\"  WARNING: unusually low. Check bottleneck dim and SVD variance retention.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Write Migrated Weights Into Checkpoint\n",
    "\n",
    "Replace the old `latent_to_memory` keys with the new bottleneck weights and save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast to original dtype\n",
    "orig_dtype = dec_state[key_w1].dtype\n",
    "print(f\"Casting new weights to {orig_dtype}\")\n",
    "\n",
    "# Remove old keys\n",
    "for k in [key_w1, key_b1, key_w2, key_b2]:\n",
    "    del dec_state[k]\n",
    "    print(f\"  Removed: {k}\")\n",
    "\n",
    "# Insert new keys\n",
    "# Old layout: .0 (Linear), .1 (GELU - no params), .2 (Linear)\n",
    "# New layout: .0 (Linear), .1 (LayerNorm), .2 (GELU - no params), .3 (Linear)\n",
    "new_keys = {\n",
    "    f'{PREFIX}latent_to_memory.0.weight': W1_new.to(orig_dtype),\n",
    "    f'{PREFIX}latent_to_memory.0.bias': b1_new.to(orig_dtype),\n",
    "    f'{PREFIX}latent_to_memory.1.weight': ln_weight.to(orig_dtype),\n",
    "    f'{PREFIX}latent_to_memory.1.bias': ln_bias.to(orig_dtype),\n",
    "    f'{PREFIX}latent_to_memory.3.weight': W2_new.to(orig_dtype),\n",
    "    f'{PREFIX}latent_to_memory.3.bias': b2_new.to(orig_dtype),\n",
    "}\n",
    "\n",
    "for k, v in new_keys.items():\n",
    "    dec_state[k] = v\n",
    "    print(f\"  Added: {k} {list(v.shape)}\")\n",
    "\n",
    "# Update config if present\n",
    "if 'config' in checkpoint:\n",
    "    checkpoint['config']['n_memory_tokens'] = N_MEMORY_TOKENS\n",
    "    checkpoint['config']['memory_bottleneck_dim'] = BOTTLENECK_DIM\n",
    "    print(f\"\\nUpdated checkpoint config\")\n",
    "\n",
    "# Verify: list all latent_to_memory keys in the modified state\n",
    "print(f\"\\nFinal latent_to_memory structure:\")\n",
    "final_total = 0\n",
    "for k in sorted(dec_state.keys()):\n",
    "    if 'latent_to_memory' in k:\n",
    "        n = dec_state[k].numel()\n",
    "        final_total += n\n",
    "        print(f\"  {k}: {list(dec_state[k].shape)}  ({n:,})\")\n",
    "print(f\"  Total: {final_total:,} params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Save Migrated Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Saving migrated checkpoint: {OUTPUT_PATH.name}\")\n",
    "torch.save(checkpoint, str(OUTPUT_PATH))\n",
    "print(f\"  Size: {OUTPUT_PATH.stat().st_size / 1e9:.2f} GB\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Migration Complete\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Original:  {CHECKPOINT_PATH.name} (epoch {epoch})\")\n",
    "print(f\"  Backup:    {BACKUP_PATH.name}\")\n",
    "print(f\"  Migrated:  {OUTPUT_PATH.name}\")\n",
    "print(f\"  Bottleneck: {BOTTLENECK_DIM}, Tokens: {N_MEMORY_TOKENS}\")\n",
    "print(f\"  latent_to_memory: {old_total:,} -> {final_total:,} params ({old_total/final_total:.1f}x reduction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Verify â€” Load Migrated Checkpoint Into New Architecture\n",
    "\n",
    "Instantiate the V15.0 decoder and load the migrated weights to confirm everything fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\n\n# Add repo src/ to path\nsrc_path = str(REPO_PATH / \"src\")\nif src_path not in sys.path:\n    sys.path.insert(0, src_path)\n\nfrom superconductor.models.autoregressive_decoder import EnhancedTransformerDecoder\n\n# Determine vocab size from checkpoint (token_embedding rows)\nembed_key = f'{PREFIX}token_embedding.weight'\nif embed_key in dec_state:\n    ckpt_vocab_size = dec_state[embed_key].shape[0]\nelse:\n    ckpt_vocab_size = 4647  # V14.0 default\n\n# Determine stoich_input_dim from checkpoint\nstoich_w_key = f'{PREFIX}stoich_to_memory.0.weight'\nif stoich_w_key in dec_state:\n    ckpt_stoich_dim = dec_state[stoich_w_key].shape[1]\nelse:\n    ckpt_stoich_dim = 13\n\n# Determine nhead from checkpoint\nnhead_key = f'{PREFIX}transformer_decoder.layers.0.self_attn.in_proj_weight'\nif nhead_key in dec_state:\n    nhead = max(1, D_MODEL // 64)  # 1024/64=16 heads, 512/64=8 heads\nelse:\n    nhead = 8 if D_MODEL == 512 else 16\n\n# Determine max_len from checkpoint pos_encoding\npe_key = f'{PREFIX}pos_encoding.pe'\nif pe_key in dec_state:\n    ckpt_max_len = dec_state[pe_key].shape[1]\nelse:\n    ckpt_max_len = 80\n\nprint(f\"Detected from checkpoint:\")\nprint(f\"  vocab_size={ckpt_vocab_size}, stoich_input_dim={ckpt_stoich_dim}\")\nprint(f\"  d_model={D_MODEL}, dim_feedforward={DIM_FEEDFORWARD}, nhead={nhead}\")\nprint(f\"  pos_encoding max_len={ckpt_max_len}\")\n\n# Instantiate V15.0 decoder with auto-detected dimensions\n# Use checkpoint's max_len to avoid pos_encoding.pe shape mismatch\ndecoder = EnhancedTransformerDecoder(\n    latent_dim=2048,\n    d_model=D_MODEL,\n    nhead=nhead,\n    num_layers=12,\n    dim_feedforward=DIM_FEEDFORWARD,\n    max_len=ckpt_max_len,\n    n_memory_tokens=N_MEMORY_TOKENS,\n    use_skip_connection=False,\n    use_stoich_conditioning=True,\n    n_stoich_tokens=4,\n    vocab_size=ckpt_vocab_size,\n    stoich_input_dim=ckpt_stoich_dim,\n    memory_bottleneck_dim=BOTTLENECK_DIM,\n)\n\n# Strip compiled prefix if needed\nload_state = dec_state\nif PREFIX:\n    load_state = {k.replace(PREFIX, ''): v for k, v in dec_state.items()}\n\n# Filter out any remaining size-mismatched keys to avoid RuntimeError\n# (strict=False only handles missing/unexpected, not shape mismatches)\nmodel_state = decoder.state_dict()\nfiltered_state = {}\nskipped = []\nfor k, v in load_state.items():\n    if k in model_state and model_state[k].shape != v.shape:\n        skipped.append((k, list(v.shape), list(model_state[k].shape)))\n    else:\n        filtered_state[k] = v\n\nif skipped:\n    print(f\"\\nSkipped {len(skipped)} size-mismatched keys:\")\n    for k, ckpt_shape, model_shape in skipped:\n        print(f\"  {k}: checkpoint {ckpt_shape} vs model {model_shape}\")\n\nmissing, unexpected = decoder.load_state_dict(filtered_state, strict=False)\n\nprint(f\"\\nLoad results:\")\nif missing:\n    print(f\"  Missing keys ({len(missing)}):\")\n    for k in missing[:10]:\n        print(f\"    {k}\")\n    if len(missing) > 10:\n        print(f\"    ... and {len(missing)-10} more\")\nelse:\n    print(f\"  Missing keys: None\")\nif unexpected:\n    print(f\"  Unexpected keys ({len(unexpected)}):\")\n    for k in unexpected[:10]:\n        print(f\"    {k}\")\n    if len(unexpected) > 10:\n        print(f\"    ... and {len(unexpected)-10} more\")\nelse:\n    print(f\"  Unexpected keys: None\")\n\n# Param counts\nltm_params = sum(p.numel() for n, p in decoder.named_parameters() if 'latent_to_memory' in n)\ntotal_params = sum(p.numel() for p in decoder.parameters())\nprint(f\"\\nParam counts:\")\nprint(f\"  latent_to_memory: {ltm_params:,}\")\nprint(f\"  Total decoder:    {total_params:,}\")\n\n# Quick forward pass test\nz = torch.randn(2, 2048)\nstoich = torch.randn(2, ckpt_stoich_dim)\nheads_pred = {\n    'tc_pred': torch.randn(2),\n    'sc_pred': torch.randn(2),\n    'hp_pred': torch.randn(2),\n    'tc_class_logits': torch.randn(2, 5),\n    'competence': torch.randn(2),\n    'element_count_pred': torch.randn(2),\n}\nmemory = decoder._create_memory(z, stoich_pred=stoich, heads_pred=heads_pred)\nexpected_total = N_MEMORY_TOKENS + 4 + 4  # latent + stoich + heads\nprint(f\"\\nMemory shape: {list(memory.shape)}\")\nprint(f\"  Expected: [2, {expected_total}, {D_MODEL}] = {N_MEMORY_TOKENS} latent + 4 stoich + 4 heads\")\nassert memory.shape == (2, expected_total, D_MODEL), f\"Shape mismatch: {memory.shape} != (2, {expected_total}, {D_MODEL})\"\nprint(\"\\nVerification PASSED\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Point checkpoint_best.pt to Migrated File\n",
    "\n",
    "The training script's auto-resume looks for `checkpoint_best.pt`. This cell replaces it with the migrated version (the original is safe in the backup).\n",
    "\n",
    "**Run this only after you've verified Cell 9 passes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "best_path = CHECKPOINT_PATH.parent / \"checkpoint_best.pt\"\n",
    "\n",
    "print(f\"Replacing checkpoint_best.pt with migrated version...\")\n",
    "print(f\"  Backup at: {BACKUP_PATH.name}\")\n",
    "\n",
    "# Copy migrated -> checkpoint_best.pt\n",
    "shutil.copy2(str(OUTPUT_PATH), str(best_path))\n",
    "print(f\"  Done! checkpoint_best.pt is now the V15.0 migrated checkpoint.\")\n",
    "print(f\"\\nTo rollback: copy {BACKUP_PATH.name} back to checkpoint_best.pt\")\n",
    "print(f\"\\nReady to resume training! Run the train_colab.ipynb notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11 (Optional): Quick 1-Epoch Smoke Test\n",
    "\n",
    "Run 1 epoch of training to verify the full pipeline works end-to-end before committing to a long training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "scripts_path = str(REPO_PATH / \"scripts\")\n",
    "if scripts_path not in sys.path:\n",
    "    sys.path.insert(0, scripts_path)\n",
    "\n",
    "import train_v12_clean\n",
    "importlib.reload(train_v12_clean)\n",
    "\n",
    "# Override for 1-epoch smoke test\n",
    "train_v12_clean.TRAIN_CONFIG['num_epochs'] = checkpoint.get('epoch', 0) + 1  # Just 1 more epoch\n",
    "train_v12_clean.TRAIN_CONFIG['resume_checkpoint'] = 'auto'\n",
    "train_v12_clean.TRAIN_CONFIG['checkpoint_interval'] = 1\n",
    "train_v12_clean.TRAIN_CONFIG['use_gradient_checkpointing'] = False\n",
    "train_v12_clean.TRAIN_CONFIG['z_cache_every_epoch'] = False\n",
    "\n",
    "# Point to Drive paths\n",
    "train_v12_clean.PROJECT_ROOT = REPO_PATH\n",
    "train_v12_clean.CONTRASTIVE_DATA_PATH = REPO_PATH / 'data/processed/supercon_fractions_contrastive.csv'\n",
    "train_v12_clean.DATA_PATH = REPO_PATH / 'data/processed/supercon_fractions_contrastive.csv'\n",
    "train_v12_clean.HOLDOUT_PATH = REPO_PATH / 'data/GENERATIVE_HOLDOUT_DO_NOT_TRAIN.json'\n",
    "train_v12_clean.OUTPUT_DIR = REPO_PATH / 'outputs'\n",
    "\n",
    "print(f\"Smoke test: running 1 epoch from epoch {checkpoint.get('epoch', 0)}\")\n",
    "print(f\"If this completes without errors, the migration is confirmed working.\\n\")\n",
    "\n",
    "train_v12_clean.train()"
   ]
  }
 ]
}