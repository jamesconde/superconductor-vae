{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V15.0 Migration: Latent-to-Memory Bottleneck via SVD\n",
    "\n",
    "This notebook migrates the `latent_to_memory` subnetwork from a 42M-param 2-layer MLP to a 5.3M-param bottleneck architecture.\n",
    "\n",
    "**Problem**: Epochs 4208-4228 showed 98.7% train exact but ~2% val exact. The `latent_to_memory` layer has 42M params (901 params/sample) - enough capacity to memorize every training formula individually.\n",
    "\n",
    "**Solution**: 512-dim bottleneck with LayerNorm forces 4x information compression. SVD preserves 96% of learned variance.\n",
    "\n",
    "**Architecture change**:\n",
    "```\n",
    "Old (42M):  Linear(2048->4096) -> GELU -> Linear(4096->8192)  -> reshape [16, 512]\n",
    "New (5.3M): Linear(2048->512)  -> LN   -> GELU -> Linear(512->8192) -> reshape [16, 512]\n",
    "```\n",
    "\n",
    "**Steps**:\n",
    "1. Mount Drive, find checkpoint\n",
    "2. SVD spectrum analysis (read-only)\n",
    "3. Save pre-contraction backup\n",
    "4. Apply SVD migration\n",
    "5. Verify migrated checkpoint\n",
    "6. Resume training with new architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Configuration\n",
    "\n",
    "Edit the repo path and checkpoint name to match your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Path to repo on Google Drive\n",
    "REPO_PATH = Path(\"/content/drive/My Drive/Colab Notebooks/SuperconductorVAE/superconductor-vae\")\n",
    "\n",
    "# Checkpoint to migrate (relative to repo)\n",
    "CHECKPOINT_NAME = \"outputs/checkpoint_best.pt\"\n",
    "\n",
    "# Migration parameters\n",
    "BOTTLENECK_DIM = 512   # 512-dim bottleneck for 2048-dim z (4x compression)\n",
    "N_MEMORY_TOKENS = 16   # Keep 16 latent tokens (V12 had good AR behavior)\n",
    "D_MODEL = 512          # Actual d_model in the checkpoint\n",
    "\n",
    "# Derived paths\n",
    "CHECKPOINT_PATH = REPO_PATH / CHECKPOINT_NAME\n",
    "BACKUP_PATH = CHECKPOINT_PATH.parent / f\"{CHECKPOINT_PATH.stem}_pre_v15_contraction{CHECKPOINT_PATH.suffix}\"\n",
    "OUTPUT_PATH = CHECKPOINT_PATH.parent / \"checkpoint_v15_migrated.pt\"\n",
    "\n",
    "print(f\"Repo:       {REPO_PATH}\")\n",
    "print(f\"Checkpoint: {CHECKPOINT_PATH}\")\n",
    "print(f\"Backup:     {BACKUP_PATH}\")\n",
    "print(f\"Output:     {OUTPUT_PATH}\")\n",
    "print(f\"\\nBottleneck: {BOTTLENECK_DIM}, Tokens: {N_MEMORY_TOKENS}, d_model: {D_MODEL}\")\n",
    "\n",
    "assert CHECKPOINT_PATH.exists(), f\"Checkpoint not found: {CHECKPOINT_PATH}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Load Checkpoint & Inspect Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"Loading checkpoint: {CHECKPOINT_PATH.name}\")\n",
    "checkpoint = torch.load(str(CHECKPOINT_PATH), map_location='cpu', weights_only=False)\n",
    "\n",
    "epoch = checkpoint.get('epoch', '?')\n",
    "best_exact = checkpoint.get('best_exact', '?')\n",
    "print(f\"  Epoch: {epoch}\")\n",
    "print(f\"  Best exact: {best_exact}\")\n",
    "\n",
    "# Detect compiled checkpoint prefix\n",
    "dec_state = checkpoint['decoder_state_dict']\n",
    "PREFIX = ''\n",
    "if any(k.startswith('_orig_mod.') for k in dec_state.keys()):\n",
    "    PREFIX = '_orig_mod.'\n",
    "    print(f\"  Compiled checkpoint detected (prefix: '{PREFIX}')\")\n",
    "\n",
    "# Show latent_to_memory structure\n",
    "print(f\"\\nlatent_to_memory weights:\")\n",
    "ltm_params_total = 0\n",
    "for k in sorted(dec_state.keys()):\n",
    "    if 'latent_to_memory' in k:\n",
    "        shape = list(dec_state[k].shape)\n",
    "        n = dec_state[k].numel()\n",
    "        ltm_params_total += n\n",
    "        print(f\"  {k}: {shape}  ({n:,} params)\")\n",
    "\n",
    "print(f\"\\nTotal latent_to_memory params: {ltm_params_total:,}\")\n",
    "print(f\"Params per training sample (46K): {ltm_params_total / 46000:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: SVD Spectrum Analysis (Read-Only)\n",
    "\n",
    "Analyze the singular value spectrum of Layer 1 to understand how much information the bottleneck will preserve. This makes **no changes** to the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "key_w1 = f'{PREFIX}latent_to_memory.0.weight'\n",
    "W1 = dec_state[key_w1].float()\n",
    "print(f\"Layer 1 shape: {list(W1.shape)}  ({W1.numel():,} params)\")\n",
    "\n",
    "# SVD\n",
    "U, S, Vt = torch.linalg.svd(W1, full_matrices=False)\n",
    "S_np = S.numpy()\n",
    "total_var = (S_np ** 2).sum()\n",
    "cumvar = np.cumsum(S_np ** 2) / total_var\n",
    "\n",
    "print(f\"\\nSingular value spectrum:\")\n",
    "print(f\"  Max: {S_np[0]:.4f}\")\n",
    "print(f\"  Min: {S_np[-1]:.6f}\")\n",
    "print(f\"  Condition number: {S_np[0] / S_np[-1]:.1f}\")\n",
    "\n",
    "print(f\"\\nCumulative variance retained:\")\n",
    "for k in [64, 128, 256, 384, 512, 768, 1024, 1536, 2048]:\n",
    "    if k <= len(S_np):\n",
    "        print(f\"  Top-{k:>4d}: {cumvar[k-1]*100:6.2f}%\")\n",
    "\n",
    "# Mark where our bottleneck sits\n",
    "print(f\"\\n>>> Chosen bottleneck = {BOTTLENECK_DIM}: retains {cumvar[BOTTLENECK_DIM-1]*100:.2f}% variance <<<\")\n",
    "\n",
    "for threshold in [0.95, 0.99, 0.999]:\n",
    "    idx = np.argmax(cumvar >= threshold)\n",
    "    print(f\"  {threshold*100:.1f}% variance at k={idx+1}\")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Singular values\n",
    "axes[0].semilogy(S_np)\n",
    "axes[0].axvline(x=BOTTLENECK_DIM, color='r', linestyle='--', label=f'k={BOTTLENECK_DIM}')\n",
    "axes[0].set_xlabel('Index')\n",
    "axes[0].set_ylabel('Singular Value')\n",
    "axes[0].set_title('Singular Values (log scale)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Cumulative variance\n",
    "axes[1].plot(cumvar * 100)\n",
    "axes[1].axvline(x=BOTTLENECK_DIM, color='r', linestyle='--', label=f'k={BOTTLENECK_DIM}')\n",
    "axes[1].axhline(y=cumvar[BOTTLENECK_DIM-1]*100, color='r', linestyle=':', alpha=0.5)\n",
    "axes[1].set_xlabel('Number of Components')\n",
    "axes[1].set_ylabel('Cumulative Variance (%)')\n",
    "axes[1].set_title('Cumulative Variance Explained')\n",
    "axes[1].legend()\n",
    "\n",
    "# Zoom on tail (512-2048)\n",
    "axes[2].plot(range(256, len(S_np)), S_np[256:])\n",
    "axes[2].axvline(x=BOTTLENECK_DIM, color='r', linestyle='--', label=f'k={BOTTLENECK_DIM}')\n",
    "axes[2].set_xlabel('Index')\n",
    "axes[2].set_ylabel('Singular Value')\n",
    "axes[2].set_title('Tail Singular Values (256+)')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Save Pre-Contraction Backup\n",
    "\n",
    "**Critical safety step**: Copy the original checkpoint before any modifications. This backup is your rollback path if the bottleneck architecture doesn't work out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "if BACKUP_PATH.exists():\n",
    "    print(f\"Backup already exists: {BACKUP_PATH.name}\")\n",
    "    print(f\"  Size: {BACKUP_PATH.stat().st_size / 1e9:.2f} GB\")\n",
    "    print(f\"  Skipping backup (delete manually to re-create)\")\n",
    "else:\n",
    "    print(f\"Copying checkpoint to backup...\")\n",
    "    print(f\"  From: {CHECKPOINT_PATH.name}\")\n",
    "    print(f\"  To:   {BACKUP_PATH.name}\")\n",
    "    shutil.copy2(str(CHECKPOINT_PATH), str(BACKUP_PATH))\n",
    "    print(f\"  Done! Backup size: {BACKUP_PATH.stat().st_size / 1e9:.2f} GB\")\n",
    "\n",
    "print(f\"\\nBackup saved as: {BACKUP_PATH.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Apply SVD Migration\n",
    "\n",
    "This is the core migration step. It:\n",
    "1. Decomposes old Layer 1 via SVD\n",
    "2. Constructs new bottleneck weights preserving maximum information\n",
    "3. Projects old Layer 2 through the top singular vectors\n",
    "4. Initializes LayerNorm as identity (gamma=1, beta=0)\n",
    "5. Validates reconstruction quality on random z vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "key_w1 = f'{PREFIX}latent_to_memory.0.weight'\n",
    "key_b1 = f'{PREFIX}latent_to_memory.0.bias'\n",
    "key_w2 = f'{PREFIX}latent_to_memory.2.weight'\n",
    "key_b2 = f'{PREFIX}latent_to_memory.2.bias'\n",
    "\n",
    "# Validate all expected keys exist\n",
    "for k in [key_w1, key_b1, key_w2, key_b2]:\n",
    "    assert k in dec_state, f\"Missing key: {k}\"\n",
    "\n",
    "W1 = dec_state[key_w1].float()  # [4096, 2048]\n",
    "b1 = dec_state[key_b1].float()  # [4096]\n",
    "W2 = dec_state[key_w2].float()  # [8192, 4096]\n",
    "b2 = dec_state[key_b2].float()  # [8192]\n",
    "\n",
    "old_n_tokens = W2.shape[0] // D_MODEL\n",
    "new_output_dim = D_MODEL * N_MEMORY_TOKENS\n",
    "\n",
    "print(\"Old architecture:\")\n",
    "print(f\"  Layer 0: Linear(2048 -> {W1.shape[0]})  [{W1.numel():,} params]\")\n",
    "print(f\"  Layer 2: Linear({W2.shape[1]} -> {W2.shape[0]})  [{W2.numel():,} params]\")\n",
    "old_total = W1.numel() + b1.numel() + W2.numel() + b2.numel()\n",
    "print(f\"  Total: {old_total:,} params\")\n",
    "print(f\"  Memory tokens: {old_n_tokens}, d_model: {D_MODEL}\")\n",
    "\n",
    "print(f\"\\nNew architecture:\")\n",
    "print(f\"  Layer 0: Linear(2048 -> {BOTTLENECK_DIM})\")\n",
    "print(f\"  Layer 1: LayerNorm({BOTTLENECK_DIM})\")\n",
    "print(f\"  Layer 3: Linear({BOTTLENECK_DIM} -> {new_output_dim})\")\n",
    "new_total = (2048 * BOTTLENECK_DIM + BOTTLENECK_DIM +    # Layer 0\n",
    "             2 * BOTTLENECK_DIM +                          # LayerNorm\n",
    "             BOTTLENECK_DIM * new_output_dim + new_output_dim)  # Layer 3\n",
    "print(f\"  Total: {new_total:,} params\")\n",
    "print(f\"  Reduction: {old_total/new_total:.1f}x\")\n",
    "\n",
    "# --- SVD decomposition ---\n",
    "U, S, Vt = torch.linalg.svd(W1, full_matrices=False)\n",
    "cumvar = torch.cumsum(S ** 2, dim=0) / (S ** 2).sum()\n",
    "retained = cumvar[BOTTLENECK_DIM - 1].item() * 100\n",
    "print(f\"\\nSVD: retaining top-{BOTTLENECK_DIM} directions ({retained:.1f}% variance)\")\n",
    "\n",
    "# New Layer 0: top-k right singular vectors scaled by singular values\n",
    "S_top = S[:BOTTLENECK_DIM]\n",
    "Vt_top = Vt[:BOTTLENECK_DIM, :]\n",
    "W1_new = torch.diag(S_top) @ Vt_top  # [BOTTLENECK_DIM, 2048]\n",
    "\n",
    "# New Layer 0 bias: project old bias through top-k left singular vectors\n",
    "U_top = U[:, :BOTTLENECK_DIM]  # [4096, BOTTLENECK_DIM]\n",
    "b1_new = U_top.T @ b1  # [BOTTLENECK_DIM]\n",
    "\n",
    "# New Layer 3: project old Layer 2 through top-k left singular vectors\n",
    "W2_new = W2[:new_output_dim, :] @ U_top  # [new_output_dim, BOTTLENECK_DIM]\n",
    "b2_new = b2[:new_output_dim]\n",
    "\n",
    "# LayerNorm: identity init\n",
    "ln_weight = torch.ones(BOTTLENECK_DIM)\n",
    "ln_bias = torch.zeros(BOTTLENECK_DIM)\n",
    "\n",
    "print(f\"\\nNew weight shapes:\")\n",
    "print(f\"  .0.weight: {list(W1_new.shape)}\")\n",
    "print(f\"  .0.bias:   {list(b1_new.shape)}\")\n",
    "print(f\"  .1.weight: {list(ln_weight.shape)}  (LayerNorm gamma=1)\")\n",
    "print(f\"  .1.bias:   {list(ln_bias.shape)}  (LayerNorm beta=0)\")\n",
    "print(f\"  .3.weight: {list(W2_new.shape)}\")\n",
    "print(f\"  .3.bias:   {list(b2_new.shape)}\")\n",
    "\n",
    "# --- Sanity check: reconstruction quality ---\n",
    "with torch.no_grad():\n",
    "    test_z = torch.randn(64, 2048)\n",
    "\n",
    "    # Old path\n",
    "    old_hidden = F.gelu(test_z @ W1.T + b1)\n",
    "    old_full = old_hidden @ W2.T + b2\n",
    "    old_output = old_full[:, :new_output_dim]\n",
    "\n",
    "    # New path (no LayerNorm - it's identity-initialized)\n",
    "    new_hidden = F.gelu(test_z @ W1_new.T + b1_new)\n",
    "    new_output = new_hidden @ W2_new.T + b2_new\n",
    "\n",
    "    cosine = F.cosine_similarity(\n",
    "        old_output.flatten(1), new_output.flatten(1), dim=1\n",
    "    ).mean()\n",
    "    mse = (old_output - new_output).pow(2).mean()\n",
    "    rel_error = mse / old_output.pow(2).mean()\n",
    "\n",
    "    print(f\"\\nReconstruction quality (64 random z vectors):\")\n",
    "    print(f\"  Cosine similarity: {cosine:.6f}\")\n",
    "    print(f\"  MSE: {mse:.6f}\")\n",
    "    print(f\"  Relative MSE: {rel_error:.6f}\")\n",
    "\n",
    "    if cosine > 0.99:\n",
    "        print(\"  EXCELLENT - very high fidelity migration\")\n",
    "    elif cosine > 0.95:\n",
    "        print(\"  GOOD - high fidelity migration\")\n",
    "    elif cosine > 0.90:\n",
    "        print(\"  ACCEPTABLE - moderate information loss\")\n",
    "    else:\n",
    "        print(\"  WARNING: significant information loss. Consider larger bottleneck.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Write Migrated Weights Into Checkpoint\n",
    "\n",
    "Replace the old `latent_to_memory` keys with the new bottleneck weights and save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast to original dtype\n",
    "orig_dtype = dec_state[key_w1].dtype\n",
    "print(f\"Casting new weights to {orig_dtype}\")\n",
    "\n",
    "# Remove old keys\n",
    "for k in [key_w1, key_b1, key_w2, key_b2]:\n",
    "    del dec_state[k]\n",
    "    print(f\"  Removed: {k}\")\n",
    "\n",
    "# Insert new keys\n",
    "# Old layout: .0 (Linear), .1 (GELU - no params), .2 (Linear)\n",
    "# New layout: .0 (Linear), .1 (LayerNorm), .2 (GELU - no params), .3 (Linear)\n",
    "new_keys = {\n",
    "    f'{PREFIX}latent_to_memory.0.weight': W1_new.to(orig_dtype),\n",
    "    f'{PREFIX}latent_to_memory.0.bias': b1_new.to(orig_dtype),\n",
    "    f'{PREFIX}latent_to_memory.1.weight': ln_weight.to(orig_dtype),\n",
    "    f'{PREFIX}latent_to_memory.1.bias': ln_bias.to(orig_dtype),\n",
    "    f'{PREFIX}latent_to_memory.3.weight': W2_new.to(orig_dtype),\n",
    "    f'{PREFIX}latent_to_memory.3.bias': b2_new.to(orig_dtype),\n",
    "}\n",
    "\n",
    "for k, v in new_keys.items():\n",
    "    dec_state[k] = v\n",
    "    print(f\"  Added: {k} {list(v.shape)}\")\n",
    "\n",
    "# Update config if present\n",
    "if 'config' in checkpoint:\n",
    "    checkpoint['config']['n_memory_tokens'] = N_MEMORY_TOKENS\n",
    "    checkpoint['config']['memory_bottleneck_dim'] = BOTTLENECK_DIM\n",
    "    print(f\"\\nUpdated checkpoint config\")\n",
    "\n",
    "# Verify: list all latent_to_memory keys in the modified state\n",
    "print(f\"\\nFinal latent_to_memory structure:\")\n",
    "final_total = 0\n",
    "for k in sorted(dec_state.keys()):\n",
    "    if 'latent_to_memory' in k:\n",
    "        n = dec_state[k].numel()\n",
    "        final_total += n\n",
    "        print(f\"  {k}: {list(dec_state[k].shape)}  ({n:,})\")\n",
    "print(f\"  Total: {final_total:,} params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Save Migrated Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Saving migrated checkpoint: {OUTPUT_PATH.name}\")\n",
    "torch.save(checkpoint, str(OUTPUT_PATH))\n",
    "print(f\"  Size: {OUTPUT_PATH.stat().st_size / 1e9:.2f} GB\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Migration Complete\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Original:  {CHECKPOINT_PATH.name} (epoch {epoch})\")\n",
    "print(f\"  Backup:    {BACKUP_PATH.name}\")\n",
    "print(f\"  Migrated:  {OUTPUT_PATH.name}\")\n",
    "print(f\"  Bottleneck: {BOTTLENECK_DIM}, Tokens: {N_MEMORY_TOKENS}\")\n",
    "print(f\"  latent_to_memory: {old_total:,} -> {final_total:,} params ({old_total/final_total:.1f}x reduction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Verify â€” Load Migrated Checkpoint Into New Architecture\n",
    "\n",
    "Instantiate the V15.0 decoder and load the migrated weights to confirm everything fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Add repo src/ to path\n",
    "src_path = str(REPO_PATH / \"src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "from superconductor.models.autoregressive_decoder import EnhancedTransformerDecoder\n",
    "\n",
    "# Determine vocab size from checkpoint (token_embedding rows)\n",
    "embed_key = f'{PREFIX}token_embedding.weight'\n",
    "if embed_key in dec_state:\n",
    "    ckpt_vocab_size = dec_state[embed_key].shape[0]\n",
    "else:\n",
    "    ckpt_vocab_size = 4647  # V14.0 default\n",
    "\n",
    "# Determine stoich_input_dim from checkpoint\n",
    "stoich_w_key = f'{PREFIX}stoich_to_memory.0.weight'\n",
    "if stoich_w_key in dec_state:\n",
    "    ckpt_stoich_dim = dec_state[stoich_w_key].shape[1]\n",
    "else:\n",
    "    ckpt_stoich_dim = 13\n",
    "\n",
    "print(f\"Detected from checkpoint: vocab_size={ckpt_vocab_size}, stoich_input_dim={ckpt_stoich_dim}\")\n",
    "\n",
    "# Instantiate V15.0 decoder\n",
    "decoder = EnhancedTransformerDecoder(\n",
    "    latent_dim=2048,\n",
    "    d_model=D_MODEL,\n",
    "    nhead=8,\n",
    "    num_layers=12,\n",
    "    dim_feedforward=2048,\n",
    "    n_memory_tokens=N_MEMORY_TOKENS,\n",
    "    use_skip_connection=False,\n",
    "    use_stoich_conditioning=True,\n",
    "    n_stoich_tokens=4,\n",
    "    vocab_size=ckpt_vocab_size,\n",
    "    stoich_input_dim=ckpt_stoich_dim,\n",
    "    memory_bottleneck_dim=BOTTLENECK_DIM,\n",
    ")\n",
    "\n",
    "# Strip compiled prefix if needed\n",
    "load_state = dec_state\n",
    "if PREFIX:\n",
    "    load_state = {k.replace(PREFIX, ''): v for k, v in dec_state.items()}\n",
    "\n",
    "missing, unexpected = decoder.load_state_dict(load_state, strict=False)\n",
    "\n",
    "print(f\"\\nLoad results:\")\n",
    "if missing:\n",
    "    print(f\"  Missing keys ({len(missing)}): {missing[:5]}{'...' if len(missing)>5 else ''}\")\n",
    "else:\n",
    "    print(f\"  Missing keys: None\")\n",
    "if unexpected:\n",
    "    print(f\"  Unexpected keys ({len(unexpected)}): {unexpected[:5]}{'...' if len(unexpected)>5 else ''}\")\n",
    "else:\n",
    "    print(f\"  Unexpected keys: None\")\n",
    "\n",
    "# Param counts\n",
    "ltm_params = sum(p.numel() for n, p in decoder.named_parameters() if 'latent_to_memory' in n)\n",
    "total_params = sum(p.numel() for p in decoder.parameters())\n",
    "print(f\"\\nParam counts:\")\n",
    "print(f\"  latent_to_memory: {ltm_params:,}\")\n",
    "print(f\"  Total decoder:    {total_params:,}\")\n",
    "\n",
    "# Quick forward pass test\n",
    "z = torch.randn(2, 2048)\n",
    "stoich = torch.randn(2, ckpt_stoich_dim)\n",
    "heads_pred = {\n",
    "    'tc_pred': torch.randn(2),\n",
    "    'sc_pred': torch.randn(2),\n",
    "    'hp_pred': torch.randn(2),\n",
    "    'tc_class_logits': torch.randn(2, 5),\n",
    "    'competence': torch.randn(2),\n",
    "    'element_count_pred': torch.randn(2),\n",
    "}\n",
    "memory = decoder._create_memory(z, stoich_pred=stoich, heads_pred=heads_pred)\n",
    "print(f\"\\nMemory shape: {list(memory.shape)}\")\n",
    "print(f\"  Expected: [2, 24, {D_MODEL}] = 16 latent + 4 stoich + 4 heads\")\n",
    "assert memory.shape == (2, 24, D_MODEL), f\"Shape mismatch: {memory.shape}\"\n",
    "print(\"\\nVerification PASSED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Point checkpoint_best.pt to Migrated File\n",
    "\n",
    "The training script's auto-resume looks for `checkpoint_best.pt`. This cell replaces it with the migrated version (the original is safe in the backup).\n",
    "\n",
    "**Run this only after you've verified Cell 9 passes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "best_path = CHECKPOINT_PATH.parent / \"checkpoint_best.pt\"\n",
    "\n",
    "print(f\"Replacing checkpoint_best.pt with migrated version...\")\n",
    "print(f\"  Backup at: {BACKUP_PATH.name}\")\n",
    "\n",
    "# Copy migrated -> checkpoint_best.pt\n",
    "shutil.copy2(str(OUTPUT_PATH), str(best_path))\n",
    "print(f\"  Done! checkpoint_best.pt is now the V15.0 migrated checkpoint.\")\n",
    "print(f\"\\nTo rollback: copy {BACKUP_PATH.name} back to checkpoint_best.pt\")\n",
    "print(f\"\\nReady to resume training! Run the train_colab.ipynb notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11 (Optional): Quick 1-Epoch Smoke Test\n",
    "\n",
    "Run 1 epoch of training to verify the full pipeline works end-to-end before committing to a long training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "scripts_path = str(REPO_PATH / \"scripts\")\n",
    "if scripts_path not in sys.path:\n",
    "    sys.path.insert(0, scripts_path)\n",
    "\n",
    "import train_v12_clean\n",
    "importlib.reload(train_v12_clean)\n",
    "\n",
    "# Override for 1-epoch smoke test\n",
    "train_v12_clean.TRAIN_CONFIG['num_epochs'] = checkpoint.get('epoch', 0) + 1  # Just 1 more epoch\n",
    "train_v12_clean.TRAIN_CONFIG['resume_checkpoint'] = 'auto'\n",
    "train_v12_clean.TRAIN_CONFIG['checkpoint_interval'] = 1\n",
    "train_v12_clean.TRAIN_CONFIG['use_gradient_checkpointing'] = False\n",
    "train_v12_clean.TRAIN_CONFIG['z_cache_every_epoch'] = False\n",
    "\n",
    "# Point to Drive paths\n",
    "train_v12_clean.PROJECT_ROOT = REPO_PATH\n",
    "train_v12_clean.CONTRASTIVE_DATA_PATH = REPO_PATH / 'data/processed/supercon_fractions_contrastive.csv'\n",
    "train_v12_clean.DATA_PATH = REPO_PATH / 'data/processed/supercon_fractions_contrastive.csv'\n",
    "train_v12_clean.HOLDOUT_PATH = REPO_PATH / 'data/GENERATIVE_HOLDOUT_DO_NOT_TRAIN.json'\n",
    "train_v12_clean.OUTPUT_DIR = REPO_PATH / 'outputs'\n",
    "\n",
    "print(f\"Smoke test: running 1 epoch from epoch {checkpoint.get('epoch', 0)}\")\n",
    "print(f\"If this completes without errors, the migration is confirmed working.\\n\")\n",
    "\n",
    "train_v12_clean.train()"
   ]
  }
 ]
}