{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Superconductor VAE - Google Colab Training\n\nTrain the FullMaterialsVAE (V12) on Google Colab using your repo uploaded to Google Drive.\n\n**Setup**: Upload the entire `superconductor-vae` repository to your Google Drive, then run these cells in order.\n\n**Checkpoints**: Saved to `outputs/` inside the repo on Drive, so they persist across Colab sessions.\n\n**Remote monitoring (optional)**: Set `GIST_ID` in Cell 2 to push live training metrics to a GitHub Gist. This lets you (or Claude Code) check training progress without being in the Colab tab."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 2: Configuration\n\nEdit these values to match your setup."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Path to the superconductor-vae repo on your Google Drive\nREPO_PATH = \"/content/drive/MyDrive/superconductor-vae\"\n\n# Training options (override defaults)\nRESUME_FROM_CHECKPOINT = True   # Resume from best checkpoint if available\nNUM_EPOCHS = 2000               # Total epochs (training resumes from last epoch)\nBATCH_SIZE = 'auto'             # 'auto' scales with GPU memory, or set integer (e.g. 32, 48)\n\n# --- Remote monitoring via GitHub Gist ---\n# Set GIST_ID to enable live training metrics visible outside Colab.\n# 1. Create a personal access token at https://github.com/settings/tokens with \"gist\" scope\n# 2. In Colab: click the key icon (left sidebar) > add secret named GITHUB_TOKEN\n# 3. Create a gist at https://gist.github.com with one file named \"training_log.json\"\n#    containing just \"{}\" — then copy the gist ID from the URL.\n# Set to None to disable gist logging.\nGIST_ID = \"acceed7daef4d6893801cc7337531b68\"\nGIST_LOG_EVERY = 5  # Update gist every N epochs"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Install Dependencies\n",
    "\n",
    "PyTorch, NumPy, pandas, and scikit-learn are pre-installed on Colab. We only need matminer and pymatgen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matminer pymatgen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Setup Paths and Verify Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "repo = Path(REPO_PATH)\n",
    "\n",
    "# Add src/ to Python path so imports work\n",
    "src_path = str(repo / \"src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "# Verify key files exist\n",
    "required_files = {\n",
    "    \"Training data\": repo / \"data/processed/supercon_fractions_combined.csv\",\n",
    "    \"Holdout set\": repo / \"data/GENERATIVE_HOLDOUT_DO_NOT_TRAIN.json\",\n",
    "    \"Training script\": repo / \"scripts/train_v12_clean.py\",\n",
    "    \"VAE model\": repo / \"src/superconductor/models/attention_vae.py\",\n",
    "    \"Decoder model\": repo / \"src/superconductor/models/autoregressive_decoder.py\",\n",
    "}\n",
    "\n",
    "all_found = True\n",
    "for name, path in required_files.items():\n",
    "    exists = path.exists()\n",
    "    status = \"OK\" if exists else \"MISSING\"\n",
    "    print(f\"  [{status}] {name}: {path}\")\n",
    "    if not exists:\n",
    "        all_found = False\n",
    "\n",
    "if not all_found:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing files. Check that REPO_PATH is correct: {REPO_PATH}\"\n",
    "    )\n",
    "\n",
    "print()\n",
    "\n",
    "# GPU info\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    capability = torch.cuda.get_device_capability(0)\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"Memory: {gpu_mem:.1f} GB\")\n",
    "    print(f\"Compute capability: {capability[0]}.{capability[1]}\")\n",
    "    print(f\"PyTorch: {torch.__version__}\")\n",
    "    print(f\"CUDA: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected! Go to Runtime > Change runtime type > GPU.\")\n",
    "    print(f\"PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Apply Colab-Specific Config Overrides\n",
    "\n",
    "Patches the training config for Colab compatibility before importing the training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import importlib\n\n# Add scripts/ to path so we can import train_v12_clean as a module\nscripts_path = str(repo / \"scripts\")\nif scripts_path not in sys.path:\n    sys.path.insert(0, scripts_path)\n\n# Import the training module\nimport train_v12_clean\n\n# --- Colab-specific overrides ---\n\n# DataLoader: Colab's containerized env has issues with multiprocessing workers\ntrain_v12_clean.TRAIN_CONFIG['num_workers'] = 0\ntrain_v12_clean.TRAIN_CONFIG['persistent_workers'] = False\ntrain_v12_clean.TRAIN_CONFIG['pin_memory'] = True\n\n# torch.compile: Colab often lacks the C++ toolchain (gcc/triton) needed\ntrain_v12_clean.TRAIN_CONFIG['use_torch_compile'] = False\n\n# More frequent checkpoints — Colab can disconnect even on Pro tier\ntrain_v12_clean.TRAIN_CONFIG['checkpoint_interval'] = 25\n\n# Gradient checkpointing not needed on A100/L4 (40GB+ VRAM)\ntrain_v12_clean.TRAIN_CONFIG['use_gradient_checkpointing'] = False\n\n# --- User overrides from Cell 2 ---\n\ntrain_v12_clean.TRAIN_CONFIG['num_epochs'] = NUM_EPOCHS\n\nif BATCH_SIZE != 'auto':\n    train_v12_clean.TRAIN_CONFIG['batch_size'] = int(BATCH_SIZE)\n\nif RESUME_FROM_CHECKPOINT:\n    train_v12_clean.TRAIN_CONFIG['resume_checkpoint'] = 'outputs/checkpoint_best.pt'\nelse:\n    train_v12_clean.TRAIN_CONFIG['resume_checkpoint'] = None\n\n# --- Redirect paths to Drive-based repo ---\n\ntrain_v12_clean.PROJECT_ROOT = repo\ntrain_v12_clean.DATA_PATH = repo / 'data/processed/supercon_fractions_combined.csv'\ntrain_v12_clean.HOLDOUT_PATH = repo / 'data/GENERATIVE_HOLDOUT_DO_NOT_TRAIN.json'\ntrain_v12_clean.OUTPUT_DIR = repo / 'outputs'\n\n# Ensure output directory exists\ntrain_v12_clean.OUTPUT_DIR.mkdir(exist_ok=True)\n\n# --- Print final config ---\n\nprint(\"Colab config applied:\")\nprint(f\"  num_workers: {train_v12_clean.TRAIN_CONFIG['num_workers']}\")\nprint(f\"  persistent_workers: {train_v12_clean.TRAIN_CONFIG['persistent_workers']}\")\nprint(f\"  use_torch_compile: {train_v12_clean.TRAIN_CONFIG['use_torch_compile']}\")\nprint(f\"  checkpoint_interval: {train_v12_clean.TRAIN_CONFIG['checkpoint_interval']}\")\nprint(f\"  use_gradient_checkpointing: {train_v12_clean.TRAIN_CONFIG['use_gradient_checkpointing']}\")\nprint(f\"  num_epochs: {train_v12_clean.TRAIN_CONFIG['num_epochs']}\")\nprint(f\"  batch_size: {train_v12_clean.TRAIN_CONFIG['batch_size']}\")\nprint(f\"  resume_checkpoint: {train_v12_clean.TRAIN_CONFIG['resume_checkpoint']}\")\nprint(f\"  OUTPUT_DIR: {train_v12_clean.OUTPUT_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 5b: Setup Gist Logging (Optional)\n\nIf `GIST_ID` is set in Cell 2, this hooks into the training loop to push metrics to a GitHub Gist every N epochs. You can then monitor training remotely via `gh gist view GIST_ID` or by visiting the gist URL.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\nimport requests\nfrom datetime import datetime, timezone\n\n_gist_token = None\n_gist_log = {\"history\": [], \"status\": \"initialized\"}\n\ndef _get_github_token():\n    \"\"\"Get GitHub token from Colab secrets.\"\"\"\n    global _gist_token\n    if _gist_token is not None:\n        return _gist_token\n    try:\n        from google.colab import userdata\n        _gist_token = userdata.get('GITHUB_TOKEN')\n        return _gist_token\n    except Exception as e:\n        print(f\"  [Gist] Could not read GITHUB_TOKEN from Colab secrets: {e}\")\n        return None\n\ndef update_gist(epoch, metrics, best_exact, status=\"training\"):\n    \"\"\"Push current training metrics to the GitHub Gist.\"\"\"\n    if GIST_ID is None:\n        return\n\n    token = _get_github_token()\n    if token is None:\n        return\n\n    entry = {\n        \"epoch\": epoch,\n        \"loss\": round(metrics.get(\"loss\", 0), 4),\n        \"exact_match\": round(metrics.get(\"exact_match\", 0) * 100, 2),\n        \"token_accuracy\": round(metrics.get(\"accuracy\", 0) * 100, 2),\n        \"tc_loss\": round(metrics.get(\"tc_loss\", 0), 4),\n        \"magpie_loss\": round(metrics.get(\"magpie_loss\", 0), 4),\n        \"stoich_loss\": round(metrics.get(\"stoich_loss\", 0), 4),\n        \"reinforce_loss\": round(metrics.get(\"reinforce_loss\", 0), 4),\n        \"mean_reward\": round(metrics.get(\"mean_reward\", 0), 3),\n        \"entropy\": round(metrics.get(\"entropy\", 0), 3),\n        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n    }\n\n    _gist_log[\"status\"] = status\n    _gist_log[\"best_exact_match\"] = round(best_exact * 100, 2)\n    _gist_log[\"last_update\"] = entry[\"timestamp\"]\n    _gist_log[\"current_epoch\"] = epoch\n    _gist_log[\"total_epochs\"] = NUM_EPOCHS\n\n    # Keep last 200 entries to avoid gist getting huge\n    _gist_log[\"history\"].append(entry)\n    if len(_gist_log[\"history\"]) > 200:\n        _gist_log[\"history\"] = _gist_log[\"history\"][-200:]\n\n    try:\n        resp = requests.patch(\n            f\"https://api.github.com/gists/{GIST_ID}\",\n            headers={\n                \"Authorization\": f\"token {token}\",\n                \"Accept\": \"application/vnd.github.v3+json\",\n            },\n            json={\"files\": {\"training_log.json\": {\"content\": json.dumps(_gist_log, indent=2)}}},\n            timeout=10,\n        )\n        if resp.status_code != 200:\n            print(f\"  [Gist] Update failed (HTTP {resp.status_code})\")\n    except Exception as e:\n        # Don't let gist errors interrupt training\n        print(f\"  [Gist] Update error: {e}\")\n\n# --- Hook into the training loop ---\n# Monkey-patch save_checkpoint to also push metrics to gist.\n# This fires on best checkpoints and periodic checkpoints without\n# modifying the training script.\n\n_original_save_checkpoint = train_v12_clean.save_checkpoint\n_latest_metrics = {}  # Filled by our patched train_epoch\n_latest_best_exact = 0.0\n\ndef _save_checkpoint_with_gist(encoder, decoder, epoch, suffix='', **kwargs):\n    \"\"\"Wrapper that calls original save_checkpoint then updates gist.\"\"\"\n    global _latest_best_exact\n    _original_save_checkpoint(encoder, decoder, epoch, suffix=suffix, **kwargs)\n\n    best = kwargs.get('best_exact', _latest_best_exact)\n    _latest_best_exact = max(_latest_best_exact, best)\n\n    if GIST_ID is not None and _latest_metrics:\n        status = \"training\"\n        if suffix == 'final':\n            status = \"completed\"\n        elif suffix == 'interrupt':\n            status = \"interrupted\"\n        update_gist(epoch, _latest_metrics, _latest_best_exact, status=status)\n\n# Patch train_epoch to capture metrics for the gist hook\n_original_train_epoch = train_v12_clean.train_epoch\n\ndef _train_epoch_with_capture(*args, **kwargs):\n    \"\"\"Wrapper that captures epoch metrics for gist logging.\"\"\"\n    global _latest_metrics\n    metrics = _original_train_epoch(*args, **kwargs)\n    _latest_metrics = metrics\n    return metrics\n\nif GIST_ID is not None:\n    token = _get_github_token()\n    if token:\n        train_v12_clean.save_checkpoint = _save_checkpoint_with_gist\n        train_v12_clean.train_epoch = _train_epoch_with_capture\n        # Push initial status\n        update_gist(0, {}, 0, status=\"starting\")\n        print(f\"Gist logging enabled: https://gist.github.com/{GIST_ID}\")\n        print(f\"  Metrics pushed on every checkpoint save\")\n    else:\n        print(\"Gist logging disabled (no GITHUB_TOKEN found in Colab secrets)\")\nelse:\n    print(\"Gist logging disabled (GIST_ID is None)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Run Training\n",
    "\n",
    "This calls the existing `train()` function directly. Training output streams to the notebook.\n",
    "\n",
    "**Tip**: If Colab disconnects, re-run Cells 1-5 then this cell. With `RESUME_FROM_CHECKPOINT = True`, training picks up from the last saved best checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_v12_clean.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Post-Training Summary (Optional)\n",
    "\n",
    "List saved checkpoints and show basic info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path(REPO_PATH) / 'outputs'\n",
    "\n",
    "if output_dir.exists():\n",
    "    checkpoints = sorted(output_dir.glob('checkpoint_*.pt'))\n",
    "    if checkpoints:\n",
    "        print(f\"Saved checkpoints ({len(checkpoints)}):\")\n",
    "        for cp in checkpoints:\n",
    "            size_mb = cp.stat().st_size / (1024 * 1024)\n",
    "            print(f\"  {cp.name} ({size_mb:.1f} MB)\")\n",
    "\n",
    "        # Load best checkpoint to show final metrics\n",
    "        best_path = output_dir / 'checkpoint_best.pt'\n",
    "        if best_path.exists():\n",
    "            import torch\n",
    "            ckpt = torch.load(best_path, map_location='cpu', weights_only=False)\n",
    "            print(f\"\\nBest checkpoint:\")\n",
    "            print(f\"  Epoch: {ckpt.get('epoch', 'unknown')}\")\n",
    "            print(f\"  Best exact match: {ckpt.get('best_exact', 'unknown')}\")\n",
    "            if 'prev_exact' in ckpt:\n",
    "                print(f\"  Exact at save: {ckpt['prev_exact']:.4f}\")\n",
    "    else:\n",
    "        print(\"No checkpoints found.\")\n",
    "else:\n",
    "    print(f\"Output directory not found: {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}