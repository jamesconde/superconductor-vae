{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Multi-Task Superconductor Generator - Google Colab Training (V12.43 + Phase 2)\n\nTrain the multi-task encoder-decoder on Google Colab using your repo uploaded to Google Drive.\n\n**Setup**: Upload the `superconductor-vae` repo to Google Drive, then run these cells.\n\n**Current model: V12.43** (pre-V13/V14/V15). This is the V12.41 best checkpoint (epoch 3292, 85.4% exact) expanded 12.5% wider via Net2Net. Key specs:\n- **Vocab**: 148 tokens (original digit-by-digit fraction encoding)\n- **Architecture**: d_model=576, dim_feedforward=2304, fusion_dim=288, latent_dim=2048\n- **Heads**: Token type classifier, enriched decoder memory (heads_to_memory), stoich conditioning, stop head\n- **NOT applied**: V13 semantic fractions (vocab 4355), V14 isotopes (vocab 4647), V15 memory bottleneck\n\nThe training script contains code for V13-V15 features but they are **disabled** (`use_semantic_fractions: False`, `memory_bottleneck_dim: 0`). No auto-migration will occur.\n\n**Phase 2 Self-Supervised**: Interleaved self-supervised sub-epochs that use the model's own generations to improve generalization. Runs every 2 supervised epochs once exact match >= 80%. This is a **model enhancement** algorithm — novel superconductor discoveries are flagged opportunistically but are not the goal. See `docs/PHASE2_SELF_SUPERVISED_DESIGN.md`.\n\n**Checkpoints**: Saved to `outputs/` inside the repo on Drive, so they persist across Colab sessions. `checkpoint_best.pt` is auto-detected on resume.\n\n**Remote monitoring (optional)**: Set `GIST_ID` in Cell 2 to push live training metrics to a GitHub Gist. Includes all physics losses and Phase 2 metrics (z-MSE, valid rate, discoveries)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 1b: Sync Repo with GitHub\n\nPull the latest code from GitHub so Colab matches your most recent push. This avoids `ImportError` or missing-feature bugs when the Drive copy is stale.\n\n**Note**: If you have local uncommitted changes on Drive (unlikely), this will attempt a merge. If you see merge conflicts, the safest fix is to delete the repo folder from Drive and re-clone.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Sync repo with GitHub (pulls latest code changes)\n# REPO_PATH is defined in the Configuration cell below — hardcode it here\n# so this cell can run independently before Configuration.\n_REPO = \"/content/drive/My Drive/Colab Notebooks/SuperconductorVAE/superconductor-vae\"\n\nimport subprocess, os\n\nif os.path.isdir(_REPO):\n    print(f\"Pulling latest from GitHub...\")\n    result = subprocess.run(\n        [\"git\", \"pull\", \"--ff-only\"],\n        cwd=_REPO,\n        capture_output=True, text=True, timeout=60,\n    )\n    print(result.stdout.strip())\n    if result.returncode != 0:\n        print(f\"git pull failed (exit {result.returncode}):\")\n        print(result.stderr.strip())\n        print(\"\\nIf you see merge conflicts, restart with a fresh clone:\")\n        print(f\"  !rm -rf '{_REPO}'\")\n        print(f\"  !git clone https://github.com/jamesconde/superconductor-vae.git '{_REPO}'\")\n    else:\n        # Show current commit for reference\n        commit = subprocess.run(\n            [\"git\", \"log\", \"--oneline\", \"-1\"],\n            cwd=_REPO, capture_output=True, text=True,\n        )\n        print(f\"Current commit: {commit.stdout.strip()}\")\nelse:\n    print(f\"Repo not found at: {_REPO}\")\n    print(\"Check that Google Drive is mounted and the path is correct.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 2: Configuration\n\nEdit these values to match your setup."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Path to the superconductor-vae repo on your Google Drive\nREPO_PATH = \"/content/drive/My Drive/Colab Notebooks/SuperconductorVAE/superconductor-vae\"\n\n# Training options (override defaults)\n# 'auto' prefers checkpoint_best.pt, then highest checkpoint_epoch_*.pt.\n# Set to a specific path (e.g. 'outputs/checkpoint_epoch_3999.pt') to override,\n# or None to train from scratch.\nRESUME_CHECKPOINT = 'auto'\nNUM_EPOCHS = 5000               # Total epochs (training resumes from last epoch)\nBATCH_SIZE = 'auto'             # 'auto' scales with GPU memory, or set integer (e.g. 32, 48)\n\n# --- Phase 2: Self-Supervised Training ---\n# Interleaved self-supervised sub-epochs to improve generalization.\n# Purpose: MODEL ENHANCEMENT (not discovery). Runs every N supervised epochs.\n# Novel superconductors found during generation are flagged and saved to\n# outputs/phase2_discoveries.jsonl but are a side effect, not the goal.\nPHASE2_ENABLED = True           # Master toggle\nPHASE2_START = 'auto'           # 'auto' = activate when exact >= 80%, or set epoch number\nPHASE2_N_SAMPLES = 'auto'       # 'auto' = scale with VRAM: clamp(3.2*GB, 32, 512)\nPHASE2_INTERVAL = 2             # Run Phase 2 every N supervised epochs (2 = every other epoch)\n\n# --- Remote monitoring via GitHub Gist ---\n# Set GIST_ID to enable live training metrics visible outside Colab.\n# 1. Create a personal access token at https://github.com/settings/tokens with \"gist\" scope\n# 2. In Colab: click the key icon (left sidebar) > add secret named GITHUB_TOKEN\n# 3. Create a gist at https://gist.github.com with one file named \"training_log.json\"\n#    containing just \"{}\" — then copy the gist ID from the URL.\n# Set to None to disable gist logging.\nGIST_ID = \"acceed7daef4d6893801cc7337531b68\"\nGIST_LOG_EVERY = 5  # Update gist every N epochs"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 3: Install Dependencies\n\nPyTorch, NumPy, pandas, and scikit-learn are pre-installed on Colab. scipy is usually pre-installed but required explicitly for V12.20 Magpie quantile transforms."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install matminer pymatgen scipy"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Setup Paths and Verify Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nfrom pathlib import Path\n\nrepo = Path(REPO_PATH)\n\n# Add src/ to Python path so imports work\nsrc_path = str(repo / \"src\")\nif src_path not in sys.path:\n    sys.path.insert(0, src_path)\n\n# Verify key files exist\nrequired_files = {\n    \"Training data (contrastive)\": repo / \"data/processed/supercon_fractions_contrastive.csv\",\n    \"Holdout set\": repo / \"data/GENERATIVE_HOLDOUT_DO_NOT_TRAIN.json\",\n    \"Training script\": repo / \"scripts/train_v12_clean.py\",\n    \"VAE model\": repo / \"src/superconductor/models/attention_vae.py\",\n    \"Decoder model\": repo / \"src/superconductor/models/autoregressive_decoder.py\",\n    \"Fraction vocab\": repo / \"data/fraction_vocab.json\",\n    \"Fraction vocab (old)\": repo / \"data/fraction_vocab_old.json\",\n}\n\n# Optional files (nice to have, not required)\noptional_files = {\n    \"Best checkpoint\": repo / \"outputs/checkpoint_best.pt\",\n    \"Isotope vocab\": repo / \"data/isotope_vocab.json\",\n}\n\n# Add specific resume checkpoint to optional checks (if not 'auto')\nif RESUME_CHECKPOINT != 'auto' and RESUME_CHECKPOINT is not None:\n    optional_files[\"Resume checkpoint\"] = repo / RESUME_CHECKPOINT\n\nall_found = True\nfor name, path in required_files.items():\n    exists = path.exists()\n    status = \"OK\" if exists else \"MISSING\"\n    print(f\"  [{status}] {name}: {path.name}\")\n    if not exists:\n        all_found = False\n\nprint()\nfor name, path in optional_files.items():\n    exists = path.exists()\n    status = \"OK\" if exists else \"---\"\n    print(f\"  [{status}] {name}: {path.name}\")\n\nif not all_found:\n    raise FileNotFoundError(\n        f\"Missing required files. Check that REPO_PATH is correct: {REPO_PATH}\"\n    )\n\n# Show resume mode\nif RESUME_CHECKPOINT == 'auto':\n    best_path = repo / \"outputs/checkpoint_best.pt\"\n    if best_path.exists():\n        print(f\"\\n  Resume: 'auto' — will load checkpoint_best.pt\")\n    else:\n        # Check for epoch checkpoints\n        epoch_files = sorted((repo / \"outputs\").glob(\"checkpoint_epoch_*.pt\"))\n        if epoch_files:\n            print(f\"\\n  Resume: 'auto' — no checkpoint_best.pt, will use {epoch_files[-1].name}\")\n        else:\n            print(f\"\\n  Resume: 'auto' — no checkpoints found, will train from scratch\")\nelif RESUME_CHECKPOINT is not None:\n    resume_path = repo / RESUME_CHECKPOINT\n    if not resume_path.exists():\n        print(f\"\\n  WARNING: Resume checkpoint not found: {resume_path.name}\")\n        print(f\"  Consider setting RESUME_CHECKPOINT = 'auto' to auto-detect.\")\nelse:\n    print(f\"\\n  Resume: None — training from scratch\")\n\nprint()\n\n# GPU info\nimport torch\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n    capability = torch.cuda.get_device_capability(0)\n    print(f\"GPU: {gpu_name}\")\n    print(f\"Memory: {gpu_mem:.1f} GB\")\n    print(f\"Compute capability: {capability[0]}.{capability[1]}\")\n    print(f\"PyTorch: {torch.__version__}\")\n    print(f\"CUDA: {torch.version.cuda}\")\nelse:\n    print(\"WARNING: No GPU detected! Go to Runtime > Change runtime type > GPU.\")\n    print(f\"PyTorch: {torch.__version__}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 5: Apply Colab-Specific Config Overrides\n\nPatches the training config for Colab before importing the training function.\n`detect_environment()` auto-detects Colab and sets optimal DataLoader/torch.compile settings.\n\n**V12.43 model**: `resume_checkpoint='auto'` auto-detects the best checkpoint (prefers `checkpoint_best.pt`, then highest `checkpoint_epoch_*.pt`). The current checkpoint is V12.43 (vocab=148, d_model=576). V13/V14/V15 features are disabled — no vocab expansion or auto-migration will occur.\n\n**Phase 2**: Self-supervised sub-epochs enabled via `PHASE2_ENABLED` in Cell 2. n_samples auto-scales with GPU VRAM (A100 40GB -> 128, A100 80GB -> 256). Discovery tracker logs novel formulas to `outputs/phase2_discoveries.jsonl`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import importlib\n\n# Add scripts/ to path so we can import train_v12_clean as a module\nscripts_path = str(repo / \"scripts\")\nif scripts_path not in sys.path:\n    sys.path.insert(0, scripts_path)\n\n# Import (or reload) the training module.\n# IMPORTANT: reload() ensures TRAIN_CONFIG is reset to defaults when cells are\n# re-run without restarting the runtime. Without this, batch_size_multiplier\n# from detect_environment() gets applied multiple times (42 -> 2100 -> 105000).\nimport train_v12_clean\nimportlib.reload(train_v12_clean)\n\n# --- Colab-specific overrides ---\n# NOTE: DataLoader settings (num_workers, pin_memory, persistent_workers)\n# and torch.compile settings (use_torch_compile, compile_mode) are\n# auto-detected by detect_environment() inside train(). No manual override needed.\n\n# More frequent checkpoints — Colab can disconnect even on Pro tier\ntrain_v12_clean.TRAIN_CONFIG['checkpoint_interval'] = 25\n\n# Gradient checkpointing incompatible with torch.compile; disabled for V12.20+\ntrain_v12_clean.TRAIN_CONFIG['use_gradient_checkpointing'] = False\n\n# Disable z_cache writes to save Colab disk space (~400MB/epoch)\ntrain_v12_clean.TRAIN_CONFIG['z_cache_every_epoch'] = False\n\n# --- RL gating: gate on AR exact >= 40% (script default) ---\n# Adaptive TF (V15.1) closes the exposure bias gap first. RL kicks in\n# when 40% of RLOO samples get positive reward — enough signal for useful\n# gradients. Saves 5x epoch time while TF does the heavy lifting.\n# (No override needed — uses script default rl_min_ar_exact=0.40)\n\n# --- PhysZ: always on — physical regularizers should always be active ---\n# PhysZ and SC losses are physically grounded regularizers, not auxiliary\n# tricks. They should always contribute to z-space organization.\n# The warmup ramp still applies on first activation to avoid sudden shock.\ntrain_v12_clean.TRAIN_CONFIG['use_physics_z'] = True\n\n# --- Phase 2: Self-Supervised Training ---\ntrain_v12_clean.TRAIN_CONFIG['phase2_enabled'] = PHASE2_ENABLED\ntrain_v12_clean.TRAIN_CONFIG['phase2_start'] = PHASE2_START\ntrain_v12_clean.TRAIN_CONFIG['phase2_n_samples'] = PHASE2_N_SAMPLES\ntrain_v12_clean.TRAIN_CONFIG['phase2_interval'] = PHASE2_INTERVAL\n# Phase 2 activation: TF exact >= 90% AND AR exact >= 60%.\n# Phase 2 generates formulas and re-encodes them (round-trip consistency).\n# At low AR, most generated formulas are garbage — the training signal is\n# thin and noisy. At 60% AR, enough generated formulas are valid for\n# meaningful self-supervised learning. RL (which activates at 40% AR)\n# already provides sequence-level training at lower AR thresholds.\ntrain_v12_clean.TRAIN_CONFIG['phase2_auto_min_exact'] = 0.90\ntrain_v12_clean.TRAIN_CONFIG['phase2_min_ar_exact'] = 0.60\ntrain_v12_clean.TRAIN_CONFIG['phase2_min_resume_epochs'] = 50\n\n# --- User overrides from Cell 2 ---\n\ntrain_v12_clean.TRAIN_CONFIG['num_epochs'] = NUM_EPOCHS\n\n# Batch size: 'auto' lets detect_environment() apply the correct multiplier\n# to the default (42). Setting a specific int skips the multiplier.\nif BATCH_SIZE == 'auto':\n    # Keep default (42) — detect_environment() in train() will apply\n    # the correct multiplier for the GPU class (e.g. x25 for A100 80GB = 1050)\n    pass\nelse:\n    train_v12_clean.TRAIN_CONFIG['batch_size'] = int(BATCH_SIZE)\n\n# Checkpoint resume — 'auto' prefers checkpoint_best.pt, then highest\n# checkpoint_epoch_*.pt. Set RESUME_CHECKPOINT to a specific path to override.\ntrain_v12_clean.TRAIN_CONFIG['resume_checkpoint'] = RESUME_CHECKPOINT\n\n# --- Redirect paths to Drive-based repo ---\n\ntrain_v12_clean.PROJECT_ROOT = repo\ntrain_v12_clean.CONTRASTIVE_DATA_PATH = repo / 'data/processed/supercon_fractions_contrastive.csv'\ntrain_v12_clean.DATA_PATH = repo / 'data/processed/supercon_fractions_contrastive.csv'\ntrain_v12_clean.HOLDOUT_PATH = repo / 'data/GENERATIVE_HOLDOUT_DO_NOT_TRAIN.json'\ntrain_v12_clean.OUTPUT_DIR = repo / 'outputs'\n\n# Ensure output directory exists\ntrain_v12_clean.OUTPUT_DIR.mkdir(exist_ok=True)\n\n# --- Print final config ---\n\nprint(\"Colab config applied (V12.43 + Phase 2):\")\nprint(f\"  resume_checkpoint: {train_v12_clean.TRAIN_CONFIG['resume_checkpoint']}\")\nprint(f\"  checkpoint_interval: {train_v12_clean.TRAIN_CONFIG['checkpoint_interval']}\")\nprint(f\"  num_epochs: {train_v12_clean.TRAIN_CONFIG['num_epochs']}\")\nprint(f\"  batch_size: {train_v12_clean.TRAIN_CONFIG['batch_size']} (detect_environment will apply GPU-specific multiplier)\")\nprint(f\"  rl_min_ar_exact: {train_v12_clean.TRAIN_CONFIG.get('rl_min_ar_exact', 0)} (suppress RL below this AR exact)\")\nprint(f\"  rl_auto_scale: {train_v12_clean.TRAIN_CONFIG.get('rl_auto_scale', False)}\")\nprint(f\"  rl_auto_scale_target: {train_v12_clean.TRAIN_CONFIG.get('rl_auto_scale_target', 10.0)}\")\nprint(f\"  use_physics_z: {train_v12_clean.TRAIN_CONFIG.get('use_physics_z', False)} (PhysZ regularization)\")\nprint(f\"  tf_onset: {train_v12_clean.TRAIN_CONFIG.get('tf_onset', 0.80)} | tf_floor: {train_v12_clean.TRAIN_CONFIG.get('tf_floor', 0.20)}\")\nprint(f\"  phase2_enabled: {train_v12_clean.TRAIN_CONFIG['phase2_enabled']}\")\nprint(f\"  phase2_auto_min_exact: {train_v12_clean.TRAIN_CONFIG['phase2_auto_min_exact']}\")\nprint(f\"  phase2_min_ar_exact: {train_v12_clean.TRAIN_CONFIG.get('phase2_min_ar_exact', 0)} (suppress Phase 2 below this AR exact)\")\nprint(f\"  phase2_min_resume_epochs: {train_v12_clean.TRAIN_CONFIG['phase2_min_resume_epochs']}\")\nprint(f\"  OUTPUT_DIR: {train_v12_clean.OUTPUT_DIR}\")\nprint(\"  (torch.compile + DataLoader settings auto-detected by detect_environment())\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 5b: Setup Gist Logging (Optional)\n\nIf `GIST_ID` is set in Cell 2, this hooks into the training loop to push metrics to a GitHub Gist every N epochs. You can then monitor training remotely via `gh gist view GIST_ID` or by visiting the gist URL.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\nimport io\nimport sys\nimport requests\nfrom datetime import datetime, timezone\n\n_gist_token = None\n_gist_log = {\"history\": [], \"console_log\": [], \"status\": \"initialized\"}\n\ndef _get_github_token():\n    \"\"\"Get GitHub token from Colab secrets.\"\"\"\n    global _gist_token\n    if _gist_token is not None:\n        return _gist_token\n    try:\n        from google.colab import userdata\n        _gist_token = userdata.get('GITHUB_TOKEN')\n        return _gist_token\n    except Exception as e:\n        print(f\"  [Gist] Could not read GITHUB_TOKEN from Colab secrets: {e}\")\n        return None\n\ndef update_gist(epoch, metrics, best_exact, status=\"training\", console_lines=None):\n    \"\"\"Push current training metrics + console log to the GitHub Gist.\"\"\"\n    if GIST_ID is None:\n        return\n\n    token = _get_github_token()\n    if token is None:\n        return\n\n    entry = {\n        \"epoch\": epoch,\n        # Core losses\n        \"loss\": round(metrics.get(\"loss\", 0), 4),\n        \"exact_match\": round(metrics.get(\"exact_match\", 0) * 100, 2),\n        \"token_accuracy\": round(metrics.get(\"accuracy\", 0) * 100, 2),\n        \"tc_loss\": round(metrics.get(\"tc_loss\", 0), 4),\n        \"magpie_loss\": round(metrics.get(\"magpie_loss\", 0), 4),\n        \"stoich_loss\": round(metrics.get(\"stoich_loss\", 0), 4),\n        \"reinforce_loss\": round(metrics.get(\"reinforce_loss\", 0), 4),\n        \"mean_reward\": round(metrics.get(\"mean_reward\", 0), 3),\n        \"entropy\": round(metrics.get(\"entropy\", 0), 3),\n        \"z_norm\": round(metrics.get(\"z_norm\", 0), 1),\n        # Physics & classification losses (V15.0: added back)\n        \"hp_loss\": round(metrics.get(\"hp_loss\", 0), 4),\n        \"sc_loss\": round(metrics.get(\"sc_loss\", 0), 4),\n        \"theory_loss\": round(metrics.get(\"theory_loss\", 0), 4),\n        \"physics_z_loss\": round(metrics.get(\"physics_z_loss\", 0), 4),\n        \"family_loss\": round(metrics.get(\"family_loss\", 0), 4),\n        \"tc_class_loss\": round(metrics.get(\"tc_class_loss\", 0), 4),\n        # Decoder auxiliary losses\n        \"stop_loss\": round(metrics.get(\"stop_loss\", 0), 4),\n        \"type_loss\": round(metrics.get(\"type_loss\", 0), 4),\n        \"type_accuracy\": round(metrics.get(\"type_accuracy\", 0) * 100, 2),\n        # Constraint zoo\n        \"constraint_zoo_loss\": round(metrics.get(\"constraint_zoo_loss\", 0), 4),\n        # SC / non-SC breakdown\n        \"sc_exact_match\": round(metrics.get(\"sc_exact_match\", 0) * 100, 2),\n        \"non_sc_exact_match\": round(metrics.get(\"non_sc_exact_match\", 0) * 100, 2),\n        # Phase 2: Self-supervised metrics (only present on Phase 2 sub-epochs)\n        \"phase2_total_loss\": round(metrics.get(\"phase2_total_loss\", 0), 4),\n        \"phase2_z_mse\": round(metrics.get(\"phase2_z_mse\", 0), 4),\n        \"phase2_tc_mse\": round(metrics.get(\"phase2_tc_mse\", 0), 4),\n        \"phase2_valid_rate\": round(metrics.get(\"phase2_valid_rate\", 0), 3),\n        \"phase2_unique_rate\": round(metrics.get(\"phase2_unique_rate\", 0), 3),\n        \"phase2_weight\": round(metrics.get(\"phase2_weight\", 0), 4),\n        \"phase2_n_valid\": int(metrics.get(\"phase2_n_valid\", 0)),\n        \"phase2_n_degenerate\": int(metrics.get(\"phase2_n_degenerate\", 0)),\n        \"phase2_n_novel\": int(metrics.get(\"phase2_n_novel\", 0)),\n        \"phase2_n_holdout_recovered\": int(metrics.get(\"phase2_n_holdout_recovered\", 0)),\n        \"phase2_collapse_active\": bool(metrics.get(\"phase2_collapse_active\", False)),\n        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n    }\n\n    _gist_log[\"status\"] = status\n    _gist_log[\"best_exact_match\"] = round(best_exact * 100, 2)\n    _gist_log[\"last_update\"] = entry[\"timestamp\"]\n    _gist_log[\"current_epoch\"] = epoch\n    _gist_log[\"total_epochs\"] = NUM_EPOCHS\n\n    # Keep last 200 metric entries\n    _gist_log[\"history\"].append(entry)\n    if len(_gist_log[\"history\"]) > 200:\n        _gist_log[\"history\"] = _gist_log[\"history\"][-200:]\n\n    # Append console lines (the full printed output from training)\n    if console_lines:\n        _gist_log[\"console_log\"].extend(console_lines)\n    # Keep last 500 console lines to avoid gist getting huge\n    if len(_gist_log[\"console_log\"]) > 500:\n        _gist_log[\"console_log\"] = _gist_log[\"console_log\"][-500:]\n\n    try:\n        resp = requests.patch(\n            f\"https://api.github.com/gists/{GIST_ID}\",\n            headers={\n                \"Authorization\": f\"token {token}\",\n                \"Accept\": \"application/vnd.github.v3+json\",\n            },\n            json={\"files\": {\n                \"training_log.json\": {\"content\": json.dumps(_gist_log, indent=2)},\n                \"console_output.txt\": {\"content\": \"\\n\".join(_gist_log[\"console_log\"][-500:])},\n            }},\n            timeout=10,\n        )\n        if resp.status_code != 200:\n            print(f\"  [Gist] Update failed (HTTP {resp.status_code})\")\n    except Exception as e:\n        # Don't let gist errors interrupt training\n        print(f\"  [Gist] Update error: {e}\")\n\n# --- Stdout capture for full console logging ---\n# Tee stdout so all print() output is captured AND still displayed in Colab.\n\nclass _GistTeeStream:\n    \"\"\"Captures stdout lines for gist logging while still printing to Colab.\"\"\"\n    def __init__(self, original_stdout):\n        self._original = original_stdout\n        self._buffer = []\n        self._line_buf = \"\"\n\n    def write(self, text):\n        self._original.write(text)\n        # Buffer lines\n        self._line_buf += text\n        while \"\\n\" in self._line_buf:\n            line, self._line_buf = self._line_buf.split(\"\\n\", 1)\n            if line.strip():  # Skip blank lines\n                self._buffer.append(line.strip())\n\n    def flush(self):\n        self._original.flush()\n        # Flush partial line if any\n        if self._line_buf.strip():\n            self._buffer.append(self._line_buf.strip())\n            self._line_buf = \"\"\n\n    def drain(self):\n        \"\"\"Return and clear captured lines.\"\"\"\n        lines = self._buffer\n        self._buffer = []\n        return lines\n\n    # Forward all other attributes to original stream\n    def __getattr__(self, name):\n        return getattr(self._original, name)\n\n_gist_tee = None\n\n# --- Hook into the training loop ---\n# Monkey-patch save_checkpoint and train_epoch to push metrics + console to gist.\n# Guard against double-patching (re-running this cell) which causes recursion.\n\n_latest_metrics = {}\n_latest_best_exact = 0.0\n\n_GIST_PATCHED = getattr(train_v12_clean, '_GIST_PATCHED', False)\n\nif not _GIST_PATCHED:\n    _original_save_checkpoint = train_v12_clean.save_checkpoint\n    _original_train_epoch = train_v12_clean.train_epoch\n\n    def _get_real_epoch():\n        \"\"\"Get the actual epoch number from the training loop's shutdown state.\"\"\"\n        return train_v12_clean._shutdown_state.get('epoch', 0)\n\n    def _save_checkpoint_with_gist(encoder, decoder, epoch, suffix='', **kwargs):\n        \"\"\"Wrapper that calls original save_checkpoint then updates gist.\"\"\"\n        global _latest_best_exact, _gist_tee\n        _original_save_checkpoint(encoder, decoder, epoch, suffix=suffix, **kwargs)\n\n        best = kwargs.get('best_exact', _latest_best_exact)\n        _latest_best_exact = max(_latest_best_exact, best)\n\n        if GIST_ID is not None and _latest_metrics:\n            status = \"training\"\n            if suffix == 'final':\n                status = \"completed\"\n            elif suffix == 'interrupt':\n                status = \"interrupted\"\n            # Drain captured console lines\n            console_lines = _gist_tee.drain() if _gist_tee else None\n            update_gist(epoch, _latest_metrics, _latest_best_exact,\n                       status=status, console_lines=console_lines)\n\n    def _train_epoch_with_capture(*args, **kwargs):\n        \"\"\"Wrapper that captures epoch metrics and pushes to gist periodically.\"\"\"\n        global _latest_metrics, _gist_tee\n        metrics = _original_train_epoch(*args, **kwargs)\n        _latest_metrics = metrics\n\n        # Get the REAL epoch number from the training loop's state\n        real_epoch = _get_real_epoch()\n\n        # Push gist every GIST_LOG_EVERY epochs (not just checkpoint saves)\n        if GIST_ID is not None and real_epoch % GIST_LOG_EVERY == 0:\n            console_lines = _gist_tee.drain() if _gist_tee else None\n            update_gist(real_epoch, metrics, _latest_best_exact,\n                       console_lines=console_lines)\n        return metrics\n\n    if GIST_ID is not None:\n        token = _get_github_token()\n        if token:\n            train_v12_clean.save_checkpoint = _save_checkpoint_with_gist\n            train_v12_clean.train_epoch = _train_epoch_with_capture\n            train_v12_clean._GIST_PATCHED = True  # Prevent double-patching\n\n            # Install stdout tee to capture all console output\n            _gist_tee = _GistTeeStream(sys.stdout)\n            sys.stdout = _gist_tee\n\n            # Push initial status\n            update_gist(0, {}, 0, status=\"starting\",\n                       console_lines=[\"=== Training session started ===\"])\n            print(f\"Gist logging enabled: https://gist.github.com/{GIST_ID}\")\n            print(f\"  Metrics + full console log pushed every {GIST_LOG_EVERY} epochs and on checkpoint saves\")\n            print(f\"  Console output also saved to 'console_output.txt' in gist\")\n        else:\n            print(\"Gist logging disabled (no GITHUB_TOKEN found in Colab secrets)\")\n    else:\n        print(\"Gist logging disabled (GIST_ID is None)\")\nelse:\n    print(\"Gist logging already patched (safe to re-run this cell)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 6: Run Training\n\nThis calls the existing `train()` function directly. Training output streams to the notebook.\n\n**V12.43 flow**: On first run, you'll see:\n1. Auto-detect checkpoint: `[AUTO] Found checkpoint_best.pt (epoch XXXX, best_exact=0.YYYY)`\n2. Pre-training baseline eval: exact match % of loaded model before any training\n3. RL probe mode: tiny RL weight for calibration, then auto-scaled\n4. Phase 2 self-supervised sub-epochs (if exact >= 80% and `PHASE2_ENABLED`)\n\n**Note**: V13/V14/V15 features are disabled. No auto-migration or vocab expansion will occur.\n\n**Tip**: If Colab disconnects, re-run Cells 1-5 then this cell. Training auto-resumes from `checkpoint_best.pt`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_v12_clean.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Post-Training Summary (Optional)\n",
    "\n",
    "List saved checkpoints and show basic info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path(REPO_PATH) / 'outputs'\n",
    "\n",
    "if output_dir.exists():\n",
    "    checkpoints = sorted(output_dir.glob('checkpoint_*.pt'))\n",
    "    if checkpoints:\n",
    "        print(f\"Saved checkpoints ({len(checkpoints)}):\")\n",
    "        for cp in checkpoints:\n",
    "            size_mb = cp.stat().st_size / (1024 * 1024)\n",
    "            print(f\"  {cp.name} ({size_mb:.1f} MB)\")\n",
    "\n",
    "        # Load best checkpoint to show final metrics\n",
    "        best_path = output_dir / 'checkpoint_best.pt'\n",
    "        if best_path.exists():\n",
    "            import torch\n",
    "            ckpt = torch.load(best_path, map_location='cpu', weights_only=False)\n",
    "            print(f\"\\nBest checkpoint:\")\n",
    "            print(f\"  Epoch: {ckpt.get('epoch', 'unknown')}\")\n",
    "            print(f\"  Best exact match: {ckpt.get('best_exact', 'unknown')}\")\n",
    "            if 'prev_exact' in ckpt:\n",
    "                print(f\"  Exact at save: {ckpt['prev_exact']:.4f}\")\n",
    "    else:\n",
    "        print(\"No checkpoints found.\")\n",
    "else:\n",
    "    print(f\"Output directory not found: {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}