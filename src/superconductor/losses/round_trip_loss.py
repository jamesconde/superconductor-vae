"""
SC Constraint Zoo: Round-Trip Cycle Consistency Loss (A5) — V12.43

Differentiable loss that enforces Z-space consistency:
  Z_original -> decode -> formula_string -> parse -> re-encode -> Z_reconstructed

The token generation step is non-differentiable (under no_grad), but the
re-encoding step IS differentiable. Gradients flow through:
  - The encoder (learns to re-encode parsed formulas consistently)
  - The magpie_head (learns self-consistent Magpie feature predictions)

The decoder's token generation does NOT receive A5 gradients directly.

Magpie Blocker Solution:
  The encoder requires 145 Magpie features. Since we can't compute Magpie
  features from a generated formula at training time, we use the decoder's
  own magpie_head predictions as proxy input for re-encoding. This creates
  a self-consistency loop: the model must predict Magpie features that are
  consistent with what the encoder expects for a given formula.

Reference: docs/SC_CONSTRAINT_ZOO.md (constraint A5)
"""

import re
from typing import Dict, Optional

import torch
import torch.nn as nn
import torch.nn.functional as F

# Lazy imports to avoid circular dependencies — resolved at runtime
_SYMBOL_TO_Z = None
_indices_to_formula = None
_parse_fraction_formula = None


def _ensure_imports():
    """Lazy-load dependencies to avoid circular imports."""
    global _SYMBOL_TO_Z, _indices_to_formula, _parse_fraction_formula
    if _SYMBOL_TO_Z is None:
        from superconductor.encoders.element_properties import SYMBOL_TO_Z
        _SYMBOL_TO_Z = SYMBOL_TO_Z
    if _indices_to_formula is None:
        from superconductor.models.autoregressive_decoder import indices_to_formula
        _indices_to_formula = indices_to_formula
    if _parse_fraction_formula is None:
        # parse_fraction_formula is defined in train_v12_clean.py
        # Replicate it here to avoid importing the training script
        def parse_fraction_formula(formula: str) -> Optional[Dict[str, float]]:
            """Parse formula like 'Ag(1/500)Al(499/500)' to element fractions."""
            pattern = r'([A-Z][a-z]?)(?:\((\d+)/(\d+)\)|(\d*\.?\d+))?'
            matches = re.findall(pattern, formula)
            result = {}
            for match in matches:
                element = match[0]
                if not element:
                    continue
                if match[1] and match[2]:
                    result[element] = float(match[1]) / float(match[2])
                elif match[3]:
                    result[element] = float(match[3])
                else:
                    result[element] = 1.0
            return result if result else None
        _parse_fraction_formula = parse_fraction_formula


def _formula_to_encoder_input(
    formula: str,
    max_elements: int = 12,
    device: torch.device = torch.device('cpu'),
) -> Optional[Dict[str, torch.Tensor]]:
    """Convert a formula string to encoder input tensors.

    Returns None if parsing fails. Otherwise returns dict with:
        element_indices: [max_elements] atomic numbers
        element_fractions: [max_elements] molar fractions
        element_mask: [max_elements] valid element mask
    """
    _ensure_imports()

    parsed = _parse_fraction_formula(formula)
    if parsed is None or len(parsed) == 0:
        return None

    indices = torch.zeros(max_elements, dtype=torch.long, device=device)
    fractions = torch.zeros(max_elements, dtype=torch.float32, device=device)
    mask = torch.zeros(max_elements, dtype=torch.float32, device=device)

    pos = 0
    for symbol, frac in parsed.items():
        if pos >= max_elements:
            break
        z = _SYMBOL_TO_Z.get(symbol, 0)
        if z == 0:
            continue  # Skip unknown elements without advancing pos
        indices[pos] = z
        fractions[pos] = frac
        mask[pos] = 1.0
        pos += 1

    if mask.sum() == 0:
        return None

    return {
        'element_indices': indices,
        'element_fractions': fractions,
        'element_mask': mask,
    }


class RoundTripConsistencyLoss(nn.Module):
    """A5: Round-trip cycle consistency loss.

    Measures how well the encoder can reconstruct the original Z vector
    from a formula generated by the decoder. Enforces that the latent
    space is semantically meaningful — a good Z should decode to a formula
    that re-encodes back to (approximately) the same Z.

    Operates on a random subset (default 10%) of each batch to control
    memory overhead (~125MB for 25 samples through the 6M encoder).
    """

    def __init__(
        self,
        z_weight: float = 1.0,
        tc_weight: float = 5.0,
        subset_fraction: float = 0.1,
        max_elements: int = 12,
    ):
        super().__init__()
        self.z_weight = z_weight
        self.tc_weight = tc_weight
        self.subset_fraction = subset_fraction
        self.max_elements = max_elements
        self._encoder = None
        self._decoder = None

    def set_models(self, encoder, decoder):
        """Set encoder and decoder references (called during training setup).

        Args:
            encoder: FullMaterialsVAE instance (has .encode() method)
            decoder: EnhancedTransformerDecoder instance (has .generate_with_kv_cache())
        """
        self._encoder = encoder
        self._decoder = decoder

    def forward(
        self,
        z: torch.Tensor,            # [batch, 2048] original latent
        magpie_pred: torch.Tensor,   # [batch, 145] from decoder's magpie_head
        tc_pred: torch.Tensor,       # [batch] from decoder's tc_head
        stoich_pred: torch.Tensor,   # [batch, 37] for decoder conditioning
        encoder_skip: torch.Tensor,  # [batch, 256] skip connections
        device: torch.device,
    ) -> Dict[str, torch.Tensor]:
        """Compute round-trip consistency loss on a random subset.

        Returns dict with:
            'round_trip_loss': scalar total loss
            'z_mse': scalar Z reconstruction error
            'tc_mse': scalar Tc reconstruction error
            'n_valid': number of successfully parsed samples
        """
        _ensure_imports()

        if self._encoder is None or self._decoder is None:
            return {
                'round_trip_loss': torch.tensor(0.0, device=device),
                'z_mse': torch.tensor(0.0, device=device),
                'tc_mse': torch.tensor(0.0, device=device),
                'n_valid': 0,
            }

        batch_size = z.shape[0]
        subset_size = max(1, int(batch_size * self.subset_fraction))

        # Random subset selection
        subset_idx = torch.randperm(batch_size, device=device)[:subset_size]

        z_subset = z[subset_idx]                        # [subset, 2048]
        magpie_subset = magpie_pred[subset_idx]         # [subset, 145]
        tc_subset = tc_pred[subset_idx]                 # [subset]
        stoich_subset = stoich_pred[subset_idx] if stoich_pred is not None else None
        skip_subset = encoder_skip[subset_idx] if encoder_skip is not None else None

        # Step 1: Greedy decode to get token sequences (no gradients)
        with torch.no_grad():
            generated_tokens, _, _ = self._decoder.generate_with_kv_cache(
                z=z_subset,
                encoder_skip=skip_subset,
                stoich_pred=stoich_subset,
                temperature=0.0,  # Greedy
                max_len=60,
                return_log_probs=False,
            )

        # Step 2: Convert tokens to formula strings and parse (no gradients)
        valid_indices = []
        parsed_inputs = []

        with torch.no_grad():
            for i in range(subset_size):
                formula = _indices_to_formula(generated_tokens[i])
                encoder_input = _formula_to_encoder_input(
                    formula, max_elements=self.max_elements, device=device
                )
                if encoder_input is not None:
                    valid_indices.append(i)
                    parsed_inputs.append(encoder_input)

        n_valid = len(valid_indices)
        if n_valid == 0:
            return {
                'round_trip_loss': torch.tensor(0.0, device=device, requires_grad=True),
                'z_mse': torch.tensor(0.0, device=device),
                'tc_mse': torch.tensor(0.0, device=device),
                'n_valid': 0,
            }

        # Step 3: Build batched encoder inputs (no gradients for indices/mask)
        valid_idx_tensor = torch.tensor(valid_indices, device=device)

        elem_indices = torch.stack([p['element_indices'] for p in parsed_inputs])    # [n_valid, 12]
        elem_fractions = torch.stack([p['element_fractions'] for p in parsed_inputs]) # [n_valid, 12]
        elem_mask = torch.stack([p['element_mask'] for p in parsed_inputs])           # [n_valid, 12]

        # Magpie and Tc from the model's own predictions (WITH gradients for magpie)
        magpie_for_encode = magpie_subset[valid_idx_tensor]  # [n_valid, 145] — has gradients
        tc_for_encode = tc_subset[valid_idx_tensor].detach() # [n_valid] — detach Tc (avoid Tc<->Z entanglement)

        # Original Z and Tc for comparison — detach immediately to prevent
        # gradients flowing backward through the original encoding path
        z_original = z_subset[valid_idx_tensor].detach()     # [n_valid, 2048]
        tc_original = tc_subset[valid_idx_tensor].detach()   # [n_valid]

        # Step 4: Re-encode WITH gradients (this is where the loss flows)
        encode_result = self._encoder.encode(
            element_indices=elem_indices,
            element_fractions=elem_fractions,
            element_mask=elem_mask,
            magpie_features=magpie_for_encode,
            tc=tc_for_encode,
        )

        z_recon = encode_result['z']  # [n_valid, 2048]

        # Step 5: Compute losses
        z_mse = F.mse_loss(z_recon, z_original)

        # Also re-decode to get Tc from reconstructed Z
        decode_result = self._encoder.decode(z_recon)
        tc_recon = decode_result['tc_pred']  # [n_valid]
        tc_mse = F.mse_loss(tc_recon, tc_original)

        total_loss = self.z_weight * z_mse + self.tc_weight * tc_mse

        return {
            'round_trip_loss': total_loss,
            'z_mse': z_mse.detach(),
            'tc_mse': tc_mse.detach(),
            'n_valid': n_valid,
        }
